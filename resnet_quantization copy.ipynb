{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import fedlern.utils as u\n",
    "from fedlern.train_utils import *\n",
    "from fedlern.quant_utils import *\n",
    "from fedlern.models.resnet_v2 import ResNet18\n",
    "from fedlern.quantize import quantize\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "stats = (0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784)\n",
    "batch_size_test = 250\n",
    "batch_size_train = 128\n",
    "\n",
    "quantize_nbits = 8\n",
    "num_epochs = 150\n",
    "learning_rate = 0.001\n",
    "eta_rate = 1.05\n",
    "eta = 1\n",
    "#global best_acc\n",
    "best_acc = 0\n",
    "best_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = prepare_dataloader_cifar(num_workers=8, \n",
    "                                                     train_batch_size=batch_size_train, \n",
    "                                                     eval_batch_size=batch_size_test, \n",
    "                                                     stats=stats)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ResNet18()\n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_model_optimizer(net, learning_rate=0.1, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "    \n",
    "# all_G_kernels = [\n",
    "#     Variable(kernel.data.clone(), requires_grad=True)\n",
    "#     for kernel in optimizer.param_groups[1]['params']\n",
    "# ]\n",
    "# Copy the parameters\n",
    "all_G_kernels = [kernel.data.clone().requires_grad_(True)\n",
    "                for kernel in optimizer.param_groups[1]['params']]\n",
    "\n",
    "#all_W_kernels = [kernel for kernel in optimizer.param_groups[1]['params']]\n",
    "all_W_kernels = optimizer.param_groups[1]['params']\n",
    "kernels = [{'params': all_G_kernels}]\n",
    "optimizer_quant = optim.SGD(kernels, lr=0)\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80,120,160], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ebcbe44156f4464bf4da5183db4dd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149 Training 0 Loss: 0.180 | Acc: 96.094% (123/128)\n",
      "Epoch: 149 Training 1 Loss: 0.262 | Acc: 92.188% (236/256)\n",
      "Epoch: 149 Training 2 Loss: 0.285 | Acc: 90.885% (349/384)\n",
      "Epoch: 149 Training 3 Loss: 0.306 | Acc: 90.430% (463/512)\n",
      "Epoch: 149 Training 4 Loss: 0.301 | Acc: 90.469% (579/640)\n",
      "Epoch: 149 Training 5 Loss: 0.295 | Acc: 90.495% (695/768)\n",
      "Epoch: 149 Training 6 Loss: 0.306 | Acc: 89.955% (806/896)\n",
      "Epoch: 149 Training 7 Loss: 0.308 | Acc: 89.844% (920/1024)\n",
      "Epoch: 149 Training 8 Loss: 0.323 | Acc: 89.410% (1030/1152)\n",
      "Epoch: 149 Training 9 Loss: 0.327 | Acc: 89.297% (1143/1280)\n",
      "Epoch: 149 Training 10 Loss: 0.324 | Acc: 89.205% (1256/1408)\n",
      "Epoch: 149 Training 11 Loss: 0.319 | Acc: 89.323% (1372/1536)\n",
      "Epoch: 149 Training 12 Loss: 0.313 | Acc: 89.483% (1489/1664)\n",
      "Epoch: 149 Training 13 Loss: 0.321 | Acc: 89.174% (1598/1792)\n",
      "Epoch: 149 Training 14 Loss: 0.317 | Acc: 89.271% (1714/1920)\n",
      "Epoch: 149 Training 15 Loss: 0.318 | Acc: 89.307% (1829/2048)\n",
      "Epoch: 149 Training 16 Loss: 0.317 | Acc: 89.384% (1945/2176)\n",
      "Epoch: 149 Training 17 Loss: 0.310 | Acc: 89.670% (2066/2304)\n",
      "Epoch: 149 Training 18 Loss: 0.304 | Acc: 89.926% (2187/2432)\n",
      "Epoch: 149 Training 19 Loss: 0.302 | Acc: 90.000% (2304/2560)\n",
      "Epoch: 149 Training 20 Loss: 0.299 | Acc: 89.955% (2418/2688)\n",
      "Epoch: 149 Training 21 Loss: 0.297 | Acc: 89.950% (2533/2816)\n",
      "Epoch: 149 Training 22 Loss: 0.297 | Acc: 89.912% (2647/2944)\n",
      "Epoch: 149 Training 23 Loss: 0.299 | Acc: 89.811% (2759/3072)\n",
      "Epoch: 149 Training 24 Loss: 0.301 | Acc: 89.875% (2876/3200)\n",
      "Epoch: 149 Training 25 Loss: 0.303 | Acc: 89.754% (2987/3328)\n",
      "Epoch: 149 Training 26 Loss: 0.306 | Acc: 89.641% (3098/3456)\n",
      "Epoch: 149 Training 27 Loss: 0.304 | Acc: 89.760% (3217/3584)\n",
      "Epoch: 149 Training 28 Loss: 0.301 | Acc: 89.871% (3336/3712)\n",
      "Epoch: 149 Training 29 Loss: 0.299 | Acc: 89.896% (3452/3840)\n",
      "Epoch: 149 Training 30 Loss: 0.302 | Acc: 89.793% (3563/3968)\n",
      "Epoch: 149 Training 31 Loss: 0.298 | Acc: 89.941% (3684/4096)\n",
      "Epoch: 149 Training 32 Loss: 0.298 | Acc: 89.962% (3800/4224)\n",
      "Epoch: 149 Training 33 Loss: 0.296 | Acc: 90.051% (3919/4352)\n",
      "Epoch: 149 Training 34 Loss: 0.296 | Acc: 90.045% (4034/4480)\n",
      "Epoch: 149 Training 35 Loss: 0.297 | Acc: 89.974% (4146/4608)\n",
      "Epoch: 149 Training 36 Loss: 0.295 | Acc: 90.055% (4265/4736)\n",
      "Epoch: 149 Training 37 Loss: 0.293 | Acc: 90.111% (4383/4864)\n",
      "Epoch: 149 Training 38 Loss: 0.293 | Acc: 90.064% (4496/4992)\n",
      "Epoch: 149 Training 39 Loss: 0.291 | Acc: 90.156% (4616/5120)\n",
      "Epoch: 149 Training 40 Loss: 0.290 | Acc: 90.168% (4732/5248)\n",
      "Epoch: 149 Training 41 Loss: 0.293 | Acc: 90.067% (4842/5376)\n",
      "Epoch: 149 Training 42 Loss: 0.294 | Acc: 90.134% (4961/5504)\n",
      "Epoch: 149 Training 43 Loss: 0.295 | Acc: 90.146% (5077/5632)\n",
      "Epoch: 149 Training 44 Loss: 0.295 | Acc: 90.156% (5193/5760)\n",
      "Epoch: 149 Training 45 Loss: 0.295 | Acc: 90.149% (5308/5888)\n",
      "Epoch: 149 Training 46 Loss: 0.295 | Acc: 90.160% (5424/6016)\n",
      "Epoch: 149 Training 47 Loss: 0.293 | Acc: 90.251% (5545/6144)\n",
      "Epoch: 149 Training 48 Loss: 0.291 | Acc: 90.274% (5662/6272)\n",
      "Epoch: 149 Training 49 Loss: 0.290 | Acc: 90.250% (5776/6400)\n",
      "Epoch: 149 Training 50 Loss: 0.290 | Acc: 90.257% (5892/6528)\n",
      "Epoch: 149 Training 51 Loss: 0.289 | Acc: 90.294% (6010/6656)\n",
      "Epoch: 149 Training 52 Loss: 0.290 | Acc: 90.330% (6128/6784)\n",
      "Epoch: 149 Training 53 Loss: 0.291 | Acc: 90.263% (6239/6912)\n",
      "Epoch: 149 Training 54 Loss: 0.291 | Acc: 90.298% (6357/7040)\n",
      "Epoch: 149 Training 55 Loss: 0.290 | Acc: 90.332% (6475/7168)\n",
      "Epoch: 149 Training 56 Loss: 0.290 | Acc: 90.282% (6587/7296)\n",
      "Epoch: 149 Training 57 Loss: 0.288 | Acc: 90.369% (6709/7424)\n",
      "Epoch: 149 Training 58 Loss: 0.287 | Acc: 90.440% (6830/7552)\n",
      "Epoch: 149 Training 59 Loss: 0.286 | Acc: 90.495% (6950/7680)\n",
      "Epoch: 149 Training 60 Loss: 0.284 | Acc: 90.574% (7072/7808)\n",
      "Epoch: 149 Training 61 Loss: 0.285 | Acc: 90.512% (7183/7936)\n",
      "Epoch: 149 Training 62 Loss: 0.287 | Acc: 90.464% (7295/8064)\n",
      "Epoch: 149 Training 63 Loss: 0.287 | Acc: 90.479% (7412/8192)\n",
      "Epoch: 149 Training 64 Loss: 0.287 | Acc: 90.457% (7526/8320)\n",
      "Epoch: 149 Training 65 Loss: 0.286 | Acc: 90.483% (7644/8448)\n",
      "Epoch: 149 Training 66 Loss: 0.287 | Acc: 90.462% (7758/8576)\n",
      "Epoch: 149 Training 67 Loss: 0.286 | Acc: 90.476% (7875/8704)\n",
      "Epoch: 149 Training 68 Loss: 0.287 | Acc: 90.444% (7988/8832)\n",
      "Epoch: 149 Training 69 Loss: 0.287 | Acc: 90.435% (8103/8960)\n",
      "Epoch: 149 Training 70 Loss: 0.288 | Acc: 90.427% (8218/9088)\n",
      "Epoch: 149 Training 71 Loss: 0.288 | Acc: 90.419% (8333/9216)\n",
      "Epoch: 149 Training 72 Loss: 0.288 | Acc: 90.432% (8450/9344)\n",
      "Epoch: 149 Training 73 Loss: 0.287 | Acc: 90.446% (8567/9472)\n",
      "Epoch: 149 Training 74 Loss: 0.288 | Acc: 90.396% (8678/9600)\n",
      "Epoch: 149 Training 75 Loss: 0.287 | Acc: 90.399% (8794/9728)\n",
      "Epoch: 149 Training 76 Loss: 0.288 | Acc: 90.381% (8908/9856)\n",
      "Epoch: 149 Training 77 Loss: 0.286 | Acc: 90.425% (9028/9984)\n",
      "Epoch: 149 Training 78 Loss: 0.286 | Acc: 90.398% (9141/10112)\n",
      "Epoch: 149 Training 79 Loss: 0.287 | Acc: 90.371% (9254/10240)\n",
      "Epoch: 149 Training 80 Loss: 0.287 | Acc: 90.355% (9368/10368)\n",
      "Epoch: 149 Training 81 Loss: 0.287 | Acc: 90.330% (9481/10496)\n",
      "Epoch: 149 Training 82 Loss: 0.288 | Acc: 90.305% (9594/10624)\n",
      "Epoch: 149 Training 83 Loss: 0.288 | Acc: 90.309% (9710/10752)\n",
      "Epoch: 149 Training 84 Loss: 0.287 | Acc: 90.322% (9827/10880)\n",
      "Epoch: 149 Training 85 Loss: 0.287 | Acc: 90.307% (9941/11008)\n",
      "Epoch: 149 Training 86 Loss: 0.288 | Acc: 90.275% (10053/11136)\n",
      "Epoch: 149 Training 87 Loss: 0.289 | Acc: 90.261% (10167/11264)\n",
      "Epoch: 149 Training 88 Loss: 0.289 | Acc: 90.256% (10282/11392)\n",
      "Epoch: 149 Training 89 Loss: 0.288 | Acc: 90.295% (10402/11520)\n",
      "Epoch: 149 Training 90 Loss: 0.288 | Acc: 90.290% (10517/11648)\n",
      "Epoch: 149 Training 91 Loss: 0.289 | Acc: 90.251% (10628/11776)\n",
      "Epoch: 149 Training 92 Loss: 0.289 | Acc: 90.272% (10746/11904)\n",
      "Epoch: 149 Training 93 Loss: 0.289 | Acc: 90.234% (10857/12032)\n",
      "Epoch: 149 Training 94 Loss: 0.289 | Acc: 90.238% (10973/12160)\n",
      "Epoch: 149 Training 95 Loss: 0.288 | Acc: 90.243% (11089/12288)\n",
      "Epoch: 149 Training 96 Loss: 0.289 | Acc: 90.222% (11202/12416)\n",
      "Epoch: 149 Training 97 Loss: 0.289 | Acc: 90.210% (11316/12544)\n",
      "Epoch: 149 Training 98 Loss: 0.288 | Acc: 90.199% (11430/12672)\n",
      "Epoch: 149 Training 99 Loss: 0.289 | Acc: 90.180% (11543/12800)\n",
      "Epoch: 149 Training 100 Loss: 0.290 | Acc: 90.176% (11658/12928)\n",
      "Epoch: 149 Training 101 Loss: 0.291 | Acc: 90.165% (11772/13056)\n",
      "Epoch: 149 Training 102 Loss: 0.291 | Acc: 90.147% (11885/13184)\n",
      "Epoch: 149 Training 103 Loss: 0.291 | Acc: 90.159% (12002/13312)\n",
      "Epoch: 149 Training 104 Loss: 0.290 | Acc: 90.179% (12120/13440)\n",
      "Epoch: 149 Training 105 Loss: 0.290 | Acc: 90.168% (12234/13568)\n",
      "Epoch: 149 Training 106 Loss: 0.291 | Acc: 90.136% (12345/13696)\n",
      "Epoch: 149 Training 107 Loss: 0.292 | Acc: 90.119% (12458/13824)\n",
      "Epoch: 149 Training 108 Loss: 0.291 | Acc: 90.145% (12577/13952)\n",
      "Epoch: 149 Training 109 Loss: 0.292 | Acc: 90.099% (12686/14080)\n",
      "Epoch: 149 Training 110 Loss: 0.293 | Acc: 90.083% (12799/14208)\n",
      "Epoch: 149 Training 111 Loss: 0.294 | Acc: 90.046% (12909/14336)\n",
      "Epoch: 149 Training 112 Loss: 0.294 | Acc: 90.065% (13027/14464)\n",
      "Epoch: 149 Training 113 Loss: 0.294 | Acc: 90.063% (13142/14592)\n",
      "Epoch: 149 Training 114 Loss: 0.294 | Acc: 90.054% (13256/14720)\n",
      "Epoch: 149 Training 115 Loss: 0.294 | Acc: 90.039% (13369/14848)\n",
      "Epoch: 149 Training 116 Loss: 0.294 | Acc: 90.017% (13481/14976)\n",
      "Epoch: 149 Training 117 Loss: 0.294 | Acc: 90.016% (13596/15104)\n",
      "Epoch: 149 Training 118 Loss: 0.294 | Acc: 90.021% (13712/15232)\n",
      "Epoch: 149 Training 119 Loss: 0.295 | Acc: 89.967% (13819/15360)\n",
      "Epoch: 149 Training 120 Loss: 0.295 | Acc: 89.960% (13933/15488)\n",
      "Epoch: 149 Training 121 Loss: 0.295 | Acc: 89.978% (14051/15616)\n",
      "Epoch: 149 Training 122 Loss: 0.296 | Acc: 89.933% (14159/15744)\n",
      "Epoch: 149 Training 123 Loss: 0.296 | Acc: 89.970% (14280/15872)\n",
      "Epoch: 149 Training 124 Loss: 0.295 | Acc: 90.019% (14403/16000)\n",
      "Epoch: 149 Training 125 Loss: 0.294 | Acc: 90.036% (14521/16128)\n",
      "Epoch: 149 Training 126 Loss: 0.294 | Acc: 90.071% (14642/16256)\n",
      "Epoch: 149 Training 127 Loss: 0.294 | Acc: 90.070% (14757/16384)\n",
      "Epoch: 149 Training 128 Loss: 0.295 | Acc: 90.019% (14864/16512)\n",
      "Epoch: 149 Training 129 Loss: 0.294 | Acc: 90.030% (14981/16640)\n",
      "Epoch: 149 Training 130 Loss: 0.295 | Acc: 90.041% (15098/16768)\n",
      "Epoch: 149 Training 131 Loss: 0.294 | Acc: 90.051% (15215/16896)\n",
      "Epoch: 149 Training 132 Loss: 0.294 | Acc: 90.038% (15328/17024)\n",
      "Epoch: 149 Training 133 Loss: 0.294 | Acc: 90.036% (15443/17152)\n",
      "Epoch: 149 Training 134 Loss: 0.294 | Acc: 90.006% (15553/17280)\n",
      "Epoch: 149 Training 135 Loss: 0.294 | Acc: 90.022% (15671/17408)\n",
      "Epoch: 149 Training 136 Loss: 0.294 | Acc: 90.026% (15787/17536)\n",
      "Epoch: 149 Training 137 Loss: 0.293 | Acc: 90.025% (15902/17664)\n",
      "Epoch: 149 Training 138 Loss: 0.293 | Acc: 90.007% (16014/17792)\n",
      "Epoch: 149 Training 139 Loss: 0.293 | Acc: 89.989% (16126/17920)\n",
      "Epoch: 149 Training 140 Loss: 0.293 | Acc: 89.988% (16241/18048)\n",
      "Epoch: 149 Training 141 Loss: 0.293 | Acc: 89.970% (16353/18176)\n",
      "Epoch: 149 Training 142 Loss: 0.293 | Acc: 89.980% (16470/18304)\n",
      "Epoch: 149 Training 143 Loss: 0.293 | Acc: 89.958% (16581/18432)\n",
      "Epoch: 149 Training 144 Loss: 0.294 | Acc: 89.946% (16694/18560)\n",
      "Epoch: 149 Training 145 Loss: 0.294 | Acc: 89.940% (16808/18688)\n",
      "Epoch: 149 Training 146 Loss: 0.294 | Acc: 89.955% (16926/18816)\n",
      "Epoch: 149 Training 147 Loss: 0.294 | Acc: 89.965% (17043/18944)\n",
      "Epoch: 149 Training 148 Loss: 0.294 | Acc: 89.959% (17157/19072)\n",
      "Epoch: 149 Training 149 Loss: 0.294 | Acc: 89.990% (17278/19200)\n",
      "Epoch: 149 Training 150 Loss: 0.294 | Acc: 89.994% (17394/19328)\n",
      "Epoch: 149 Training 151 Loss: 0.295 | Acc: 89.983% (17507/19456)\n",
      "Epoch: 149 Training 152 Loss: 0.294 | Acc: 89.997% (17625/19584)\n",
      "Epoch: 149 Training 153 Loss: 0.294 | Acc: 90.001% (17741/19712)\n",
      "Epoch: 149 Training 154 Loss: 0.294 | Acc: 90.005% (17857/19840)\n",
      "Epoch: 149 Training 155 Loss: 0.294 | Acc: 89.994% (17970/19968)\n",
      "Epoch: 149 Training 156 Loss: 0.294 | Acc: 89.988% (18084/20096)\n",
      "Epoch: 149 Training 157 Loss: 0.295 | Acc: 89.972% (18196/20224)\n",
      "Epoch: 149 Training 158 Loss: 0.295 | Acc: 89.981% (18313/20352)\n",
      "Epoch: 149 Training 159 Loss: 0.294 | Acc: 89.995% (18431/20480)\n",
      "Epoch: 149 Training 160 Loss: 0.295 | Acc: 89.970% (18541/20608)\n",
      "Epoch: 149 Training 161 Loss: 0.295 | Acc: 89.955% (18653/20736)\n",
      "Epoch: 149 Training 162 Loss: 0.295 | Acc: 89.940% (18765/20864)\n",
      "Epoch: 149 Training 163 Loss: 0.296 | Acc: 89.915% (18875/20992)\n",
      "Epoch: 149 Training 164 Loss: 0.296 | Acc: 89.901% (18987/21120)\n",
      "Epoch: 149 Training 165 Loss: 0.297 | Acc: 89.881% (19098/21248)\n",
      "Epoch: 149 Training 166 Loss: 0.297 | Acc: 89.862% (19209/21376)\n",
      "Epoch: 149 Training 167 Loss: 0.297 | Acc: 89.853% (19322/21504)\n",
      "Epoch: 149 Training 168 Loss: 0.296 | Acc: 89.871% (19441/21632)\n",
      "Epoch: 149 Training 169 Loss: 0.296 | Acc: 89.890% (19560/21760)\n",
      "Epoch: 149 Training 170 Loss: 0.296 | Acc: 89.880% (19673/21888)\n",
      "Epoch: 149 Training 171 Loss: 0.296 | Acc: 89.880% (19788/22016)\n",
      "Epoch: 149 Training 172 Loss: 0.297 | Acc: 89.875% (19902/22144)\n",
      "Epoch: 149 Training 173 Loss: 0.297 | Acc: 89.866% (20015/22272)\n",
      "Epoch: 149 Training 174 Loss: 0.298 | Acc: 89.844% (20125/22400)\n",
      "Epoch: 149 Training 175 Loss: 0.298 | Acc: 89.826% (20236/22528)\n",
      "Epoch: 149 Training 176 Loss: 0.298 | Acc: 89.813% (20348/22656)\n",
      "Epoch: 149 Training 177 Loss: 0.298 | Acc: 89.817% (20464/22784)\n",
      "Epoch: 149 Training 178 Loss: 0.298 | Acc: 89.818% (20579/22912)\n",
      "Epoch: 149 Training 179 Loss: 0.298 | Acc: 89.805% (20691/23040)\n",
      "Epoch: 149 Training 180 Loss: 0.298 | Acc: 89.805% (20806/23168)\n",
      "Epoch: 149 Training 181 Loss: 0.298 | Acc: 89.805% (20921/23296)\n",
      "Epoch: 149 Training 182 Loss: 0.298 | Acc: 89.780% (21030/23424)\n",
      "Epoch: 149 Training 183 Loss: 0.299 | Acc: 89.772% (21143/23552)\n",
      "Epoch: 149 Training 184 Loss: 0.299 | Acc: 89.755% (21254/23680)\n",
      "Epoch: 149 Training 185 Loss: 0.299 | Acc: 89.739% (21365/23808)\n",
      "Epoch: 149 Training 186 Loss: 0.299 | Acc: 89.714% (21474/23936)\n",
      "Epoch: 149 Training 187 Loss: 0.299 | Acc: 89.723% (21591/24064)\n",
      "Epoch: 149 Training 188 Loss: 0.299 | Acc: 89.740% (21710/24192)\n",
      "Epoch: 149 Training 189 Loss: 0.299 | Acc: 89.737% (21824/24320)\n",
      "Epoch: 149 Training 190 Loss: 0.299 | Acc: 89.737% (21939/24448)\n",
      "Epoch: 149 Training 191 Loss: 0.299 | Acc: 89.726% (22051/24576)\n",
      "Epoch: 149 Training 192 Loss: 0.300 | Acc: 89.694% (22158/24704)\n",
      "Epoch: 149 Training 193 Loss: 0.300 | Acc: 89.675% (22268/24832)\n",
      "Epoch: 149 Training 194 Loss: 0.300 | Acc: 89.692% (22387/24960)\n",
      "Epoch: 149 Training 195 Loss: 0.300 | Acc: 89.692% (22502/25088)\n",
      "Epoch: 149 Training 196 Loss: 0.300 | Acc: 89.677% (22613/25216)\n",
      "Epoch: 149 Training 197 Loss: 0.301 | Acc: 89.670% (22726/25344)\n",
      "Epoch: 149 Training 198 Loss: 0.300 | Acc: 89.695% (22847/25472)\n",
      "Epoch: 149 Training 199 Loss: 0.300 | Acc: 89.691% (22961/25600)\n",
      "Epoch: 149 Training 200 Loss: 0.300 | Acc: 89.688% (23075/25728)\n",
      "Epoch: 149 Training 201 Loss: 0.300 | Acc: 89.685% (23189/25856)\n",
      "Epoch: 149 Training 202 Loss: 0.300 | Acc: 89.690% (23305/25984)\n",
      "Epoch: 149 Training 203 Loss: 0.300 | Acc: 89.652% (23410/26112)\n",
      "Epoch: 149 Training 204 Loss: 0.301 | Acc: 89.646% (23523/26240)\n",
      "Epoch: 149 Training 205 Loss: 0.301 | Acc: 89.643% (23637/26368)\n",
      "Epoch: 149 Training 206 Loss: 0.301 | Acc: 89.629% (23748/26496)\n",
      "Epoch: 149 Training 207 Loss: 0.301 | Acc: 89.630% (23863/26624)\n",
      "Epoch: 149 Training 208 Loss: 0.302 | Acc: 89.597% (23969/26752)\n",
      "Epoch: 149 Training 209 Loss: 0.302 | Acc: 89.587% (24081/26880)\n",
      "Epoch: 149 Training 210 Loss: 0.302 | Acc: 89.588% (24196/27008)\n",
      "Epoch: 149 Training 211 Loss: 0.302 | Acc: 89.597% (24313/27136)\n",
      "Epoch: 149 Training 212 Loss: 0.302 | Acc: 89.598% (24428/27264)\n",
      "Epoch: 149 Training 213 Loss: 0.302 | Acc: 89.588% (24540/27392)\n",
      "Epoch: 149 Training 214 Loss: 0.301 | Acc: 89.611% (24661/27520)\n",
      "Epoch: 149 Training 215 Loss: 0.302 | Acc: 89.612% (24776/27648)\n",
      "Epoch: 149 Training 216 Loss: 0.301 | Acc: 89.631% (24896/27776)\n",
      "Epoch: 149 Training 217 Loss: 0.302 | Acc: 89.629% (25010/27904)\n",
      "Epoch: 149 Training 218 Loss: 0.302 | Acc: 89.633% (25126/28032)\n",
      "Epoch: 149 Training 219 Loss: 0.302 | Acc: 89.638% (25242/28160)\n",
      "Epoch: 149 Training 220 Loss: 0.302 | Acc: 89.625% (25353/28288)\n",
      "Epoch: 149 Training 221 Loss: 0.302 | Acc: 89.629% (25469/28416)\n",
      "Epoch: 149 Training 222 Loss: 0.302 | Acc: 89.627% (25583/28544)\n",
      "Epoch: 149 Training 223 Loss: 0.302 | Acc: 89.638% (25701/28672)\n",
      "Epoch: 149 Training 224 Loss: 0.301 | Acc: 89.649% (25819/28800)\n",
      "Epoch: 149 Training 225 Loss: 0.302 | Acc: 89.633% (25929/28928)\n",
      "Epoch: 149 Training 226 Loss: 0.302 | Acc: 89.634% (26044/29056)\n",
      "Epoch: 149 Training 227 Loss: 0.301 | Acc: 89.652% (26164/29184)\n",
      "Epoch: 149 Training 228 Loss: 0.302 | Acc: 89.649% (26278/29312)\n",
      "Epoch: 149 Training 229 Loss: 0.302 | Acc: 89.650% (26393/29440)\n",
      "Epoch: 149 Training 230 Loss: 0.302 | Acc: 89.641% (26505/29568)\n",
      "Epoch: 149 Training 231 Loss: 0.302 | Acc: 89.608% (26610/29696)\n",
      "Epoch: 149 Training 232 Loss: 0.302 | Acc: 89.612% (26726/29824)\n",
      "Epoch: 149 Training 233 Loss: 0.302 | Acc: 89.613% (26841/29952)\n",
      "Epoch: 149 Training 234 Loss: 0.302 | Acc: 89.624% (26959/30080)\n",
      "Epoch: 149 Training 235 Loss: 0.302 | Acc: 89.615% (27071/30208)\n",
      "Epoch: 149 Training 236 Loss: 0.302 | Acc: 89.620% (27187/30336)\n",
      "Epoch: 149 Training 237 Loss: 0.302 | Acc: 89.614% (27300/30464)\n",
      "Epoch: 149 Training 238 Loss: 0.302 | Acc: 89.621% (27417/30592)\n",
      "Epoch: 149 Training 239 Loss: 0.302 | Acc: 89.629% (27534/30720)\n",
      "Epoch: 149 Training 240 Loss: 0.302 | Acc: 89.610% (27643/30848)\n",
      "Epoch: 149 Training 241 Loss: 0.302 | Acc: 89.598% (27754/30976)\n",
      "Epoch: 149 Training 242 Loss: 0.303 | Acc: 89.593% (27867/31104)\n",
      "Epoch: 149 Training 243 Loss: 0.303 | Acc: 89.584% (27979/31232)\n",
      "Epoch: 149 Training 244 Loss: 0.303 | Acc: 89.570% (28089/31360)\n",
      "Epoch: 149 Training 245 Loss: 0.303 | Acc: 89.577% (28206/31488)\n",
      "Epoch: 149 Training 246 Loss: 0.303 | Acc: 89.578% (28321/31616)\n",
      "Epoch: 149 Training 247 Loss: 0.304 | Acc: 89.557% (28429/31744)\n",
      "Epoch: 149 Training 248 Loss: 0.304 | Acc: 89.558% (28544/31872)\n",
      "Epoch: 149 Training 249 Loss: 0.304 | Acc: 89.547% (28655/32000)\n",
      "Epoch: 149 Training 250 Loss: 0.305 | Acc: 89.532% (28765/32128)\n",
      "Epoch: 149 Training 251 Loss: 0.305 | Acc: 89.512% (28873/32256)\n",
      "Epoch: 149 Training 252 Loss: 0.305 | Acc: 89.526% (28992/32384)\n",
      "Epoch: 149 Training 253 Loss: 0.304 | Acc: 89.536% (29110/32512)\n",
      "Epoch: 149 Training 254 Loss: 0.304 | Acc: 89.544% (29227/32640)\n",
      "Epoch: 149 Training 255 Loss: 0.304 | Acc: 89.560% (29347/32768)\n",
      "Epoch: 149 Training 256 Loss: 0.304 | Acc: 89.552% (29459/32896)\n",
      "Epoch: 149 Training 257 Loss: 0.304 | Acc: 89.559% (29576/33024)\n",
      "Epoch: 149 Training 258 Loss: 0.304 | Acc: 89.563% (29692/33152)\n",
      "Epoch: 149 Training 259 Loss: 0.304 | Acc: 89.570% (29809/33280)\n",
      "Epoch: 149 Training 260 Loss: 0.304 | Acc: 89.583% (29928/33408)\n",
      "Epoch: 149 Training 261 Loss: 0.304 | Acc: 89.581% (30042/33536)\n",
      "Epoch: 149 Training 262 Loss: 0.304 | Acc: 89.565% (30151/33664)\n",
      "Epoch: 149 Training 263 Loss: 0.304 | Acc: 89.574% (30269/33792)\n",
      "Epoch: 149 Training 264 Loss: 0.304 | Acc: 89.581% (30386/33920)\n",
      "Epoch: 149 Training 265 Loss: 0.304 | Acc: 89.568% (30496/34048)\n",
      "Epoch: 149 Training 266 Loss: 0.304 | Acc: 89.569% (30611/34176)\n",
      "Epoch: 149 Training 267 Loss: 0.304 | Acc: 89.561% (30723/34304)\n",
      "Epoch: 149 Training 268 Loss: 0.304 | Acc: 89.562% (30838/34432)\n",
      "Epoch: 149 Training 269 Loss: 0.304 | Acc: 89.566% (30954/34560)\n",
      "Epoch: 149 Training 270 Loss: 0.304 | Acc: 89.581% (31074/34688)\n",
      "Epoch: 149 Training 271 Loss: 0.304 | Acc: 89.571% (31185/34816)\n",
      "Epoch: 149 Training 272 Loss: 0.304 | Acc: 89.575% (31301/34944)\n",
      "Epoch: 149 Training 273 Loss: 0.303 | Acc: 89.584% (31419/35072)\n",
      "Epoch: 149 Training 274 Loss: 0.303 | Acc: 89.585% (31534/35200)\n",
      "Epoch: 149 Training 275 Loss: 0.303 | Acc: 89.583% (31648/35328)\n",
      "Epoch: 149 Training 276 Loss: 0.303 | Acc: 89.579% (31761/35456)\n",
      "Epoch: 149 Training 277 Loss: 0.304 | Acc: 89.566% (31871/35584)\n",
      "Epoch: 149 Training 278 Loss: 0.304 | Acc: 89.572% (31988/35712)\n",
      "Epoch: 149 Training 279 Loss: 0.304 | Acc: 89.573% (32103/35840)\n",
      "Epoch: 149 Training 280 Loss: 0.304 | Acc: 89.557% (32212/35968)\n",
      "Epoch: 149 Training 281 Loss: 0.304 | Acc: 89.572% (32332/36096)\n",
      "Epoch: 149 Training 282 Loss: 0.304 | Acc: 89.568% (32445/36224)\n",
      "Epoch: 149 Training 283 Loss: 0.303 | Acc: 89.582% (32565/36352)\n",
      "Epoch: 149 Training 284 Loss: 0.303 | Acc: 89.589% (32682/36480)\n",
      "Epoch: 149 Training 285 Loss: 0.303 | Acc: 89.595% (32799/36608)\n",
      "Epoch: 149 Training 286 Loss: 0.303 | Acc: 89.601% (32916/36736)\n",
      "Epoch: 149 Training 287 Loss: 0.303 | Acc: 89.597% (33029/36864)\n",
      "Epoch: 149 Training 288 Loss: 0.303 | Acc: 89.603% (33146/36992)\n",
      "Epoch: 149 Training 289 Loss: 0.303 | Acc: 89.591% (33256/37120)\n",
      "Epoch: 149 Training 290 Loss: 0.303 | Acc: 89.589% (33370/37248)\n",
      "Epoch: 149 Training 291 Loss: 0.303 | Acc: 89.582% (33482/37376)\n",
      "Epoch: 149 Training 292 Loss: 0.303 | Acc: 89.580% (33596/37504)\n",
      "Epoch: 149 Training 293 Loss: 0.303 | Acc: 89.583% (33712/37632)\n",
      "Epoch: 149 Training 294 Loss: 0.303 | Acc: 89.574% (33823/37760)\n",
      "Epoch: 149 Training 295 Loss: 0.303 | Acc: 89.572% (33937/37888)\n",
      "Epoch: 149 Training 296 Loss: 0.303 | Acc: 89.570% (34051/38016)\n",
      "Epoch: 149 Training 297 Loss: 0.303 | Acc: 89.576% (34168/38144)\n",
      "Epoch: 149 Training 298 Loss: 0.303 | Acc: 89.569% (34280/38272)\n",
      "Epoch: 149 Training 299 Loss: 0.303 | Acc: 89.573% (34396/38400)\n",
      "Epoch: 149 Training 300 Loss: 0.303 | Acc: 89.566% (34508/38528)\n",
      "Epoch: 149 Training 301 Loss: 0.303 | Acc: 89.572% (34625/38656)\n",
      "Epoch: 149 Training 302 Loss: 0.303 | Acc: 89.570% (34739/38784)\n",
      "Epoch: 149 Training 303 Loss: 0.303 | Acc: 89.561% (34850/38912)\n",
      "Epoch: 149 Training 304 Loss: 0.304 | Acc: 89.562% (34965/39040)\n",
      "Epoch: 149 Training 305 Loss: 0.304 | Acc: 89.560% (35079/39168)\n",
      "Epoch: 149 Training 306 Loss: 0.304 | Acc: 89.551% (35190/39296)\n",
      "Epoch: 149 Training 307 Loss: 0.304 | Acc: 89.547% (35303/39424)\n",
      "Epoch: 149 Training 308 Loss: 0.305 | Acc: 89.523% (35408/39552)\n",
      "Epoch: 149 Training 309 Loss: 0.305 | Acc: 89.511% (35518/39680)\n",
      "Epoch: 149 Training 310 Loss: 0.305 | Acc: 89.505% (35630/39808)\n",
      "Epoch: 149 Training 311 Loss: 0.305 | Acc: 89.506% (35745/39936)\n",
      "Epoch: 149 Training 312 Loss: 0.305 | Acc: 89.517% (35864/40064)\n",
      "Epoch: 149 Training 313 Loss: 0.305 | Acc: 89.515% (35978/40192)\n",
      "Epoch: 149 Training 314 Loss: 0.305 | Acc: 89.514% (36092/40320)\n",
      "Epoch: 149 Training 315 Loss: 0.305 | Acc: 89.520% (36209/40448)\n",
      "Epoch: 149 Training 316 Loss: 0.305 | Acc: 89.526% (36326/40576)\n",
      "Epoch: 149 Training 317 Loss: 0.305 | Acc: 89.532% (36443/40704)\n",
      "Epoch: 149 Training 318 Loss: 0.305 | Acc: 89.528% (36556/40832)\n",
      "Epoch: 149 Training 319 Loss: 0.305 | Acc: 89.536% (36674/40960)\n",
      "Epoch: 149 Training 320 Loss: 0.305 | Acc: 89.542% (36791/41088)\n",
      "Epoch: 149 Training 321 Loss: 0.305 | Acc: 89.531% (36901/41216)\n",
      "Epoch: 149 Training 322 Loss: 0.305 | Acc: 89.524% (37013/41344)\n",
      "Epoch: 149 Training 323 Loss: 0.305 | Acc: 89.518% (37125/41472)\n",
      "Epoch: 149 Training 324 Loss: 0.306 | Acc: 89.512% (37237/41600)\n",
      "Epoch: 149 Training 325 Loss: 0.305 | Acc: 89.520% (37355/41728)\n",
      "Epoch: 149 Training 326 Loss: 0.306 | Acc: 89.512% (37466/41856)\n",
      "Epoch: 149 Training 327 Loss: 0.306 | Acc: 89.525% (37586/41984)\n",
      "Epoch: 149 Training 328 Loss: 0.305 | Acc: 89.537% (37706/42112)\n",
      "Epoch: 149 Training 329 Loss: 0.305 | Acc: 89.534% (37819/42240)\n",
      "Epoch: 149 Training 330 Loss: 0.305 | Acc: 89.525% (37930/42368)\n",
      "Epoch: 149 Training 331 Loss: 0.305 | Acc: 89.524% (38044/42496)\n",
      "Epoch: 149 Training 332 Loss: 0.306 | Acc: 89.513% (38154/42624)\n",
      "Epoch: 149 Training 333 Loss: 0.306 | Acc: 89.509% (38267/42752)\n",
      "Epoch: 149 Training 334 Loss: 0.306 | Acc: 89.506% (38380/42880)\n",
      "Epoch: 149 Training 335 Loss: 0.306 | Acc: 89.509% (38496/43008)\n",
      "Epoch: 149 Training 336 Loss: 0.306 | Acc: 89.496% (38605/43136)\n",
      "Epoch: 149 Training 337 Loss: 0.305 | Acc: 89.499% (38721/43264)\n",
      "Epoch: 149 Training 338 Loss: 0.305 | Acc: 89.505% (38838/43392)\n",
      "Epoch: 149 Training 339 Loss: 0.305 | Acc: 89.499% (38950/43520)\n",
      "Epoch: 149 Training 340 Loss: 0.306 | Acc: 89.482% (39057/43648)\n",
      "Epoch: 149 Training 341 Loss: 0.306 | Acc: 89.481% (39171/43776)\n",
      "Epoch: 149 Training 342 Loss: 0.306 | Acc: 89.484% (39287/43904)\n",
      "Epoch: 149 Training 343 Loss: 0.306 | Acc: 89.494% (39406/44032)\n",
      "Epoch: 149 Training 344 Loss: 0.305 | Acc: 89.513% (39529/44160)\n",
      "Epoch: 149 Training 345 Loss: 0.305 | Acc: 89.510% (39642/44288)\n",
      "Epoch: 149 Training 346 Loss: 0.305 | Acc: 89.499% (39752/44416)\n",
      "Epoch: 149 Training 347 Loss: 0.306 | Acc: 89.485% (39860/44544)\n",
      "Epoch: 149 Training 348 Loss: 0.306 | Acc: 89.488% (39976/44672)\n",
      "Epoch: 149 Training 349 Loss: 0.306 | Acc: 89.493% (40093/44800)\n",
      "Epoch: 149 Training 350 Loss: 0.305 | Acc: 89.494% (40208/44928)\n",
      "Epoch: 149 Training 351 Loss: 0.305 | Acc: 89.502% (40326/45056)\n",
      "Epoch: 149 Training 352 Loss: 0.305 | Acc: 89.507% (40443/45184)\n",
      "Epoch: 149 Training 353 Loss: 0.305 | Acc: 89.506% (40557/45312)\n",
      "Epoch: 149 Training 354 Loss: 0.305 | Acc: 89.511% (40674/45440)\n",
      "Epoch: 149 Training 355 Loss: 0.305 | Acc: 89.512% (40789/45568)\n",
      "Epoch: 149 Training 356 Loss: 0.305 | Acc: 89.518% (40906/45696)\n",
      "Epoch: 149 Training 357 Loss: 0.305 | Acc: 89.521% (41022/45824)\n",
      "Epoch: 149 Training 358 Loss: 0.305 | Acc: 89.530% (41141/45952)\n",
      "Epoch: 149 Training 359 Loss: 0.305 | Acc: 89.533% (41257/46080)\n",
      "Epoch: 149 Training 360 Loss: 0.305 | Acc: 89.526% (41368/46208)\n",
      "Epoch: 149 Training 361 Loss: 0.305 | Acc: 89.524% (41482/46336)\n",
      "Epoch: 149 Training 362 Loss: 0.305 | Acc: 89.532% (41600/46464)\n",
      "Epoch: 149 Training 363 Loss: 0.305 | Acc: 89.524% (41711/46592)\n",
      "Epoch: 149 Training 364 Loss: 0.305 | Acc: 89.523% (41825/46720)\n",
      "Epoch: 149 Training 365 Loss: 0.305 | Acc: 89.528% (41942/46848)\n",
      "Epoch: 149 Training 366 Loss: 0.305 | Acc: 89.518% (42052/46976)\n",
      "Epoch: 149 Training 367 Loss: 0.305 | Acc: 89.523% (42169/47104)\n",
      "Epoch: 149 Training 368 Loss: 0.305 | Acc: 89.528% (42286/47232)\n",
      "Epoch: 149 Training 369 Loss: 0.305 | Acc: 89.529% (42401/47360)\n",
      "Epoch: 149 Training 370 Loss: 0.305 | Acc: 89.530% (42516/47488)\n",
      "Epoch: 149 Training 371 Loss: 0.305 | Acc: 89.527% (42629/47616)\n",
      "Epoch: 149 Training 372 Loss: 0.305 | Acc: 89.519% (42740/47744)\n",
      "Epoch: 149 Training 373 Loss: 0.305 | Acc: 89.518% (42854/47872)\n",
      "Epoch: 149 Training 374 Loss: 0.305 | Acc: 89.531% (42975/48000)\n",
      "Epoch: 149 Training 375 Loss: 0.305 | Acc: 89.528% (43088/48128)\n",
      "Epoch: 149 Training 376 Loss: 0.305 | Acc: 89.531% (43204/48256)\n",
      "Epoch: 149 Training 377 Loss: 0.306 | Acc: 89.523% (43315/48384)\n",
      "Epoch: 149 Training 378 Loss: 0.306 | Acc: 89.516% (43426/48512)\n",
      "Epoch: 149 Training 379 Loss: 0.306 | Acc: 89.517% (43541/48640)\n",
      "Epoch: 149 Training 380 Loss: 0.306 | Acc: 89.520% (43657/48768)\n",
      "Epoch: 149 Training 381 Loss: 0.306 | Acc: 89.519% (43771/48896)\n",
      "Epoch: 149 Training 382 Loss: 0.306 | Acc: 89.507% (43880/49024)\n",
      "Epoch: 149 Training 383 Loss: 0.306 | Acc: 89.506% (43994/49152)\n",
      "Epoch: 149 Training 384 Loss: 0.306 | Acc: 89.495% (44103/49280)\n",
      "Epoch: 149 Training 385 Loss: 0.306 | Acc: 89.492% (44216/49408)\n",
      "Epoch: 149 Training 386 Loss: 0.307 | Acc: 89.478% (44324/49536)\n",
      "Epoch: 149 Training 387 Loss: 0.307 | Acc: 89.463% (44431/49664)\n",
      "Epoch: 149 Training 388 Loss: 0.308 | Acc: 89.446% (44537/49792)\n",
      "Epoch: 149 Training 389 Loss: 0.308 | Acc: 89.443% (44650/49920)\n",
      "Epoch: 149 Training 390 Loss: 0.308 | Acc: 89.442% (44721/50000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ebe901d832413f923581a2619fed62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 0 Loss: 120.287 | Acc: 83.600% (209/250)\n",
      "Testing 1 Loss: 60.144 | Acc: 84.200% (421/500)\n",
      "Testing 2 Loss: 40.096 | Acc: 85.733% (643/750)\n",
      "Testing 3 Loss: 30.072 | Acc: 84.800% (848/1000)\n",
      "Testing 4 Loss: 24.057 | Acc: 83.520% (1044/1250)\n",
      "Testing 5 Loss: 20.048 | Acc: 83.667% (1255/1500)\n",
      "Testing 6 Loss: 17.184 | Acc: 83.829% (1467/1750)\n",
      "Testing 7 Loss: 15.036 | Acc: 83.500% (1670/2000)\n",
      "Testing 8 Loss: 13.365 | Acc: 83.600% (1881/2250)\n",
      "Testing 9 Loss: 12.029 | Acc: 83.640% (2091/2500)\n",
      "Testing 10 Loss: 10.935 | Acc: 83.636% (2300/2750)\n",
      "Testing 11 Loss: 10.024 | Acc: 83.367% (2501/3000)\n",
      "Testing 12 Loss: 9.253 | Acc: 83.108% (2701/3250)\n",
      "Testing 13 Loss: 8.592 | Acc: 83.200% (2912/3500)\n",
      "Testing 14 Loss: 8.019 | Acc: 83.280% (3123/3750)\n",
      "Testing 15 Loss: 7.518 | Acc: 83.225% (3329/4000)\n",
      "Testing 16 Loss: 7.076 | Acc: 83.294% (3540/4250)\n",
      "Testing 17 Loss: 6.683 | Acc: 83.511% (3758/4500)\n",
      "Testing 18 Loss: 6.331 | Acc: 83.432% (3963/4750)\n",
      "Testing 19 Loss: 6.014 | Acc: 83.520% (4176/5000)\n",
      "Testing 20 Loss: 5.728 | Acc: 83.371% (4377/5250)\n",
      "Testing 21 Loss: 5.468 | Acc: 83.491% (4592/5500)\n",
      "Testing 22 Loss: 5.230 | Acc: 83.304% (4790/5750)\n",
      "Testing 23 Loss: 5.012 | Acc: 83.383% (5003/6000)\n",
      "Testing 24 Loss: 4.811 | Acc: 83.552% (5222/6250)\n",
      "Testing 25 Loss: 4.626 | Acc: 83.523% (5429/6500)\n",
      "Testing 26 Loss: 4.455 | Acc: 83.393% (5629/6750)\n",
      "Testing 27 Loss: 4.296 | Acc: 83.186% (5823/7000)\n",
      "Testing 28 Loss: 4.148 | Acc: 83.172% (6030/7250)\n",
      "Testing 29 Loss: 4.010 | Acc: 83.333% (6250/7500)\n",
      "Testing 30 Loss: 3.880 | Acc: 83.432% (6466/7750)\n",
      "Testing 31 Loss: 3.759 | Acc: 83.475% (6678/8000)\n",
      "Testing 32 Loss: 3.645 | Acc: 83.479% (6887/8250)\n",
      "Testing 33 Loss: 3.538 | Acc: 83.353% (7085/8500)\n",
      "Testing 34 Loss: 3.437 | Acc: 83.257% (7285/8750)\n",
      "Testing 35 Loss: 3.341 | Acc: 83.289% (7496/9000)\n",
      "Testing 36 Loss: 3.251 | Acc: 83.286% (7704/9250)\n",
      "Testing 37 Loss: 3.165 | Acc: 83.337% (7917/9500)\n",
      "Testing 38 Loss: 3.084 | Acc: 83.354% (8127/9750)\n",
      "Testing 39 Loss: 3.007 | Acc: 83.430% (8343/10000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for epoch in tqdm(range(num_epochs)):    num_epochs = 200\n",
    "for epoch in (pbar := tqdm(range(num_epochs))):\n",
    "    #print('Epoch ID', epoch)\n",
    "    #----------------------------------------------------------------------\n",
    "    # Training\n",
    "    #----------------------------------------------------------------------\n",
    "    pbar.set_description(f\"Training {epoch}\",refresh=True)\n",
    "    correct = 0; total = 0; train_loss = 0\n",
    "    net.train()\n",
    "    for batch_idx, (x, target) in enumerate(tqdm(train_loader, leave=False)):\n",
    "    #for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        #if batch_idx < 1:\n",
    "        optimizer.zero_grad()\n",
    "        x, target = Variable(x.cuda()), Variable(target.cuda())\n",
    "        all_W_kernels = optimizer.param_groups[1]['params']\n",
    "        all_G_kernels = optimizer_quant.param_groups[0]['params']\n",
    "        \n",
    "        for k_W, k_G in zip(all_W_kernels, all_G_kernels):\n",
    "            V = k_W.data\n",
    "            #print(type(V))\n",
    "            #####Binary Connect#########################\n",
    "            #k_G.data = quantize_bw(V)\n",
    "            ############################################\n",
    "            \n",
    "            ######Binary Relax##########################\n",
    "            if epoch<120:\n",
    "                #k_G.data = (eta*quantize_bw(V)+V)/(1+eta)\n",
    "                k_G.data = (eta*quantize(V,num_bits=quantize_nbits)+V)/(1+eta)\n",
    "                \n",
    "            else:\n",
    "                k_G.data = quantize(V, num_bits=quantize_nbits)\n",
    "            #############################################\n",
    "            \n",
    "            k_W.data, k_G.data = k_G.data, k_W.data\n",
    "            \n",
    "            \n",
    "        score = net(x)\n",
    "        loss = criterion(score, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        for k_W, k_G in zip(all_W_kernels, all_G_kernels):\n",
    "            k_W.data, k_G.data = k_G.data, k_W.data\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data\n",
    "        _, predicted = torch.max(score.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        pbar.write(f\"Epoch: {epoch} Training {batch_idx} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f}% ({correct}/{total})\")\n",
    "        \n",
    "    #----------------------------------------------------------------------\n",
    "    # Testing\n",
    "    #----------------------------------------------------------------------\n",
    "    pbar.set_description(f\"Testing {epoch}\",refresh=True)\n",
    "    test_loss = 0; correct = 0; total = 0\n",
    "    net.eval()\n",
    "    \n",
    "    for k_W, k_G in zip(all_W_kernels, all_G_kernels):\n",
    "        k_W.data, k_G.data = k_G.data, k_W.data\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, target) in enumerate(tqdm(test_loader, leave=False)):\n",
    "        #for batch_idx, (x, target) in enumerate(test_loader):\n",
    "            x, target = Variable(x.cuda()), Variable(target.cuda())\n",
    "            score= net(x)\n",
    "            \n",
    "            loss = criterion(score, target)\n",
    "            test_loss += loss.data\n",
    "            _, predicted = torch.max(score.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target.data).cpu().sum()\n",
    "            pbar.write(f\"Testing {batch_idx} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f}% ({correct}/{total})\")\n",
    "\n",
    "    \n",
    "    #----------------------------------------------------------------------\n",
    "    # Save the checkpoint\n",
    "    #----------------------------------------------------------------------\n",
    "    '''\n",
    "    for i in range(len(all_W_kernels)):\n",
    "        k_W = all_W_kernels[i]\n",
    "        k_quant = all_G_kernels[i]    \n",
    "        k_W.data, k_quant.data = k_quant.data, k_W.data\n",
    "        '''  \n",
    "    acc = 100.*correct/total\n",
    "    #if acc > best_acc:\n",
    "    if correct > best_count:\n",
    "        # print('Saving model...')\n",
    "        # state = {\n",
    "        #     'state': net.state_dict(), #net,\n",
    "        #     'acc': acc,\n",
    "        #     'epoch': epoch,\n",
    "        # }\n",
    "        \n",
    "        # torch.save(state, f'./saved_models/resnet_{quantize_nbits}bits_{u.time_stamp()}.pth')\n",
    "        #net.save_state_dict('resnet20.pt')\n",
    "        best_acc = acc\n",
    "        best_count = correct\n",
    "\n",
    "    for k_W, k_G in zip(all_W_kernels, all_G_kernels):\n",
    "        k_W.data, k_G.data = k_G.data, k_W.data\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "save_model(net, \"saved_models\", f'resnet_{quantize_nbits}bits_{u.time_stamp()}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
