{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import fedlern.utils as u\n",
    "from fedlern.train_utils import *\n",
    "from fedlern.quant_utils import *\n",
    "from fedlern.models.resnet_v2 import ResNet18\n",
    "from fedlern.quantize import quantize\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "stats = (0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784)\n",
    "batch_size_test = 250\n",
    "batch_size_train = 128\n",
    "\n",
    "quantize_nbits = 4\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "eta_rate = 1.05\n",
    "eta = 1\n",
    "global best_acc\n",
    "best_acc = 0\n",
    "best_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = prepare_dataloader_cifar(num_workers=8, \n",
    "                                                     train_batch_size=batch_size_train, \n",
    "                                                     eval_batch_size=batch_size_test, \n",
    "                                                     stats=stats)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ResNet18()\n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, criterion, optimizer = get_model2(net, learning_rate=0.1, weight_decay=5e-4)\n",
    "\n",
    "    \n",
    "all_G_kernels = [\n",
    "    Variable(kernel.data.clone(), requires_grad=True)\n",
    "    for kernel in optimizer.param_groups[1]['params']\n",
    "]\n",
    "\n",
    "\n",
    "all_W_kernels = [kernel for kernel in optimizer.param_groups[1]['params']]\n",
    "kernels = [{'params': all_G_kernels}]\n",
    "optimizer_quant = optim.SGD(kernels, lr=0)\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80,120,160], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dcfc58fa7c6452aafdfd80f16e01f07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 0 Loss: 0.374 | Acc: 85.938% (110/128)\n",
      "Training 1 Loss: 0.380 | Acc: 86.719% (222/256)\n",
      "Training 2 Loss: 0.375 | Acc: 86.198% (331/384)\n",
      "Training 3 Loss: 0.368 | Acc: 86.914% (445/512)\n",
      "Training 4 Loss: 0.377 | Acc: 87.031% (557/640)\n",
      "Training 5 Loss: 0.366 | Acc: 87.630% (673/768)\n",
      "Training 6 Loss: 0.358 | Acc: 88.058% (789/896)\n",
      "Training 7 Loss: 0.354 | Acc: 88.184% (903/1024)\n",
      "Training 8 Loss: 0.351 | Acc: 88.281% (1017/1152)\n",
      "Training 9 Loss: 0.343 | Acc: 88.359% (1131/1280)\n",
      "Training 10 Loss: 0.346 | Acc: 88.068% (1240/1408)\n",
      "Training 11 Loss: 0.354 | Acc: 87.695% (1347/1536)\n",
      "Training 12 Loss: 0.354 | Acc: 88.041% (1465/1664)\n",
      "Training 13 Loss: 0.350 | Acc: 88.114% (1579/1792)\n",
      "Training 14 Loss: 0.354 | Acc: 88.229% (1694/1920)\n",
      "Training 15 Loss: 0.350 | Acc: 88.428% (1811/2048)\n",
      "Training 16 Loss: 0.352 | Acc: 88.097% (1917/2176)\n",
      "Training 17 Loss: 0.351 | Acc: 88.064% (2029/2304)\n",
      "Training 18 Loss: 0.361 | Acc: 87.788% (2135/2432)\n",
      "Training 19 Loss: 0.363 | Acc: 87.734% (2246/2560)\n",
      "Training 20 Loss: 0.365 | Acc: 87.760% (2359/2688)\n",
      "Training 21 Loss: 0.367 | Acc: 87.607% (2467/2816)\n",
      "Training 22 Loss: 0.375 | Acc: 87.364% (2572/2944)\n",
      "Training 23 Loss: 0.374 | Acc: 87.500% (2688/3072)\n",
      "Training 24 Loss: 0.377 | Acc: 87.438% (2798/3200)\n",
      "Training 25 Loss: 0.386 | Acc: 87.169% (2901/3328)\n",
      "Training 26 Loss: 0.388 | Acc: 87.037% (3008/3456)\n",
      "Training 27 Loss: 0.387 | Acc: 87.081% (3121/3584)\n",
      "Training 28 Loss: 0.387 | Acc: 87.069% (3232/3712)\n",
      "Training 29 Loss: 0.390 | Acc: 86.979% (3340/3840)\n",
      "Training 30 Loss: 0.394 | Acc: 86.794% (3444/3968)\n",
      "Training 31 Loss: 0.396 | Acc: 86.646% (3549/4096)\n",
      "Training 32 Loss: 0.396 | Acc: 86.671% (3661/4224)\n",
      "Training 33 Loss: 0.401 | Acc: 86.489% (3764/4352)\n",
      "Training 34 Loss: 0.400 | Acc: 86.496% (3875/4480)\n",
      "Training 35 Loss: 0.400 | Acc: 86.458% (3984/4608)\n",
      "Training 36 Loss: 0.402 | Acc: 86.444% (4094/4736)\n",
      "Training 37 Loss: 0.401 | Acc: 86.451% (4205/4864)\n",
      "Training 38 Loss: 0.405 | Acc: 86.338% (4310/4992)\n",
      "Training 39 Loss: 0.406 | Acc: 86.289% (4418/5120)\n",
      "Training 40 Loss: 0.409 | Acc: 86.242% (4526/5248)\n",
      "Training 41 Loss: 0.409 | Acc: 86.254% (4637/5376)\n",
      "Training 42 Loss: 0.409 | Acc: 86.210% (4745/5504)\n",
      "Training 43 Loss: 0.407 | Acc: 86.310% (4861/5632)\n",
      "Training 44 Loss: 0.408 | Acc: 86.354% (4974/5760)\n",
      "Training 45 Loss: 0.409 | Acc: 86.311% (5082/5888)\n",
      "Training 46 Loss: 0.407 | Acc: 86.370% (5196/6016)\n",
      "Training 47 Loss: 0.408 | Acc: 86.328% (5304/6144)\n",
      "Training 48 Loss: 0.407 | Acc: 86.320% (5414/6272)\n",
      "Training 49 Loss: 0.407 | Acc: 86.359% (5527/6400)\n",
      "Training 50 Loss: 0.406 | Acc: 86.336% (5636/6528)\n",
      "Training 51 Loss: 0.406 | Acc: 86.373% (5749/6656)\n",
      "Training 52 Loss: 0.405 | Acc: 86.394% (5861/6784)\n",
      "Training 53 Loss: 0.406 | Acc: 86.400% (5972/6912)\n",
      "Training 54 Loss: 0.409 | Acc: 86.349% (6079/7040)\n",
      "Training 55 Loss: 0.408 | Acc: 86.342% (6189/7168)\n",
      "Training 56 Loss: 0.410 | Acc: 86.266% (6294/7296)\n",
      "Training 57 Loss: 0.407 | Acc: 86.395% (6414/7424)\n",
      "Training 58 Loss: 0.405 | Acc: 86.480% (6531/7552)\n",
      "Training 59 Loss: 0.404 | Acc: 86.549% (6647/7680)\n",
      "Training 60 Loss: 0.405 | Acc: 86.565% (6759/7808)\n",
      "Training 61 Loss: 0.405 | Acc: 86.555% (6869/7936)\n",
      "Training 62 Loss: 0.405 | Acc: 86.520% (6977/8064)\n",
      "Training 63 Loss: 0.409 | Acc: 86.438% (7081/8192)\n",
      "Training 64 Loss: 0.406 | Acc: 86.526% (7199/8320)\n",
      "Training 65 Loss: 0.407 | Acc: 86.482% (7306/8448)\n",
      "Training 66 Loss: 0.405 | Acc: 86.567% (7424/8576)\n",
      "Training 67 Loss: 0.404 | Acc: 86.604% (7538/8704)\n",
      "Training 68 Loss: 0.403 | Acc: 86.639% (7652/8832)\n",
      "Training 69 Loss: 0.402 | Acc: 86.663% (7765/8960)\n",
      "Training 70 Loss: 0.403 | Acc: 86.598% (7870/9088)\n",
      "Training 71 Loss: 0.402 | Acc: 86.664% (7987/9216)\n",
      "Training 72 Loss: 0.400 | Acc: 86.719% (8103/9344)\n",
      "Training 73 Loss: 0.399 | Acc: 86.793% (8221/9472)\n",
      "Training 74 Loss: 0.397 | Acc: 86.833% (8336/9600)\n",
      "Training 75 Loss: 0.397 | Acc: 86.822% (8446/9728)\n",
      "Training 76 Loss: 0.397 | Acc: 86.830% (8558/9856)\n",
      "Training 77 Loss: 0.396 | Acc: 86.879% (8674/9984)\n",
      "Training 78 Loss: 0.394 | Acc: 86.976% (8795/10112)\n",
      "Training 79 Loss: 0.394 | Acc: 86.943% (8903/10240)\n",
      "Training 80 Loss: 0.394 | Acc: 86.931% (9013/10368)\n",
      "Training 81 Loss: 0.393 | Acc: 86.947% (9126/10496)\n",
      "Training 82 Loss: 0.392 | Acc: 86.982% (9241/10624)\n",
      "Training 83 Loss: 0.392 | Acc: 86.970% (9351/10752)\n",
      "Training 84 Loss: 0.392 | Acc: 86.939% (9459/10880)\n",
      "Training 85 Loss: 0.392 | Acc: 86.955% (9572/11008)\n",
      "Training 86 Loss: 0.392 | Acc: 86.979% (9686/11136)\n",
      "Training 87 Loss: 0.391 | Acc: 86.994% (9799/11264)\n",
      "Training 88 Loss: 0.390 | Acc: 87.017% (9913/11392)\n",
      "Training 89 Loss: 0.391 | Acc: 86.988% (10021/11520)\n",
      "Training 90 Loss: 0.390 | Acc: 87.028% (10137/11648)\n",
      "Training 91 Loss: 0.390 | Acc: 87.007% (10246/11776)\n",
      "Training 92 Loss: 0.389 | Acc: 87.030% (10360/11904)\n",
      "Training 93 Loss: 0.389 | Acc: 87.018% (10470/12032)\n",
      "Training 94 Loss: 0.390 | Acc: 86.990% (10578/12160)\n",
      "Training 95 Loss: 0.390 | Acc: 86.955% (10685/12288)\n",
      "Training 96 Loss: 0.390 | Acc: 86.936% (10794/12416)\n",
      "Training 97 Loss: 0.391 | Acc: 86.934% (10905/12544)\n",
      "Training 98 Loss: 0.390 | Acc: 86.948% (11018/12672)\n",
      "Training 99 Loss: 0.388 | Acc: 87.039% (11141/12800)\n",
      "Training 100 Loss: 0.387 | Acc: 87.067% (11256/12928)\n",
      "Training 101 Loss: 0.388 | Acc: 87.040% (11364/13056)\n",
      "Training 102 Loss: 0.387 | Acc: 87.060% (11478/13184)\n",
      "Training 103 Loss: 0.387 | Acc: 87.027% (11585/13312)\n",
      "Training 104 Loss: 0.386 | Acc: 87.046% (11699/13440)\n",
      "Training 105 Loss: 0.385 | Acc: 87.087% (11816/13568)\n",
      "Training 106 Loss: 0.385 | Acc: 87.062% (11924/13696)\n",
      "Training 107 Loss: 0.386 | Acc: 87.059% (12035/13824)\n",
      "Training 108 Loss: 0.385 | Acc: 87.070% (12148/13952)\n",
      "Training 109 Loss: 0.384 | Acc: 87.095% (12263/14080)\n",
      "Training 110 Loss: 0.384 | Acc: 87.113% (12377/14208)\n",
      "Training 111 Loss: 0.382 | Acc: 87.165% (12496/14336)\n",
      "Training 112 Loss: 0.381 | Acc: 87.189% (12611/14464)\n",
      "Training 113 Loss: 0.380 | Acc: 87.205% (12725/14592)\n",
      "Training 114 Loss: 0.380 | Acc: 87.208% (12837/14720)\n",
      "Training 115 Loss: 0.381 | Acc: 87.183% (12945/14848)\n",
      "Training 116 Loss: 0.380 | Acc: 87.186% (13057/14976)\n",
      "Training 117 Loss: 0.380 | Acc: 87.195% (13170/15104)\n",
      "Training 118 Loss: 0.380 | Acc: 87.185% (13280/15232)\n",
      "Training 119 Loss: 0.380 | Acc: 87.188% (13392/15360)\n",
      "Training 120 Loss: 0.379 | Acc: 87.216% (13508/15488)\n",
      "Training 121 Loss: 0.378 | Acc: 87.212% (13619/15616)\n",
      "Training 122 Loss: 0.378 | Acc: 87.201% (13729/15744)\n",
      "Training 123 Loss: 0.377 | Acc: 87.242% (13847/15872)\n",
      "Training 124 Loss: 0.376 | Acc: 87.287% (13966/16000)\n",
      "Training 125 Loss: 0.375 | Acc: 87.320% (14083/16128)\n",
      "Training 126 Loss: 0.375 | Acc: 87.303% (14192/16256)\n",
      "Training 127 Loss: 0.374 | Acc: 87.305% (14304/16384)\n",
      "Training 128 Loss: 0.375 | Acc: 87.306% (14416/16512)\n",
      "Training 129 Loss: 0.375 | Acc: 87.290% (14525/16640)\n",
      "Training 130 Loss: 0.375 | Acc: 87.327% (14643/16768)\n",
      "Training 131 Loss: 0.374 | Acc: 87.358% (14760/16896)\n",
      "Training 132 Loss: 0.375 | Acc: 87.336% (14868/17024)\n",
      "Training 133 Loss: 0.374 | Acc: 87.366% (14985/17152)\n",
      "Training 134 Loss: 0.376 | Acc: 87.326% (15090/17280)\n",
      "Training 135 Loss: 0.376 | Acc: 87.322% (15201/17408)\n",
      "Training 136 Loss: 0.376 | Acc: 87.295% (15308/17536)\n",
      "Training 137 Loss: 0.376 | Acc: 87.274% (15416/17664)\n",
      "Training 138 Loss: 0.376 | Acc: 87.264% (15526/17792)\n",
      "Training 139 Loss: 0.376 | Acc: 87.266% (15638/17920)\n",
      "Training 140 Loss: 0.376 | Acc: 87.278% (15752/18048)\n",
      "Training 141 Loss: 0.376 | Acc: 87.296% (15867/18176)\n",
      "Training 142 Loss: 0.376 | Acc: 87.298% (15979/18304)\n",
      "Training 143 Loss: 0.375 | Acc: 87.305% (16092/18432)\n",
      "Training 144 Loss: 0.375 | Acc: 87.274% (16198/18560)\n",
      "Training 145 Loss: 0.376 | Acc: 87.259% (16307/18688)\n",
      "Training 146 Loss: 0.376 | Acc: 87.266% (16420/18816)\n",
      "Training 147 Loss: 0.375 | Acc: 87.284% (16535/18944)\n",
      "Training 148 Loss: 0.376 | Acc: 87.275% (16645/19072)\n",
      "Training 149 Loss: 0.375 | Acc: 87.281% (16758/19200)\n",
      "Training 150 Loss: 0.375 | Acc: 87.262% (16866/19328)\n",
      "Training 151 Loss: 0.376 | Acc: 87.258% (16977/19456)\n",
      "Training 152 Loss: 0.376 | Acc: 87.250% (17087/19584)\n",
      "Training 153 Loss: 0.375 | Acc: 87.272% (17203/19712)\n",
      "Training 154 Loss: 0.376 | Acc: 87.248% (17310/19840)\n",
      "Training 155 Loss: 0.376 | Acc: 87.225% (17417/19968)\n",
      "Training 156 Loss: 0.377 | Acc: 87.206% (17525/20096)\n",
      "Training 157 Loss: 0.376 | Acc: 87.228% (17641/20224)\n",
      "Training 158 Loss: 0.376 | Acc: 87.225% (17752/20352)\n",
      "Training 159 Loss: 0.375 | Acc: 87.246% (17868/20480)\n",
      "Training 160 Loss: 0.376 | Acc: 87.233% (17977/20608)\n",
      "Training 161 Loss: 0.378 | Acc: 87.201% (18082/20736)\n",
      "Training 162 Loss: 0.378 | Acc: 87.188% (18191/20864)\n",
      "Training 163 Loss: 0.378 | Acc: 87.167% (18298/20992)\n",
      "Training 164 Loss: 0.379 | Acc: 87.098% (18395/21120)\n",
      "Training 165 Loss: 0.379 | Acc: 87.105% (18508/21248)\n",
      "Training 166 Loss: 0.380 | Acc: 87.093% (18617/21376)\n",
      "Training 167 Loss: 0.381 | Acc: 87.044% (18718/21504)\n",
      "Training 168 Loss: 0.381 | Acc: 87.061% (18833/21632)\n",
      "Training 169 Loss: 0.381 | Acc: 87.036% (18939/21760)\n",
      "Training 170 Loss: 0.381 | Acc: 87.034% (19050/21888)\n",
      "Training 171 Loss: 0.382 | Acc: 87.014% (19157/22016)\n",
      "Training 172 Loss: 0.382 | Acc: 87.021% (19270/22144)\n",
      "Training 173 Loss: 0.382 | Acc: 86.993% (19375/22272)\n",
      "Training 174 Loss: 0.382 | Acc: 86.996% (19487/22400)\n",
      "Training 175 Loss: 0.383 | Acc: 86.981% (19595/22528)\n",
      "Training 176 Loss: 0.382 | Acc: 86.992% (19709/22656)\n",
      "Training 177 Loss: 0.383 | Acc: 86.978% (19817/22784)\n",
      "Training 178 Loss: 0.382 | Acc: 86.985% (19930/22912)\n",
      "Training 179 Loss: 0.382 | Acc: 86.992% (20043/23040)\n",
      "Training 180 Loss: 0.383 | Acc: 86.973% (20150/23168)\n",
      "Training 181 Loss: 0.382 | Acc: 87.002% (20268/23296)\n",
      "Training 182 Loss: 0.382 | Acc: 86.996% (20378/23424)\n",
      "Training 183 Loss: 0.382 | Acc: 87.003% (20491/23552)\n",
      "Training 184 Loss: 0.382 | Acc: 87.010% (20604/23680)\n",
      "Training 185 Loss: 0.382 | Acc: 86.996% (20712/23808)\n",
      "Training 186 Loss: 0.382 | Acc: 86.994% (20823/23936)\n",
      "Training 187 Loss: 0.382 | Acc: 87.001% (20936/24064)\n",
      "Training 188 Loss: 0.381 | Acc: 87.008% (21049/24192)\n",
      "Training 189 Loss: 0.381 | Acc: 87.002% (21159/24320)\n",
      "Training 190 Loss: 0.381 | Acc: 86.997% (21269/24448)\n",
      "Training 191 Loss: 0.380 | Acc: 87.028% (21388/24576)\n",
      "Training 192 Loss: 0.380 | Acc: 87.034% (21501/24704)\n",
      "Training 193 Loss: 0.380 | Acc: 87.041% (21614/24832)\n",
      "Training 194 Loss: 0.380 | Acc: 87.031% (21723/24960)\n",
      "Training 195 Loss: 0.380 | Acc: 87.010% (21829/25088)\n",
      "Training 196 Loss: 0.380 | Acc: 87.000% (21938/25216)\n",
      "Training 197 Loss: 0.379 | Acc: 87.015% (22053/25344)\n",
      "Training 198 Loss: 0.379 | Acc: 87.017% (22165/25472)\n",
      "Training 199 Loss: 0.379 | Acc: 87.023% (22278/25600)\n",
      "Training 200 Loss: 0.379 | Acc: 87.041% (22394/25728)\n",
      "Training 201 Loss: 0.378 | Acc: 87.051% (22508/25856)\n",
      "Training 202 Loss: 0.378 | Acc: 87.057% (22621/25984)\n",
      "Training 203 Loss: 0.378 | Acc: 87.048% (22730/26112)\n",
      "Training 204 Loss: 0.378 | Acc: 87.035% (22838/26240)\n",
      "Training 205 Loss: 0.379 | Acc: 87.049% (22953/26368)\n",
      "Training 206 Loss: 0.378 | Acc: 87.066% (23069/26496)\n",
      "Training 207 Loss: 0.378 | Acc: 87.057% (23178/26624)\n",
      "Training 208 Loss: 0.378 | Acc: 87.070% (23293/26752)\n",
      "Training 209 Loss: 0.378 | Acc: 87.061% (23402/26880)\n",
      "Training 210 Loss: 0.378 | Acc: 87.056% (23512/27008)\n",
      "Training 211 Loss: 0.378 | Acc: 87.069% (23627/27136)\n",
      "Training 212 Loss: 0.378 | Acc: 87.060% (23736/27264)\n",
      "Training 213 Loss: 0.377 | Acc: 87.080% (23853/27392)\n",
      "Training 214 Loss: 0.377 | Acc: 87.097% (23969/27520)\n",
      "Training 215 Loss: 0.377 | Acc: 87.102% (24082/27648)\n",
      "Training 216 Loss: 0.377 | Acc: 87.086% (24189/27776)\n",
      "Training 217 Loss: 0.377 | Acc: 87.099% (24304/27904)\n",
      "Training 218 Loss: 0.376 | Acc: 87.115% (24420/28032)\n",
      "Training 219 Loss: 0.375 | Acc: 87.141% (24539/28160)\n",
      "Training 220 Loss: 0.375 | Acc: 87.150% (24653/28288)\n",
      "Training 221 Loss: 0.375 | Acc: 87.141% (24762/28416)\n",
      "Training 222 Loss: 0.376 | Acc: 87.101% (24862/28544)\n",
      "Training 223 Loss: 0.376 | Acc: 87.095% (24972/28672)\n",
      "Training 224 Loss: 0.376 | Acc: 87.087% (25081/28800)\n",
      "Training 225 Loss: 0.376 | Acc: 87.075% (25189/28928)\n",
      "Training 226 Loss: 0.376 | Acc: 87.080% (25302/29056)\n",
      "Training 227 Loss: 0.377 | Acc: 87.061% (25408/29184)\n",
      "Training 228 Loss: 0.377 | Acc: 87.043% (25514/29312)\n",
      "Training 229 Loss: 0.377 | Acc: 87.045% (25626/29440)\n",
      "Training 230 Loss: 0.378 | Acc: 87.023% (25731/29568)\n",
      "Training 231 Loss: 0.377 | Acc: 87.039% (25847/29696)\n",
      "Training 232 Loss: 0.377 | Acc: 87.034% (25957/29824)\n",
      "Training 233 Loss: 0.377 | Acc: 87.019% (26064/29952)\n",
      "Training 234 Loss: 0.377 | Acc: 87.011% (26173/30080)\n",
      "Training 235 Loss: 0.378 | Acc: 87.003% (26282/30208)\n",
      "Training 236 Loss: 0.378 | Acc: 86.996% (26391/30336)\n",
      "Training 237 Loss: 0.377 | Acc: 87.021% (26510/30464)\n",
      "Training 238 Loss: 0.378 | Acc: 86.997% (26614/30592)\n",
      "Training 239 Loss: 0.378 | Acc: 87.002% (26727/30720)\n",
      "Training 240 Loss: 0.379 | Acc: 86.962% (26826/30848)\n",
      "Training 241 Loss: 0.379 | Acc: 86.958% (26936/30976)\n",
      "Training 242 Loss: 0.379 | Acc: 86.963% (27049/31104)\n",
      "Training 243 Loss: 0.379 | Acc: 86.968% (27162/31232)\n",
      "Training 244 Loss: 0.379 | Acc: 86.964% (27272/31360)\n",
      "Training 245 Loss: 0.378 | Acc: 86.982% (27389/31488)\n",
      "Training 246 Loss: 0.379 | Acc: 86.975% (27498/31616)\n",
      "Training 247 Loss: 0.379 | Acc: 86.974% (27609/31744)\n",
      "Training 248 Loss: 0.379 | Acc: 86.967% (27718/31872)\n",
      "Training 249 Loss: 0.380 | Acc: 86.953% (27825/32000)\n",
      "Training 250 Loss: 0.380 | Acc: 86.946% (27934/32128)\n",
      "Training 251 Loss: 0.380 | Acc: 86.930% (28040/32256)\n",
      "Training 252 Loss: 0.381 | Acc: 86.916% (28147/32384)\n",
      "Training 253 Loss: 0.380 | Acc: 86.925% (28261/32512)\n",
      "Training 254 Loss: 0.380 | Acc: 86.918% (28370/32640)\n",
      "Training 255 Loss: 0.380 | Acc: 86.929% (28485/32768)\n",
      "Training 256 Loss: 0.380 | Acc: 86.929% (28596/32896)\n",
      "Training 257 Loss: 0.380 | Acc: 86.928% (28707/33024)\n",
      "Training 258 Loss: 0.380 | Acc: 86.942% (28823/33152)\n",
      "Training 259 Loss: 0.380 | Acc: 86.926% (28929/33280)\n",
      "Training 260 Loss: 0.380 | Acc: 86.934% (29043/33408)\n",
      "Training 261 Loss: 0.380 | Acc: 86.927% (29152/33536)\n",
      "Training 262 Loss: 0.380 | Acc: 86.939% (29267/33664)\n",
      "Training 263 Loss: 0.380 | Acc: 86.929% (29375/33792)\n",
      "Training 264 Loss: 0.380 | Acc: 86.934% (29488/33920)\n",
      "Training 265 Loss: 0.380 | Acc: 86.930% (29598/34048)\n",
      "Training 266 Loss: 0.380 | Acc: 86.918% (29705/34176)\n",
      "Training 267 Loss: 0.380 | Acc: 86.926% (29819/34304)\n",
      "Training 268 Loss: 0.380 | Acc: 86.925% (29930/34432)\n",
      "Training 269 Loss: 0.380 | Acc: 86.933% (30044/34560)\n",
      "Training 270 Loss: 0.380 | Acc: 86.946% (30160/34688)\n",
      "Training 271 Loss: 0.379 | Acc: 86.954% (30274/34816)\n",
      "Training 272 Loss: 0.380 | Acc: 86.956% (30386/34944)\n",
      "Training 273 Loss: 0.379 | Acc: 86.958% (30498/35072)\n",
      "Training 274 Loss: 0.379 | Acc: 86.957% (30609/35200)\n",
      "Training 275 Loss: 0.379 | Acc: 86.959% (30721/35328)\n",
      "Training 276 Loss: 0.379 | Acc: 86.961% (30833/35456)\n",
      "Training 277 Loss: 0.379 | Acc: 86.969% (30947/35584)\n",
      "Training 278 Loss: 0.379 | Acc: 86.976% (31061/35712)\n",
      "Training 279 Loss: 0.379 | Acc: 86.995% (31179/35840)\n",
      "Training 280 Loss: 0.379 | Acc: 87.002% (31293/35968)\n",
      "Training 281 Loss: 0.378 | Acc: 87.007% (31406/36096)\n",
      "Training 282 Loss: 0.379 | Acc: 87.000% (31515/36224)\n",
      "Training 283 Loss: 0.378 | Acc: 87.010% (31630/36352)\n",
      "Training 284 Loss: 0.378 | Acc: 87.018% (31744/36480)\n",
      "Training 285 Loss: 0.379 | Acc: 87.011% (31853/36608)\n",
      "Training 286 Loss: 0.379 | Acc: 87.015% (31966/36736)\n",
      "Training 287 Loss: 0.379 | Acc: 87.012% (32076/36864)\n",
      "Training 288 Loss: 0.379 | Acc: 86.997% (32182/36992)\n",
      "Training 289 Loss: 0.379 | Acc: 86.999% (32294/37120)\n",
      "Training 290 Loss: 0.379 | Acc: 87.014% (32411/37248)\n",
      "Training 291 Loss: 0.379 | Acc: 87.005% (32519/37376)\n",
      "Training 292 Loss: 0.379 | Acc: 86.996% (32627/37504)\n",
      "Training 293 Loss: 0.379 | Acc: 86.990% (32736/37632)\n",
      "Training 294 Loss: 0.379 | Acc: 86.989% (32847/37760)\n",
      "Training 295 Loss: 0.379 | Acc: 86.983% (32956/37888)\n",
      "Training 296 Loss: 0.379 | Acc: 87.005% (33076/38016)\n",
      "Training 297 Loss: 0.379 | Acc: 87.020% (33193/38144)\n",
      "Training 298 Loss: 0.379 | Acc: 87.019% (33304/38272)\n",
      "Training 299 Loss: 0.378 | Acc: 87.021% (33416/38400)\n",
      "Training 300 Loss: 0.378 | Acc: 87.020% (33527/38528)\n",
      "Training 301 Loss: 0.378 | Acc: 87.027% (33641/38656)\n",
      "Training 302 Loss: 0.378 | Acc: 87.018% (33749/38784)\n",
      "Training 303 Loss: 0.378 | Acc: 87.030% (33865/38912)\n",
      "Training 304 Loss: 0.378 | Acc: 87.047% (33983/39040)\n",
      "Training 305 Loss: 0.377 | Acc: 87.043% (34093/39168)\n",
      "Training 306 Loss: 0.378 | Acc: 87.042% (34204/39296)\n",
      "Training 307 Loss: 0.378 | Acc: 87.041% (34315/39424)\n",
      "Training 308 Loss: 0.377 | Acc: 87.047% (34429/39552)\n",
      "Training 309 Loss: 0.377 | Acc: 87.046% (34540/39680)\n",
      "Training 310 Loss: 0.378 | Acc: 87.045% (34651/39808)\n",
      "Training 311 Loss: 0.378 | Acc: 87.042% (34761/39936)\n",
      "Training 312 Loss: 0.378 | Acc: 87.056% (34878/40064)\n",
      "Training 313 Loss: 0.377 | Acc: 87.067% (34994/40192)\n",
      "Training 314 Loss: 0.377 | Acc: 87.071% (35107/40320)\n",
      "Training 315 Loss: 0.377 | Acc: 87.062% (35215/40448)\n",
      "Training 316 Loss: 0.377 | Acc: 87.074% (35331/40576)\n",
      "Training 317 Loss: 0.377 | Acc: 87.080% (35445/40704)\n",
      "Training 318 Loss: 0.377 | Acc: 87.093% (35562/40832)\n",
      "Training 319 Loss: 0.377 | Acc: 87.095% (35674/40960)\n",
      "Training 320 Loss: 0.376 | Acc: 87.108% (35791/41088)\n",
      "Training 321 Loss: 0.377 | Acc: 87.092% (35896/41216)\n",
      "Training 322 Loss: 0.376 | Acc: 87.108% (36014/41344)\n",
      "Training 323 Loss: 0.376 | Acc: 87.121% (36131/41472)\n",
      "Training 324 Loss: 0.376 | Acc: 87.120% (36242/41600)\n",
      "Training 325 Loss: 0.376 | Acc: 87.129% (36357/41728)\n",
      "Training 326 Loss: 0.376 | Acc: 87.120% (36465/41856)\n",
      "Training 327 Loss: 0.376 | Acc: 87.121% (36577/41984)\n",
      "Training 328 Loss: 0.376 | Acc: 87.134% (36694/42112)\n",
      "Training 329 Loss: 0.375 | Acc: 87.143% (36809/42240)\n",
      "Training 330 Loss: 0.375 | Acc: 87.134% (36917/42368)\n",
      "Training 331 Loss: 0.375 | Acc: 87.123% (37024/42496)\n",
      "Training 332 Loss: 0.375 | Acc: 87.127% (37137/42624)\n",
      "Training 333 Loss: 0.375 | Acc: 87.142% (37255/42752)\n",
      "Training 334 Loss: 0.375 | Acc: 87.136% (37364/42880)\n",
      "Training 335 Loss: 0.375 | Acc: 87.144% (37479/43008)\n",
      "Training 336 Loss: 0.376 | Acc: 87.138% (37588/43136)\n",
      "Training 337 Loss: 0.376 | Acc: 87.130% (37696/43264)\n",
      "Training 338 Loss: 0.376 | Acc: 87.131% (37808/43392)\n",
      "Training 339 Loss: 0.375 | Acc: 87.130% (37919/43520)\n",
      "Training 340 Loss: 0.376 | Acc: 87.122% (38027/43648)\n",
      "Training 341 Loss: 0.376 | Acc: 87.128% (38141/43776)\n",
      "Training 342 Loss: 0.375 | Acc: 87.142% (38259/43904)\n",
      "Training 343 Loss: 0.375 | Acc: 87.143% (38371/44032)\n",
      "Training 344 Loss: 0.376 | Acc: 87.133% (38478/44160)\n",
      "Training 345 Loss: 0.376 | Acc: 87.141% (38593/44288)\n",
      "Training 346 Loss: 0.376 | Acc: 87.142% (38705/44416)\n",
      "Training 347 Loss: 0.376 | Acc: 87.141% (38816/44544)\n",
      "Training 348 Loss: 0.376 | Acc: 87.137% (38926/44672)\n",
      "Training 349 Loss: 0.376 | Acc: 87.143% (39040/44800)\n",
      "Training 350 Loss: 0.376 | Acc: 87.157% (39158/44928)\n",
      "Training 351 Loss: 0.376 | Acc: 87.165% (39273/45056)\n",
      "Training 352 Loss: 0.376 | Acc: 87.168% (39386/45184)\n",
      "Training 353 Loss: 0.376 | Acc: 87.173% (39500/45312)\n",
      "Training 354 Loss: 0.376 | Acc: 87.179% (39614/45440)\n",
      "Training 355 Loss: 0.376 | Acc: 87.169% (39721/45568)\n",
      "Training 356 Loss: 0.376 | Acc: 87.170% (39833/45696)\n",
      "Training 357 Loss: 0.375 | Acc: 87.181% (39950/45824)\n",
      "Training 358 Loss: 0.375 | Acc: 87.182% (40062/45952)\n",
      "Training 359 Loss: 0.375 | Acc: 87.196% (40180/46080)\n",
      "Training 360 Loss: 0.375 | Acc: 87.197% (40292/46208)\n",
      "Training 361 Loss: 0.375 | Acc: 87.194% (40402/46336)\n",
      "Training 362 Loss: 0.375 | Acc: 87.192% (40513/46464)\n",
      "Training 363 Loss: 0.375 | Acc: 87.187% (40622/46592)\n",
      "Training 364 Loss: 0.375 | Acc: 87.190% (40735/46720)\n",
      "Training 365 Loss: 0.375 | Acc: 87.190% (40847/46848)\n",
      "Training 366 Loss: 0.375 | Acc: 87.196% (40961/46976)\n",
      "Training 367 Loss: 0.375 | Acc: 87.184% (41067/47104)\n",
      "Training 368 Loss: 0.375 | Acc: 87.182% (41178/47232)\n",
      "Training 369 Loss: 0.375 | Acc: 87.185% (41291/47360)\n",
      "Training 370 Loss: 0.375 | Acc: 87.171% (41396/47488)\n",
      "Training 371 Loss: 0.375 | Acc: 87.185% (41514/47616)\n",
      "Training 372 Loss: 0.375 | Acc: 87.196% (41631/47744)\n",
      "Training 373 Loss: 0.375 | Acc: 87.189% (41739/47872)\n",
      "Training 374 Loss: 0.375 | Acc: 87.204% (41858/48000)\n",
      "Training 375 Loss: 0.375 | Acc: 87.199% (41967/48128)\n",
      "Training 376 Loss: 0.375 | Acc: 87.189% (42074/48256)\n",
      "Training 377 Loss: 0.375 | Acc: 87.180% (42181/48384)\n",
      "Training 378 Loss: 0.376 | Acc: 87.183% (42294/48512)\n",
      "Training 379 Loss: 0.376 | Acc: 87.173% (42401/48640)\n",
      "Training 380 Loss: 0.376 | Acc: 87.158% (42505/48768)\n",
      "Training 381 Loss: 0.376 | Acc: 87.146% (42611/48896)\n",
      "Training 382 Loss: 0.376 | Acc: 87.143% (42721/49024)\n",
      "Training 383 Loss: 0.376 | Acc: 87.140% (42831/49152)\n",
      "Training 384 Loss: 0.376 | Acc: 87.133% (42939/49280)\n",
      "Training 385 Loss: 0.376 | Acc: 87.132% (43050/49408)\n",
      "Training 386 Loss: 0.377 | Acc: 87.120% (43156/49536)\n",
      "Training 387 Loss: 0.377 | Acc: 87.127% (43271/49664)\n",
      "Training 388 Loss: 0.377 | Acc: 87.108% (43373/49792)\n",
      "Training 389 Loss: 0.377 | Acc: 87.123% (43492/49920)\n",
      "Training 390 Loss: 0.377 | Acc: 87.118% (43559/50000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc8e81a53ac41db8351c86421b6118c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 0 Loss: 147.301 | Acc: 77.200% (193/250)\n",
      "Testing 1 Loss: 73.651 | Acc: 77.400% (387/500)\n",
      "Testing 2 Loss: 49.100 | Acc: 78.933% (592/750)\n",
      "Testing 3 Loss: 36.825 | Acc: 79.200% (792/1000)\n",
      "Testing 4 Loss: 29.460 | Acc: 78.320% (979/1250)\n",
      "Testing 5 Loss: 24.550 | Acc: 78.533% (1178/1500)\n",
      "Testing 6 Loss: 21.043 | Acc: 79.029% (1383/1750)\n",
      "Testing 7 Loss: 18.413 | Acc: 79.050% (1581/2000)\n",
      "Testing 8 Loss: 16.367 | Acc: 78.756% (1772/2250)\n",
      "Testing 9 Loss: 14.730 | Acc: 78.680% (1967/2500)\n",
      "Testing 10 Loss: 13.391 | Acc: 78.945% (2171/2750)\n",
      "Testing 11 Loss: 12.275 | Acc: 79.133% (2374/3000)\n",
      "Testing 12 Loss: 11.331 | Acc: 79.231% (2575/3250)\n",
      "Testing 13 Loss: 10.522 | Acc: 79.171% (2771/3500)\n",
      "Testing 14 Loss: 9.820 | Acc: 79.333% (2975/3750)\n",
      "Testing 15 Loss: 9.206 | Acc: 79.675% (3187/4000)\n",
      "Testing 16 Loss: 8.665 | Acc: 79.788% (3391/4250)\n",
      "Testing 17 Loss: 8.183 | Acc: 79.711% (3587/4500)\n",
      "Testing 18 Loss: 7.753 | Acc: 79.537% (3778/4750)\n",
      "Testing 19 Loss: 7.365 | Acc: 79.640% (3982/5000)\n",
      "Testing 20 Loss: 7.014 | Acc: 79.524% (4175/5250)\n",
      "Testing 21 Loss: 6.696 | Acc: 79.600% (4378/5500)\n",
      "Testing 22 Loss: 6.404 | Acc: 79.600% (4577/5750)\n",
      "Testing 23 Loss: 6.138 | Acc: 79.667% (4780/6000)\n",
      "Testing 24 Loss: 5.892 | Acc: 79.744% (4984/6250)\n",
      "Testing 25 Loss: 5.665 | Acc: 79.831% (5189/6500)\n",
      "Testing 26 Loss: 5.456 | Acc: 79.867% (5391/6750)\n",
      "Testing 27 Loss: 5.261 | Acc: 79.729% (5581/7000)\n",
      "Testing 28 Loss: 5.079 | Acc: 79.890% (5792/7250)\n",
      "Testing 29 Loss: 4.910 | Acc: 79.893% (5992/7500)\n",
      "Testing 30 Loss: 4.752 | Acc: 79.897% (6192/7750)\n",
      "Testing 31 Loss: 4.603 | Acc: 79.900% (6392/8000)\n",
      "Testing 32 Loss: 4.464 | Acc: 79.891% (6591/8250)\n",
      "Testing 33 Loss: 4.332 | Acc: 79.824% (6785/8500)\n",
      "Testing 34 Loss: 4.209 | Acc: 79.737% (6977/8750)\n",
      "Testing 35 Loss: 4.092 | Acc: 79.733% (7176/9000)\n",
      "Testing 36 Loss: 3.981 | Acc: 79.643% (7367/9250)\n",
      "Testing 37 Loss: 3.876 | Acc: 79.611% (7563/9500)\n",
      "Testing 38 Loss: 3.777 | Acc: 79.662% (7767/9750)\n",
      "Testing 39 Loss: 3.683 | Acc: 79.610% (7961/10000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for epoch in tqdm(range(num_epochs)):    num_epochs = 200\n",
    "for epoch in (pbar := tqdm(range(num_epochs))):\n",
    "    #print('Epoch ID', epoch)\n",
    "    #----------------------------------------------------------------------\n",
    "    # Training\n",
    "    #----------------------------------------------------------------------\n",
    "    pbar.set_description(f\"Training {epoch}\",refresh=True)\n",
    "    correct = 0; total = 0; train_loss = 0\n",
    "    net.train()\n",
    "    for batch_idx, (x, target) in enumerate(tqdm(train_loader, leave=False)):\n",
    "    #for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        #if batch_idx < 1:\n",
    "        optimizer.zero_grad()\n",
    "        x, target = Variable(x.cuda()), Variable(target.cuda())\n",
    "        all_W_kernels = optimizer.param_groups[1]['params']\n",
    "        all_G_kernels = optimizer_quant.param_groups[0]['params']\n",
    "        \n",
    "        for i in range(len(all_W_kernels)):\n",
    "            k_W = all_W_kernels[i]\n",
    "            k_G = all_G_kernels[i]\n",
    "            V = k_W.data\n",
    "            #print(type(V))\n",
    "            #####Binary Connect#########################\n",
    "            #k_G.data = quantize_bw(V)\n",
    "            ############################################\n",
    "            \n",
    "            ######Binary Relax##########################\n",
    "            if epoch<120:\n",
    "                #k_G.data = (eta*quantize_bw(V)+V)/(1+eta)\n",
    "                k_G.data = (eta*quantize(V,num_bits=quantize_nbits)+V)/(1+eta)\n",
    "                \n",
    "            else:\n",
    "                k_G.data = quantize(V, num_bits=quantize_nbits)\n",
    "            #############################################\n",
    "            \n",
    "            k_W.data, k_G.data = k_G.data, k_W.data\n",
    "            \n",
    "            \n",
    "        score = net(x)\n",
    "        loss = criterion(score, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        for i in range(len(all_W_kernels)):\n",
    "            k_W = all_W_kernels[i]\n",
    "            k_G = all_G_kernels[i]\n",
    "            k_W.data, k_G.data = k_G.data, k_W.data\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data\n",
    "        _, predicted = torch.max(score.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        pbar.write(f\"Training {batch_idx} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f}% ({correct}/{total})\")\n",
    "        \n",
    "    #----------------------------------------------------------------------\n",
    "    # Testing\n",
    "    #----------------------------------------------------------------------\n",
    "    pbar.set_description(f\"Testing {epoch}\",refresh=True)\n",
    "    test_loss = 0; correct = 0; total = 0\n",
    "    net.eval()\n",
    "    \n",
    "    for i in range(len(all_W_kernels)):\n",
    "        k_W = all_W_kernels[i]\n",
    "        k_quant = all_G_kernels[i]    \n",
    "        k_W.data, k_quant.data = k_quant.data, k_W.data\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, target) in enumerate(tqdm(test_loader, leave=False)):\n",
    "        #for batch_idx, (x, target) in enumerate(test_loader):\n",
    "            x, target = Variable(x.cuda()), Variable(target.cuda())\n",
    "            score= net(x)\n",
    "            \n",
    "            loss = criterion(score, target)\n",
    "            test_loss += loss.data\n",
    "            _, predicted = torch.max(score.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target.data).cpu().sum()\n",
    "            pbar.write(f\"Testing {batch_idx} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f}% ({correct}/{total})\")\n",
    "\n",
    "    \n",
    "    #----------------------------------------------------------------------\n",
    "    # Save the checkpoint\n",
    "    #----------------------------------------------------------------------\n",
    "    '''\n",
    "    for i in range(len(all_W_kernels)):\n",
    "        k_W = all_W_kernels[i]\n",
    "        k_quant = all_G_kernels[i]    \n",
    "        k_W.data, k_quant.data = k_quant.data, k_W.data\n",
    "        '''  \n",
    "    acc = 100.*correct/total\n",
    "    #if acc > best_acc:\n",
    "    if correct > best_count:\n",
    "        # print('Saving model...')\n",
    "        # state = {\n",
    "        #     'state': net.state_dict(), #net,\n",
    "        #     'acc': acc,\n",
    "        #     'epoch': epoch,\n",
    "        # }\n",
    "        \n",
    "        # torch.save(state, f'./saved_models/resnet_{quantize_nbits}bits_{u.time_stamp()}.pth')\n",
    "        #net.save_state_dict('resnet20.pt')\n",
    "        best_acc = acc\n",
    "        best_count = correct\n",
    "\n",
    "    for i in range(len(all_W_kernels)):\n",
    "        k_W=all_W_kernels[i]\n",
    "        k_quant=all_W_kernels[i]\n",
    "        k_W.data, k_quant.data =k_quant.data,k_W.data\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "save_model(net, \"saved_models\", f'resnet_{quantize_nbits}bits_{u.time_stamp()}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
