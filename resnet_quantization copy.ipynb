{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import fedlern.utils as u\n",
    "from fedlern.train_utils import *\n",
    "from fedlern.quant_utils import *\n",
    "from fedlern.models.resnet_v2 import ResNet18\n",
    "from fedlern.quantize import quantize\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "stats = (0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784)\n",
    "batch_size_test = 64 #250\n",
    "batch_size_train = 64 #128\n",
    "\n",
    "quantize_nbits = 16\n",
    "num_epochs = 150\n",
    "learning_rate = 0.0001\n",
    "eta_rate = 1.05\n",
    "eta = 1\n",
    "#global best_acc\n",
    "best_acc = 0\n",
    "best_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = prepare_dataloader_cifar(num_workers=8, \n",
    "                                                     train_batch_size=batch_size_train, \n",
    "                                                     eval_batch_size=batch_size_test, \n",
    "                                                     stats=stats)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (skip_add): FloatFunctional(\n",
       "        (activation_post_process): Identity()\n",
       "      )\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ResNet18()\n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_model_optimizer(net, learning_rate=0.1, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "    \n",
    "# all_G_kernels = [\n",
    "#     Variable(kernel.data.clone(), requires_grad=True)\n",
    "#     for kernel in optimizer.param_groups[1]['params']\n",
    "# ]\n",
    "# Copy the parameters\n",
    "all_G_kernels = [kernel.data.clone().requires_grad_(True)\n",
    "                for kernel in optimizer.param_groups[1]['params']]\n",
    "\n",
    "#all_W_kernels = [kernel for kernel in optimizer.param_groups[1]['params']]\n",
    "all_W_kernels = optimizer.param_groups[1]['params']\n",
    "kernels = [{'params': all_G_kernels}]\n",
    "optimizer_quant = optim.SGD(kernels, lr=0)\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80,120,160], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df0555f34a04dcdada18d3d0d67d8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 149 Training 0 Loss: 0.328 | Acc: 88.281% (113/128)\n",
      "Epoch: 149 Training 1 Loss: 0.307 | Acc: 90.625% (232/256)\n",
      "Epoch: 149 Training 2 Loss: 0.277 | Acc: 90.885% (349/384)\n",
      "Epoch: 149 Training 3 Loss: 0.293 | Acc: 90.039% (461/512)\n",
      "Epoch: 149 Training 4 Loss: 0.295 | Acc: 89.688% (574/640)\n",
      "Epoch: 149 Training 5 Loss: 0.292 | Acc: 89.714% (689/768)\n",
      "Epoch: 149 Training 6 Loss: 0.295 | Acc: 89.732% (804/896)\n",
      "Epoch: 149 Training 7 Loss: 0.284 | Acc: 90.137% (923/1024)\n",
      "Epoch: 149 Training 8 Loss: 0.283 | Acc: 90.104% (1038/1152)\n",
      "Epoch: 149 Training 9 Loss: 0.291 | Acc: 90.078% (1153/1280)\n",
      "Epoch: 149 Training 10 Loss: 0.298 | Acc: 89.986% (1267/1408)\n",
      "Epoch: 149 Training 11 Loss: 0.292 | Acc: 90.299% (1387/1536)\n",
      "Epoch: 149 Training 12 Loss: 0.298 | Acc: 90.144% (1500/1664)\n",
      "Epoch: 149 Training 13 Loss: 0.293 | Acc: 90.234% (1617/1792)\n",
      "Epoch: 149 Training 14 Loss: 0.290 | Acc: 90.208% (1732/1920)\n",
      "Epoch: 149 Training 15 Loss: 0.285 | Acc: 90.332% (1850/2048)\n",
      "Epoch: 149 Training 16 Loss: 0.286 | Acc: 90.349% (1966/2176)\n",
      "Epoch: 149 Training 17 Loss: 0.293 | Acc: 90.191% (2078/2304)\n",
      "Epoch: 149 Training 18 Loss: 0.300 | Acc: 90.049% (2190/2432)\n",
      "Epoch: 149 Training 19 Loss: 0.302 | Acc: 90.078% (2306/2560)\n",
      "Epoch: 149 Training 20 Loss: 0.300 | Acc: 90.067% (2421/2688)\n",
      "Epoch: 149 Training 21 Loss: 0.299 | Acc: 90.092% (2537/2816)\n",
      "Epoch: 149 Training 22 Loss: 0.295 | Acc: 90.285% (2658/2944)\n",
      "Epoch: 149 Training 23 Loss: 0.293 | Acc: 90.365% (2776/3072)\n",
      "Epoch: 149 Training 24 Loss: 0.290 | Acc: 90.469% (2895/3200)\n",
      "Epoch: 149 Training 25 Loss: 0.288 | Acc: 90.475% (3011/3328)\n",
      "Epoch: 149 Training 26 Loss: 0.289 | Acc: 90.422% (3125/3456)\n",
      "Epoch: 149 Training 27 Loss: 0.289 | Acc: 90.458% (3242/3584)\n",
      "Epoch: 149 Training 28 Loss: 0.290 | Acc: 90.409% (3356/3712)\n",
      "Epoch: 149 Training 29 Loss: 0.289 | Acc: 90.469% (3474/3840)\n",
      "Epoch: 149 Training 30 Loss: 0.287 | Acc: 90.499% (3591/3968)\n",
      "Epoch: 149 Training 31 Loss: 0.289 | Acc: 90.356% (3701/4096)\n",
      "Epoch: 149 Training 32 Loss: 0.289 | Acc: 90.412% (3819/4224)\n",
      "Epoch: 149 Training 33 Loss: 0.292 | Acc: 90.303% (3930/4352)\n",
      "Epoch: 149 Training 34 Loss: 0.292 | Acc: 90.290% (4045/4480)\n",
      "Epoch: 149 Training 35 Loss: 0.291 | Acc: 90.299% (4161/4608)\n",
      "Epoch: 149 Training 36 Loss: 0.289 | Acc: 90.393% (4281/4736)\n",
      "Epoch: 149 Training 37 Loss: 0.289 | Acc: 90.419% (4398/4864)\n",
      "Epoch: 149 Training 38 Loss: 0.287 | Acc: 90.525% (4519/4992)\n",
      "Epoch: 149 Training 39 Loss: 0.287 | Acc: 90.625% (4640/5120)\n",
      "Epoch: 149 Training 40 Loss: 0.286 | Acc: 90.644% (4757/5248)\n",
      "Epoch: 149 Training 41 Loss: 0.286 | Acc: 90.662% (4874/5376)\n",
      "Epoch: 149 Training 42 Loss: 0.285 | Acc: 90.698% (4992/5504)\n",
      "Epoch: 149 Training 43 Loss: 0.285 | Acc: 90.696% (5108/5632)\n",
      "Epoch: 149 Training 44 Loss: 0.283 | Acc: 90.694% (5224/5760)\n",
      "Epoch: 149 Training 45 Loss: 0.284 | Acc: 90.659% (5338/5888)\n",
      "Epoch: 149 Training 46 Loss: 0.283 | Acc: 90.642% (5453/6016)\n",
      "Epoch: 149 Training 47 Loss: 0.283 | Acc: 90.609% (5567/6144)\n",
      "Epoch: 149 Training 48 Loss: 0.283 | Acc: 90.577% (5681/6272)\n",
      "Epoch: 149 Training 49 Loss: 0.284 | Acc: 90.484% (5791/6400)\n",
      "Epoch: 149 Training 50 Loss: 0.283 | Acc: 90.502% (5908/6528)\n",
      "Epoch: 149 Training 51 Loss: 0.283 | Acc: 90.505% (6024/6656)\n",
      "Epoch: 149 Training 52 Loss: 0.282 | Acc: 90.507% (6140/6784)\n",
      "Epoch: 149 Training 53 Loss: 0.281 | Acc: 90.553% (6259/6912)\n",
      "Epoch: 149 Training 54 Loss: 0.282 | Acc: 90.511% (6372/7040)\n",
      "Epoch: 149 Training 55 Loss: 0.282 | Acc: 90.472% (6485/7168)\n",
      "Epoch: 149 Training 56 Loss: 0.281 | Acc: 90.488% (6602/7296)\n",
      "Epoch: 149 Training 57 Loss: 0.281 | Acc: 90.450% (6715/7424)\n",
      "Epoch: 149 Training 58 Loss: 0.281 | Acc: 90.466% (6832/7552)\n",
      "Epoch: 149 Training 59 Loss: 0.280 | Acc: 90.495% (6950/7680)\n",
      "Epoch: 149 Training 60 Loss: 0.280 | Acc: 90.484% (7065/7808)\n",
      "Epoch: 149 Training 61 Loss: 0.280 | Acc: 90.499% (7182/7936)\n",
      "Epoch: 149 Training 62 Loss: 0.281 | Acc: 90.451% (7294/8064)\n",
      "Epoch: 149 Training 63 Loss: 0.282 | Acc: 90.454% (7410/8192)\n",
      "Epoch: 149 Training 64 Loss: 0.283 | Acc: 90.312% (7514/8320)\n",
      "Epoch: 149 Training 65 Loss: 0.284 | Acc: 90.282% (7627/8448)\n",
      "Epoch: 149 Training 66 Loss: 0.286 | Acc: 90.229% (7738/8576)\n",
      "Epoch: 149 Training 67 Loss: 0.286 | Acc: 90.211% (7852/8704)\n",
      "Epoch: 149 Training 68 Loss: 0.285 | Acc: 90.274% (7973/8832)\n",
      "Epoch: 149 Training 69 Loss: 0.287 | Acc: 90.212% (8083/8960)\n",
      "Epoch: 149 Training 70 Loss: 0.289 | Acc: 90.152% (8193/9088)\n",
      "Epoch: 149 Training 71 Loss: 0.291 | Acc: 90.115% (8305/9216)\n",
      "Epoch: 149 Training 72 Loss: 0.291 | Acc: 90.111% (8420/9344)\n",
      "Epoch: 149 Training 73 Loss: 0.292 | Acc: 90.034% (8528/9472)\n",
      "Epoch: 149 Training 74 Loss: 0.293 | Acc: 90.000% (8640/9600)\n",
      "Epoch: 149 Training 75 Loss: 0.294 | Acc: 89.967% (8752/9728)\n",
      "Epoch: 149 Training 76 Loss: 0.295 | Acc: 89.905% (8861/9856)\n",
      "Epoch: 149 Training 77 Loss: 0.295 | Acc: 89.884% (8974/9984)\n",
      "Epoch: 149 Training 78 Loss: 0.297 | Acc: 89.834% (9084/10112)\n",
      "Epoch: 149 Training 79 Loss: 0.298 | Acc: 89.805% (9196/10240)\n",
      "Epoch: 149 Training 80 Loss: 0.298 | Acc: 89.824% (9313/10368)\n",
      "Epoch: 149 Training 81 Loss: 0.298 | Acc: 89.853% (9431/10496)\n",
      "Epoch: 149 Training 82 Loss: 0.299 | Acc: 89.768% (9537/10624)\n",
      "Epoch: 149 Training 83 Loss: 0.299 | Acc: 89.797% (9655/10752)\n",
      "Epoch: 149 Training 84 Loss: 0.298 | Acc: 89.807% (9771/10880)\n",
      "Epoch: 149 Training 85 Loss: 0.299 | Acc: 89.780% (9883/11008)\n",
      "Epoch: 149 Training 86 Loss: 0.299 | Acc: 89.790% (9999/11136)\n",
      "Epoch: 149 Training 87 Loss: 0.298 | Acc: 89.817% (10117/11264)\n",
      "Epoch: 149 Training 88 Loss: 0.299 | Acc: 89.809% (10231/11392)\n",
      "Epoch: 149 Training 89 Loss: 0.299 | Acc: 89.800% (10345/11520)\n",
      "Epoch: 149 Training 90 Loss: 0.300 | Acc: 89.792% (10459/11648)\n",
      "Epoch: 149 Training 91 Loss: 0.299 | Acc: 89.801% (10575/11776)\n",
      "Epoch: 149 Training 92 Loss: 0.299 | Acc: 89.802% (10690/11904)\n",
      "Epoch: 149 Training 93 Loss: 0.299 | Acc: 89.827% (10808/12032)\n",
      "Epoch: 149 Training 94 Loss: 0.298 | Acc: 89.827% (10923/12160)\n",
      "Epoch: 149 Training 95 Loss: 0.298 | Acc: 89.844% (11040/12288)\n",
      "Epoch: 149 Training 96 Loss: 0.298 | Acc: 89.828% (11153/12416)\n",
      "Epoch: 149 Training 97 Loss: 0.299 | Acc: 89.820% (11267/12544)\n",
      "Epoch: 149 Training 98 Loss: 0.298 | Acc: 89.812% (11381/12672)\n",
      "Epoch: 149 Training 99 Loss: 0.298 | Acc: 89.805% (11495/12800)\n",
      "Epoch: 149 Training 100 Loss: 0.299 | Acc: 89.759% (11604/12928)\n",
      "Epoch: 149 Training 101 Loss: 0.300 | Acc: 89.729% (11715/13056)\n",
      "Epoch: 149 Training 102 Loss: 0.301 | Acc: 89.730% (11830/13184)\n",
      "Epoch: 149 Training 103 Loss: 0.300 | Acc: 89.739% (11946/13312)\n",
      "Epoch: 149 Training 104 Loss: 0.300 | Acc: 89.740% (12061/13440)\n",
      "Epoch: 149 Training 105 Loss: 0.301 | Acc: 89.711% (12172/13568)\n",
      "Epoch: 149 Training 106 Loss: 0.301 | Acc: 89.654% (12279/13696)\n",
      "Epoch: 149 Training 107 Loss: 0.302 | Acc: 89.641% (12392/13824)\n",
      "Epoch: 149 Training 108 Loss: 0.301 | Acc: 89.665% (12510/13952)\n",
      "Epoch: 149 Training 109 Loss: 0.302 | Acc: 89.652% (12623/14080)\n",
      "Epoch: 149 Training 110 Loss: 0.303 | Acc: 89.604% (12731/14208)\n",
      "Epoch: 149 Training 111 Loss: 0.303 | Acc: 89.586% (12843/14336)\n",
      "Epoch: 149 Training 112 Loss: 0.303 | Acc: 89.588% (12958/14464)\n",
      "Epoch: 149 Training 113 Loss: 0.303 | Acc: 89.583% (13072/14592)\n",
      "Epoch: 149 Training 114 Loss: 0.302 | Acc: 89.626% (13193/14720)\n",
      "Epoch: 149 Training 115 Loss: 0.303 | Acc: 89.608% (13305/14848)\n",
      "Epoch: 149 Training 116 Loss: 0.303 | Acc: 89.623% (13422/14976)\n",
      "Epoch: 149 Training 117 Loss: 0.302 | Acc: 89.632% (13538/15104)\n",
      "Epoch: 149 Training 118 Loss: 0.303 | Acc: 89.640% (13654/15232)\n",
      "Epoch: 149 Training 119 Loss: 0.302 | Acc: 89.655% (13771/15360)\n",
      "Epoch: 149 Training 120 Loss: 0.303 | Acc: 89.650% (13885/15488)\n",
      "Epoch: 149 Training 121 Loss: 0.303 | Acc: 89.645% (13999/15616)\n",
      "Epoch: 149 Training 122 Loss: 0.303 | Acc: 89.660% (14116/15744)\n",
      "Epoch: 149 Training 123 Loss: 0.302 | Acc: 89.680% (14234/15872)\n",
      "Epoch: 149 Training 124 Loss: 0.301 | Acc: 89.719% (14355/16000)\n",
      "Epoch: 149 Training 125 Loss: 0.301 | Acc: 89.714% (14469/16128)\n",
      "Epoch: 149 Training 126 Loss: 0.300 | Acc: 89.745% (14589/16256)\n",
      "Epoch: 149 Training 127 Loss: 0.300 | Acc: 89.752% (14705/16384)\n",
      "Epoch: 149 Training 128 Loss: 0.301 | Acc: 89.741% (14818/16512)\n",
      "Epoch: 149 Training 129 Loss: 0.301 | Acc: 89.736% (14932/16640)\n",
      "Epoch: 149 Training 130 Loss: 0.301 | Acc: 89.754% (15050/16768)\n",
      "Epoch: 149 Training 131 Loss: 0.301 | Acc: 89.749% (15164/16896)\n",
      "Epoch: 149 Training 132 Loss: 0.301 | Acc: 89.767% (15282/17024)\n",
      "Epoch: 149 Training 133 Loss: 0.301 | Acc: 89.774% (15398/17152)\n",
      "Epoch: 149 Training 134 Loss: 0.301 | Acc: 89.763% (15511/17280)\n",
      "Epoch: 149 Training 135 Loss: 0.301 | Acc: 89.769% (15627/17408)\n",
      "Epoch: 149 Training 136 Loss: 0.301 | Acc: 89.741% (15737/17536)\n",
      "Epoch: 149 Training 137 Loss: 0.302 | Acc: 89.714% (15847/17664)\n",
      "Epoch: 149 Training 138 Loss: 0.301 | Acc: 89.754% (15969/17792)\n",
      "Epoch: 149 Training 139 Loss: 0.301 | Acc: 89.749% (16083/17920)\n",
      "Epoch: 149 Training 140 Loss: 0.302 | Acc: 89.727% (16194/18048)\n",
      "Epoch: 149 Training 141 Loss: 0.301 | Acc: 89.756% (16314/18176)\n",
      "Epoch: 149 Training 142 Loss: 0.301 | Acc: 89.778% (16433/18304)\n",
      "Epoch: 149 Training 143 Loss: 0.301 | Acc: 89.773% (16547/18432)\n",
      "Epoch: 149 Training 144 Loss: 0.301 | Acc: 89.790% (16665/18560)\n",
      "Epoch: 149 Training 145 Loss: 0.301 | Acc: 89.790% (16780/18688)\n",
      "Epoch: 149 Training 146 Loss: 0.300 | Acc: 89.812% (16899/18816)\n",
      "Epoch: 149 Training 147 Loss: 0.300 | Acc: 89.817% (17015/18944)\n",
      "Epoch: 149 Training 148 Loss: 0.299 | Acc: 89.802% (17127/19072)\n",
      "Epoch: 149 Training 149 Loss: 0.300 | Acc: 89.781% (17238/19200)\n",
      "Epoch: 149 Training 150 Loss: 0.300 | Acc: 89.776% (17352/19328)\n",
      "Epoch: 149 Training 151 Loss: 0.300 | Acc: 89.777% (17467/19456)\n",
      "Epoch: 149 Training 152 Loss: 0.301 | Acc: 89.772% (17581/19584)\n",
      "Epoch: 149 Training 153 Loss: 0.301 | Acc: 89.763% (17694/19712)\n",
      "Epoch: 149 Training 154 Loss: 0.301 | Acc: 89.748% (17806/19840)\n",
      "Epoch: 149 Training 155 Loss: 0.302 | Acc: 89.749% (17921/19968)\n",
      "Epoch: 149 Training 156 Loss: 0.301 | Acc: 89.779% (18042/20096)\n",
      "Epoch: 149 Training 157 Loss: 0.302 | Acc: 89.765% (18154/20224)\n",
      "Epoch: 149 Training 158 Loss: 0.302 | Acc: 89.755% (18267/20352)\n",
      "Epoch: 149 Training 159 Loss: 0.301 | Acc: 89.761% (18383/20480)\n",
      "Epoch: 149 Training 160 Loss: 0.302 | Acc: 89.737% (18493/20608)\n",
      "Epoch: 149 Training 161 Loss: 0.302 | Acc: 89.738% (18608/20736)\n",
      "Epoch: 149 Training 162 Loss: 0.302 | Acc: 89.724% (18720/20864)\n",
      "Epoch: 149 Training 163 Loss: 0.302 | Acc: 89.729% (18836/20992)\n",
      "Epoch: 149 Training 164 Loss: 0.302 | Acc: 89.740% (18953/21120)\n",
      "Epoch: 149 Training 165 Loss: 0.303 | Acc: 89.712% (19062/21248)\n",
      "Epoch: 149 Training 166 Loss: 0.303 | Acc: 89.713% (19177/21376)\n",
      "Epoch: 149 Training 167 Loss: 0.302 | Acc: 89.732% (19296/21504)\n",
      "Epoch: 149 Training 168 Loss: 0.302 | Acc: 89.737% (19412/21632)\n",
      "Epoch: 149 Training 169 Loss: 0.301 | Acc: 89.761% (19532/21760)\n",
      "Epoch: 149 Training 170 Loss: 0.301 | Acc: 89.771% (19649/21888)\n",
      "Epoch: 149 Training 171 Loss: 0.301 | Acc: 89.771% (19764/22016)\n",
      "Epoch: 149 Training 172 Loss: 0.301 | Acc: 89.771% (19879/22144)\n",
      "Epoch: 149 Training 173 Loss: 0.302 | Acc: 89.763% (19992/22272)\n",
      "Epoch: 149 Training 174 Loss: 0.302 | Acc: 89.781% (20111/22400)\n",
      "Epoch: 149 Training 175 Loss: 0.302 | Acc: 89.786% (20227/22528)\n",
      "Epoch: 149 Training 176 Loss: 0.302 | Acc: 89.795% (20344/22656)\n",
      "Epoch: 149 Training 177 Loss: 0.301 | Acc: 89.795% (20459/22784)\n",
      "Epoch: 149 Training 178 Loss: 0.302 | Acc: 89.783% (20571/22912)\n",
      "Epoch: 149 Training 179 Loss: 0.301 | Acc: 89.779% (20685/23040)\n",
      "Epoch: 149 Training 180 Loss: 0.302 | Acc: 89.783% (20801/23168)\n",
      "Epoch: 149 Training 181 Loss: 0.302 | Acc: 89.784% (20916/23296)\n",
      "Epoch: 149 Training 182 Loss: 0.302 | Acc: 89.775% (21029/23424)\n",
      "Epoch: 149 Training 183 Loss: 0.302 | Acc: 89.742% (21136/23552)\n",
      "Epoch: 149 Training 184 Loss: 0.301 | Acc: 89.764% (21256/23680)\n",
      "Epoch: 149 Training 185 Loss: 0.301 | Acc: 89.768% (21372/23808)\n",
      "Epoch: 149 Training 186 Loss: 0.302 | Acc: 89.764% (21486/23936)\n",
      "Epoch: 149 Training 187 Loss: 0.302 | Acc: 89.769% (21602/24064)\n",
      "Epoch: 149 Training 188 Loss: 0.302 | Acc: 89.773% (21718/24192)\n",
      "Epoch: 149 Training 189 Loss: 0.303 | Acc: 89.737% (21824/24320)\n",
      "Epoch: 149 Training 190 Loss: 0.303 | Acc: 89.717% (21934/24448)\n",
      "Epoch: 149 Training 191 Loss: 0.303 | Acc: 89.730% (22052/24576)\n",
      "Epoch: 149 Training 192 Loss: 0.304 | Acc: 89.718% (22164/24704)\n",
      "Epoch: 149 Training 193 Loss: 0.304 | Acc: 89.711% (22277/24832)\n",
      "Epoch: 149 Training 194 Loss: 0.304 | Acc: 89.696% (22388/24960)\n",
      "Epoch: 149 Training 195 Loss: 0.304 | Acc: 89.700% (22504/25088)\n",
      "Epoch: 149 Training 196 Loss: 0.305 | Acc: 89.665% (22610/25216)\n",
      "Epoch: 149 Training 197 Loss: 0.305 | Acc: 89.650% (22721/25344)\n",
      "Epoch: 149 Training 198 Loss: 0.305 | Acc: 89.640% (22833/25472)\n",
      "Epoch: 149 Training 199 Loss: 0.305 | Acc: 89.648% (22950/25600)\n",
      "Epoch: 149 Training 200 Loss: 0.305 | Acc: 89.653% (23066/25728)\n",
      "Epoch: 149 Training 201 Loss: 0.305 | Acc: 89.650% (23180/25856)\n",
      "Epoch: 149 Training 202 Loss: 0.305 | Acc: 89.655% (23296/25984)\n",
      "Epoch: 149 Training 203 Loss: 0.305 | Acc: 89.648% (23409/26112)\n",
      "Epoch: 149 Training 204 Loss: 0.305 | Acc: 89.646% (23523/26240)\n",
      "Epoch: 149 Training 205 Loss: 0.304 | Acc: 89.666% (23643/26368)\n",
      "Epoch: 149 Training 206 Loss: 0.305 | Acc: 89.644% (23752/26496)\n",
      "Epoch: 149 Training 207 Loss: 0.304 | Acc: 89.660% (23871/26624)\n",
      "Epoch: 149 Training 208 Loss: 0.305 | Acc: 89.649% (23983/26752)\n",
      "Epoch: 149 Training 209 Loss: 0.305 | Acc: 89.650% (24098/26880)\n",
      "Epoch: 149 Training 210 Loss: 0.304 | Acc: 89.662% (24216/27008)\n",
      "Epoch: 149 Training 211 Loss: 0.304 | Acc: 89.663% (24331/27136)\n",
      "Epoch: 149 Training 212 Loss: 0.304 | Acc: 89.675% (24449/27264)\n",
      "Epoch: 149 Training 213 Loss: 0.304 | Acc: 89.669% (24562/27392)\n",
      "Epoch: 149 Training 214 Loss: 0.305 | Acc: 89.651% (24672/27520)\n",
      "Epoch: 149 Training 215 Loss: 0.305 | Acc: 89.634% (24782/27648)\n",
      "Epoch: 149 Training 216 Loss: 0.305 | Acc: 89.617% (24892/27776)\n",
      "Epoch: 149 Training 217 Loss: 0.305 | Acc: 89.632% (25011/27904)\n",
      "Epoch: 149 Training 218 Loss: 0.305 | Acc: 89.615% (25121/28032)\n",
      "Epoch: 149 Training 219 Loss: 0.304 | Acc: 89.631% (25240/28160)\n",
      "Epoch: 149 Training 220 Loss: 0.304 | Acc: 89.635% (25356/28288)\n",
      "Epoch: 149 Training 221 Loss: 0.305 | Acc: 89.615% (25465/28416)\n",
      "Epoch: 149 Training 222 Loss: 0.305 | Acc: 89.609% (25578/28544)\n",
      "Epoch: 149 Training 223 Loss: 0.305 | Acc: 89.631% (25699/28672)\n",
      "Epoch: 149 Training 224 Loss: 0.305 | Acc: 89.615% (25809/28800)\n",
      "Epoch: 149 Training 225 Loss: 0.305 | Acc: 89.623% (25926/28928)\n",
      "Epoch: 149 Training 226 Loss: 0.305 | Acc: 89.627% (26042/29056)\n",
      "Epoch: 149 Training 227 Loss: 0.305 | Acc: 89.628% (26157/29184)\n",
      "Epoch: 149 Training 228 Loss: 0.306 | Acc: 89.595% (26262/29312)\n",
      "Epoch: 149 Training 229 Loss: 0.306 | Acc: 89.592% (26376/29440)\n",
      "Epoch: 149 Training 230 Loss: 0.306 | Acc: 89.590% (26490/29568)\n",
      "Epoch: 149 Training 231 Loss: 0.306 | Acc: 89.605% (26609/29696)\n",
      "Epoch: 149 Training 232 Loss: 0.306 | Acc: 89.612% (26726/29824)\n",
      "Epoch: 149 Training 233 Loss: 0.306 | Acc: 89.607% (26839/29952)\n",
      "Epoch: 149 Training 234 Loss: 0.306 | Acc: 89.618% (26957/30080)\n",
      "Epoch: 149 Training 235 Loss: 0.306 | Acc: 89.609% (27069/30208)\n",
      "Epoch: 149 Training 236 Loss: 0.306 | Acc: 89.597% (27180/30336)\n",
      "Epoch: 149 Training 237 Loss: 0.306 | Acc: 89.594% (27294/30464)\n",
      "Epoch: 149 Training 238 Loss: 0.307 | Acc: 89.589% (27407/30592)\n",
      "Epoch: 149 Training 239 Loss: 0.307 | Acc: 89.587% (27521/30720)\n",
      "Epoch: 149 Training 240 Loss: 0.306 | Acc: 89.601% (27640/30848)\n",
      "Epoch: 149 Training 241 Loss: 0.306 | Acc: 89.631% (27764/30976)\n",
      "Epoch: 149 Training 242 Loss: 0.306 | Acc: 89.625% (27877/31104)\n",
      "Epoch: 149 Training 243 Loss: 0.306 | Acc: 89.620% (27990/31232)\n",
      "Epoch: 149 Training 244 Loss: 0.306 | Acc: 89.624% (28106/31360)\n",
      "Epoch: 149 Training 245 Loss: 0.306 | Acc: 89.634% (28224/31488)\n",
      "Epoch: 149 Training 246 Loss: 0.306 | Acc: 89.641% (28341/31616)\n",
      "Epoch: 149 Training 247 Loss: 0.306 | Acc: 89.652% (28459/31744)\n",
      "Epoch: 149 Training 248 Loss: 0.305 | Acc: 89.662% (28577/31872)\n",
      "Epoch: 149 Training 249 Loss: 0.305 | Acc: 89.669% (28694/32000)\n",
      "Epoch: 149 Training 250 Loss: 0.305 | Acc: 89.679% (28812/32128)\n",
      "Epoch: 149 Training 251 Loss: 0.305 | Acc: 89.667% (28923/32256)\n",
      "Epoch: 149 Training 252 Loss: 0.305 | Acc: 89.671% (29039/32384)\n",
      "Epoch: 149 Training 253 Loss: 0.305 | Acc: 89.678% (29156/32512)\n",
      "Epoch: 149 Training 254 Loss: 0.306 | Acc: 89.663% (29266/32640)\n",
      "Epoch: 149 Training 255 Loss: 0.306 | Acc: 89.655% (29378/32768)\n",
      "Epoch: 149 Training 256 Loss: 0.306 | Acc: 89.652% (29492/32896)\n",
      "Epoch: 149 Training 257 Loss: 0.306 | Acc: 89.653% (29607/33024)\n",
      "Epoch: 149 Training 258 Loss: 0.306 | Acc: 89.645% (29719/33152)\n",
      "Epoch: 149 Training 259 Loss: 0.305 | Acc: 89.660% (29839/33280)\n",
      "Epoch: 149 Training 260 Loss: 0.305 | Acc: 89.673% (29958/33408)\n",
      "Epoch: 149 Training 261 Loss: 0.305 | Acc: 89.668% (30071/33536)\n",
      "Epoch: 149 Training 262 Loss: 0.305 | Acc: 89.671% (30187/33664)\n",
      "Epoch: 149 Training 263 Loss: 0.305 | Acc: 89.672% (30302/33792)\n",
      "Epoch: 149 Training 264 Loss: 0.306 | Acc: 89.640% (30406/33920)\n",
      "Epoch: 149 Training 265 Loss: 0.306 | Acc: 89.638% (30520/34048)\n",
      "Epoch: 149 Training 266 Loss: 0.306 | Acc: 89.642% (30636/34176)\n",
      "Epoch: 149 Training 267 Loss: 0.306 | Acc: 89.646% (30752/34304)\n",
      "Epoch: 149 Training 268 Loss: 0.306 | Acc: 89.649% (30868/34432)\n",
      "Epoch: 149 Training 269 Loss: 0.305 | Acc: 89.664% (30988/34560)\n",
      "Epoch: 149 Training 270 Loss: 0.305 | Acc: 89.668% (31104/34688)\n",
      "Epoch: 149 Training 271 Loss: 0.305 | Acc: 89.677% (31222/34816)\n",
      "Epoch: 149 Training 272 Loss: 0.305 | Acc: 89.683% (31339/34944)\n",
      "Epoch: 149 Training 273 Loss: 0.305 | Acc: 89.678% (31452/35072)\n",
      "Epoch: 149 Training 274 Loss: 0.305 | Acc: 89.696% (31573/35200)\n",
      "Epoch: 149 Training 275 Loss: 0.305 | Acc: 89.699% (31689/35328)\n",
      "Epoch: 149 Training 276 Loss: 0.305 | Acc: 89.708% (31807/35456)\n",
      "Epoch: 149 Training 277 Loss: 0.305 | Acc: 89.709% (31922/35584)\n",
      "Epoch: 149 Training 278 Loss: 0.305 | Acc: 89.709% (32037/35712)\n",
      "Epoch: 149 Training 279 Loss: 0.305 | Acc: 89.710% (32152/35840)\n",
      "Epoch: 149 Training 280 Loss: 0.305 | Acc: 89.727% (32273/35968)\n",
      "Epoch: 149 Training 281 Loss: 0.304 | Acc: 89.736% (32391/36096)\n",
      "Epoch: 149 Training 282 Loss: 0.305 | Acc: 89.720% (32500/36224)\n",
      "Epoch: 149 Training 283 Loss: 0.305 | Acc: 89.701% (32608/36352)\n",
      "Epoch: 149 Training 284 Loss: 0.306 | Acc: 89.696% (32721/36480)\n",
      "Epoch: 149 Training 285 Loss: 0.305 | Acc: 89.694% (32835/36608)\n",
      "Epoch: 149 Training 286 Loss: 0.305 | Acc: 89.691% (32949/36736)\n",
      "Epoch: 149 Training 287 Loss: 0.306 | Acc: 89.684% (33061/36864)\n",
      "Epoch: 149 Training 288 Loss: 0.306 | Acc: 89.682% (33175/36992)\n",
      "Epoch: 149 Training 289 Loss: 0.306 | Acc: 89.685% (33291/37120)\n",
      "Epoch: 149 Training 290 Loss: 0.306 | Acc: 89.669% (33400/37248)\n",
      "Epoch: 149 Training 291 Loss: 0.306 | Acc: 89.673% (33516/37376)\n",
      "Epoch: 149 Training 292 Loss: 0.306 | Acc: 89.662% (33627/37504)\n",
      "Epoch: 149 Training 293 Loss: 0.306 | Acc: 89.660% (33741/37632)\n",
      "Epoch: 149 Training 294 Loss: 0.306 | Acc: 89.664% (33857/37760)\n",
      "Epoch: 149 Training 295 Loss: 0.307 | Acc: 89.646% (33965/37888)\n",
      "Epoch: 149 Training 296 Loss: 0.307 | Acc: 89.646% (34080/38016)\n",
      "Epoch: 149 Training 297 Loss: 0.307 | Acc: 89.645% (34194/38144)\n",
      "Epoch: 149 Training 298 Loss: 0.306 | Acc: 89.656% (34313/38272)\n",
      "Epoch: 149 Training 299 Loss: 0.307 | Acc: 89.648% (34425/38400)\n",
      "Epoch: 149 Training 300 Loss: 0.307 | Acc: 89.641% (34537/38528)\n",
      "Epoch: 149 Training 301 Loss: 0.307 | Acc: 89.632% (34648/38656)\n",
      "Epoch: 149 Training 302 Loss: 0.307 | Acc: 89.627% (34761/38784)\n",
      "Epoch: 149 Training 303 Loss: 0.307 | Acc: 89.636% (34879/38912)\n",
      "Epoch: 149 Training 304 Loss: 0.307 | Acc: 89.629% (34991/39040)\n",
      "Epoch: 149 Training 305 Loss: 0.307 | Acc: 89.619% (35102/39168)\n",
      "Epoch: 149 Training 306 Loss: 0.307 | Acc: 89.622% (35218/39296)\n",
      "Epoch: 149 Training 307 Loss: 0.307 | Acc: 89.641% (35340/39424)\n",
      "Epoch: 149 Training 308 Loss: 0.307 | Acc: 89.654% (35460/39552)\n",
      "Epoch: 149 Training 309 Loss: 0.306 | Acc: 89.657% (35576/39680)\n",
      "Epoch: 149 Training 310 Loss: 0.306 | Acc: 89.670% (35696/39808)\n",
      "Epoch: 149 Training 311 Loss: 0.306 | Acc: 89.681% (35815/39936)\n",
      "Epoch: 149 Training 312 Loss: 0.306 | Acc: 89.667% (35924/40064)\n",
      "Epoch: 149 Training 313 Loss: 0.306 | Acc: 89.665% (36038/40192)\n",
      "Epoch: 149 Training 314 Loss: 0.307 | Acc: 89.643% (36144/40320)\n",
      "Epoch: 149 Training 315 Loss: 0.307 | Acc: 89.639% (36257/40448)\n",
      "Epoch: 149 Training 316 Loss: 0.307 | Acc: 89.637% (36371/40576)\n",
      "Epoch: 149 Training 317 Loss: 0.307 | Acc: 89.630% (36483/40704)\n",
      "Epoch: 149 Training 318 Loss: 0.307 | Acc: 89.640% (36602/40832)\n",
      "Epoch: 149 Training 319 Loss: 0.306 | Acc: 89.661% (36725/40960)\n",
      "Epoch: 149 Training 320 Loss: 0.307 | Acc: 89.644% (36833/41088)\n",
      "Epoch: 149 Training 321 Loss: 0.307 | Acc: 89.645% (36948/41216)\n",
      "Epoch: 149 Training 322 Loss: 0.307 | Acc: 89.626% (37055/41344)\n",
      "Epoch: 149 Training 323 Loss: 0.307 | Acc: 89.627% (37170/41472)\n",
      "Epoch: 149 Training 324 Loss: 0.308 | Acc: 89.620% (37282/41600)\n",
      "Epoch: 149 Training 325 Loss: 0.307 | Acc: 89.626% (37399/41728)\n",
      "Epoch: 149 Training 326 Loss: 0.307 | Acc: 89.629% (37515/41856)\n",
      "Epoch: 149 Training 327 Loss: 0.307 | Acc: 89.637% (37633/41984)\n",
      "Epoch: 149 Training 328 Loss: 0.307 | Acc: 89.640% (37749/42112)\n",
      "Epoch: 149 Training 329 Loss: 0.307 | Acc: 89.633% (37861/42240)\n",
      "Epoch: 149 Training 330 Loss: 0.307 | Acc: 89.643% (37980/42368)\n",
      "Epoch: 149 Training 331 Loss: 0.307 | Acc: 89.639% (38093/42496)\n",
      "Epoch: 149 Training 332 Loss: 0.308 | Acc: 89.626% (38202/42624)\n",
      "Epoch: 149 Training 333 Loss: 0.308 | Acc: 89.619% (38314/42752)\n",
      "Epoch: 149 Training 334 Loss: 0.308 | Acc: 89.606% (38423/42880)\n",
      "Epoch: 149 Training 335 Loss: 0.308 | Acc: 89.609% (38539/43008)\n",
      "Epoch: 149 Training 336 Loss: 0.308 | Acc: 89.612% (38655/43136)\n",
      "Epoch: 149 Training 337 Loss: 0.308 | Acc: 89.615% (38771/43264)\n",
      "Epoch: 149 Training 338 Loss: 0.308 | Acc: 89.613% (38885/43392)\n",
      "Epoch: 149 Training 339 Loss: 0.308 | Acc: 89.609% (38998/43520)\n",
      "Epoch: 149 Training 340 Loss: 0.308 | Acc: 89.610% (39113/43648)\n",
      "Epoch: 149 Training 341 Loss: 0.308 | Acc: 89.604% (39225/43776)\n",
      "Epoch: 149 Training 342 Loss: 0.308 | Acc: 89.605% (39340/43904)\n",
      "Epoch: 149 Training 343 Loss: 0.308 | Acc: 89.612% (39458/44032)\n",
      "Epoch: 149 Training 344 Loss: 0.308 | Acc: 89.613% (39573/44160)\n",
      "Epoch: 149 Training 345 Loss: 0.308 | Acc: 89.613% (39688/44288)\n",
      "Epoch: 149 Training 346 Loss: 0.308 | Acc: 89.614% (39803/44416)\n",
      "Epoch: 149 Training 347 Loss: 0.307 | Acc: 89.626% (39923/44544)\n",
      "Epoch: 149 Training 348 Loss: 0.307 | Acc: 89.638% (40043/44672)\n",
      "Epoch: 149 Training 349 Loss: 0.307 | Acc: 89.632% (40155/44800)\n",
      "Epoch: 149 Training 350 Loss: 0.308 | Acc: 89.628% (40268/44928)\n",
      "Epoch: 149 Training 351 Loss: 0.308 | Acc: 89.620% (40379/45056)\n",
      "Epoch: 149 Training 352 Loss: 0.308 | Acc: 89.629% (40498/45184)\n",
      "Epoch: 149 Training 353 Loss: 0.308 | Acc: 89.630% (40613/45312)\n",
      "Epoch: 149 Training 354 Loss: 0.308 | Acc: 89.624% (40725/45440)\n",
      "Epoch: 149 Training 355 Loss: 0.308 | Acc: 89.622% (40839/45568)\n",
      "Epoch: 149 Training 356 Loss: 0.308 | Acc: 89.621% (40953/45696)\n",
      "Epoch: 149 Training 357 Loss: 0.308 | Acc: 89.610% (41063/45824)\n",
      "Epoch: 149 Training 358 Loss: 0.308 | Acc: 89.613% (41179/45952)\n",
      "Epoch: 149 Training 359 Loss: 0.308 | Acc: 89.614% (41294/46080)\n",
      "Epoch: 149 Training 360 Loss: 0.308 | Acc: 89.599% (41402/46208)\n",
      "Epoch: 149 Training 361 Loss: 0.308 | Acc: 89.598% (41516/46336)\n",
      "Epoch: 149 Training 362 Loss: 0.308 | Acc: 89.592% (41628/46464)\n",
      "Epoch: 149 Training 363 Loss: 0.308 | Acc: 89.593% (41743/46592)\n",
      "Epoch: 149 Training 364 Loss: 0.309 | Acc: 89.591% (41857/46720)\n",
      "Epoch: 149 Training 365 Loss: 0.308 | Acc: 89.603% (41977/46848)\n",
      "Epoch: 149 Training 366 Loss: 0.308 | Acc: 89.607% (42094/46976)\n",
      "Epoch: 149 Training 367 Loss: 0.308 | Acc: 89.610% (42210/47104)\n",
      "Epoch: 149 Training 368 Loss: 0.309 | Acc: 89.602% (42321/47232)\n",
      "Epoch: 149 Training 369 Loss: 0.309 | Acc: 89.597% (42433/47360)\n",
      "Epoch: 149 Training 370 Loss: 0.308 | Acc: 89.599% (42549/47488)\n",
      "Epoch: 149 Training 371 Loss: 0.308 | Acc: 89.606% (42667/47616)\n",
      "Epoch: 149 Training 372 Loss: 0.308 | Acc: 89.605% (42781/47744)\n",
      "Epoch: 149 Training 373 Loss: 0.309 | Acc: 89.595% (42891/47872)\n",
      "Epoch: 149 Training 374 Loss: 0.309 | Acc: 89.592% (43004/48000)\n",
      "Epoch: 149 Training 375 Loss: 0.308 | Acc: 89.586% (43116/48128)\n",
      "Epoch: 149 Training 376 Loss: 0.309 | Acc: 89.587% (43231/48256)\n",
      "Epoch: 149 Training 377 Loss: 0.309 | Acc: 89.585% (43345/48384)\n",
      "Epoch: 149 Training 378 Loss: 0.309 | Acc: 89.584% (43459/48512)\n",
      "Epoch: 149 Training 379 Loss: 0.309 | Acc: 89.570% (43567/48640)\n",
      "Epoch: 149 Training 380 Loss: 0.309 | Acc: 89.567% (43680/48768)\n",
      "Epoch: 149 Training 381 Loss: 0.309 | Acc: 89.562% (43792/48896)\n",
      "Epoch: 149 Training 382 Loss: 0.309 | Acc: 89.562% (43907/49024)\n",
      "Epoch: 149 Training 383 Loss: 0.309 | Acc: 89.557% (44019/49152)\n",
      "Epoch: 149 Training 384 Loss: 0.309 | Acc: 89.564% (44137/49280)\n",
      "Epoch: 149 Training 385 Loss: 0.309 | Acc: 89.554% (44247/49408)\n",
      "Epoch: 149 Training 386 Loss: 0.309 | Acc: 89.559% (44364/49536)\n",
      "Epoch: 149 Training 387 Loss: 0.309 | Acc: 89.556% (44477/49664)\n",
      "Epoch: 149 Training 388 Loss: 0.309 | Acc: 89.555% (44591/49792)\n",
      "Epoch: 149 Training 389 Loss: 0.309 | Acc: 89.543% (44700/49920)\n",
      "Epoch: 149 Training 390 Loss: 0.310 | Acc: 89.536% (44768/50000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e91cc09b3b4ddabddf0549a4153ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 0 Loss: 121.079 | Acc: 79.200% (198/250)\n",
      "Testing 1 Loss: 60.539 | Acc: 79.200% (396/500)\n",
      "Testing 2 Loss: 40.360 | Acc: 79.600% (597/750)\n",
      "Testing 3 Loss: 30.270 | Acc: 79.800% (798/1000)\n",
      "Testing 4 Loss: 24.216 | Acc: 78.800% (985/1250)\n",
      "Testing 5 Loss: 20.180 | Acc: 79.267% (1189/1500)\n",
      "Testing 6 Loss: 17.297 | Acc: 79.257% (1387/1750)\n",
      "Testing 7 Loss: 15.135 | Acc: 79.350% (1587/2000)\n",
      "Testing 8 Loss: 13.453 | Acc: 79.600% (1791/2250)\n",
      "Testing 9 Loss: 12.108 | Acc: 79.720% (1993/2500)\n",
      "Testing 10 Loss: 11.007 | Acc: 79.527% (2187/2750)\n",
      "Testing 11 Loss: 10.090 | Acc: 79.467% (2384/3000)\n",
      "Testing 12 Loss: 9.314 | Acc: 79.231% (2575/3250)\n",
      "Testing 13 Loss: 8.648 | Acc: 79.029% (2766/3500)\n",
      "Testing 14 Loss: 8.072 | Acc: 79.147% (2968/3750)\n",
      "Testing 15 Loss: 7.567 | Acc: 79.200% (3168/4000)\n",
      "Testing 16 Loss: 7.122 | Acc: 79.459% (3377/4250)\n",
      "Testing 17 Loss: 6.727 | Acc: 79.444% (3575/4500)\n",
      "Testing 18 Loss: 6.373 | Acc: 79.368% (3770/4750)\n",
      "Testing 19 Loss: 6.054 | Acc: 79.460% (3973/5000)\n",
      "Testing 20 Loss: 5.766 | Acc: 79.371% (4167/5250)\n",
      "Testing 21 Loss: 5.504 | Acc: 79.273% (4360/5500)\n",
      "Testing 22 Loss: 5.264 | Acc: 79.235% (4556/5750)\n",
      "Testing 23 Loss: 5.045 | Acc: 79.317% (4759/6000)\n",
      "Testing 24 Loss: 4.843 | Acc: 79.280% (4955/6250)\n",
      "Testing 25 Loss: 4.657 | Acc: 79.262% (5152/6500)\n",
      "Testing 26 Loss: 4.484 | Acc: 79.274% (5351/6750)\n",
      "Testing 27 Loss: 4.324 | Acc: 79.129% (5539/7000)\n",
      "Testing 28 Loss: 4.175 | Acc: 79.269% (5747/7250)\n",
      "Testing 29 Loss: 4.036 | Acc: 79.440% (5958/7500)\n",
      "Testing 30 Loss: 3.906 | Acc: 79.497% (6161/7750)\n",
      "Testing 31 Loss: 3.784 | Acc: 79.412% (6353/8000)\n",
      "Testing 32 Loss: 3.669 | Acc: 79.418% (6552/8250)\n",
      "Testing 33 Loss: 3.561 | Acc: 79.294% (6740/8500)\n",
      "Testing 34 Loss: 3.459 | Acc: 79.394% (6947/8750)\n",
      "Testing 35 Loss: 3.363 | Acc: 79.289% (7136/9000)\n",
      "Testing 36 Loss: 3.272 | Acc: 79.297% (7335/9250)\n",
      "Testing 37 Loss: 3.186 | Acc: 79.253% (7529/9500)\n",
      "Testing 38 Loss: 3.105 | Acc: 79.354% (7737/9750)\n",
      "Testing 39 Loss: 3.027 | Acc: 79.300% (7930/10000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for epoch in tqdm(range(num_epochs)):    num_epochs = 200\n",
    "for epoch in (pbar := tqdm(range(num_epochs))):\n",
    "    #print('Epoch ID', epoch)\n",
    "    #----------------------------------------------------------------------\n",
    "    # Training\n",
    "    #----------------------------------------------------------------------\n",
    "    pbar.set_description(f\"Training {epoch}\",refresh=True)\n",
    "    correct = 0; total = 0; train_loss = 0\n",
    "    net.train()\n",
    "    for batch_idx, (x, target) in enumerate(tqdm(train_loader, leave=False)):\n",
    "    #for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        #if batch_idx < 1:\n",
    "        optimizer.zero_grad()\n",
    "        x, target = Variable(x.cuda()), Variable(target.cuda())\n",
    "        all_W_kernels = optimizer.param_groups[1]['params']\n",
    "        all_G_kernels = optimizer_quant.param_groups[0]['params']\n",
    "        \n",
    "        for k_W, k_G in zip(all_W_kernels, all_G_kernels):\n",
    "            V = k_W.data\n",
    "            #print(type(V))\n",
    "            #####Binary Connect#########################\n",
    "            #k_G.data = quantize_bw(V)\n",
    "            ############################################\n",
    "            \n",
    "            ######Binary Relax##########################\n",
    "            if epoch<120:\n",
    "                #k_G.data = (eta*quantize_bw(V)+V)/(1+eta)\n",
    "                k_G.data = (eta*quantize(V,num_bits=quantize_nbits)+V)/(1+eta)\n",
    "                \n",
    "            else:\n",
    "                k_G.data = quantize(V, num_bits=quantize_nbits)\n",
    "            #############################################\n",
    "            \n",
    "            k_W.data, k_G.data = k_G.data, k_W.data\n",
    "            \n",
    "            \n",
    "        score = net(x)\n",
    "        loss = criterion(score, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        for k_W, k_G in zip(all_W_kernels, all_G_kernels):\n",
    "            k_W.data, k_G.data = k_G.data, k_W.data\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data\n",
    "        _, predicted = torch.max(score.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        pbar.write(f\"Epoch: {epoch} Training {batch_idx} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f}% ({correct}/{total})\")\n",
    "        \n",
    "    #----------------------------------------------------------------------\n",
    "    # Testing\n",
    "    #----------------------------------------------------------------------\n",
    "    pbar.set_description(f\"Testing {epoch}\",refresh=True)\n",
    "    test_loss = 0; correct = 0; total = 0\n",
    "    net.eval()\n",
    "    \n",
    "    for k_W, k_G in zip(all_W_kernels, all_G_kernels):\n",
    "        k_W.data, k_G.data = k_G.data, k_W.data\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, target) in enumerate(tqdm(test_loader, leave=False)):\n",
    "            x, target = Variable(x.cuda()), Variable(target.cuda())\n",
    "            score= net(x)\n",
    "            \n",
    "            loss = criterion(score, target)\n",
    "            test_loss += loss.data\n",
    "            _, predicted = torch.max(score.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target.data).cpu().sum()\n",
    "            pbar.write(f\"Testing {batch_idx} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f}% ({correct}/{total})\")\n",
    "\n",
    "    \n",
    "    #----------------------------------------------------------------------\n",
    "    # Save the checkpoint\n",
    "    #----------------------------------------------------------------------\n",
    "    '''\n",
    "    for i in range(len(all_W_kernels)):\n",
    "        k_W = all_W_kernels[i]\n",
    "        k_quant = all_G_kernels[i]    \n",
    "        k_W.data, k_quant.data = k_quant.data, k_W.data\n",
    "        '''  \n",
    "    acc = 100.*correct/total\n",
    "    #if acc > best_acc:\n",
    "    if correct > best_count:\n",
    "        # print('Saving model...')\n",
    "        # state = {\n",
    "        #     'state': net.state_dict(), #net,\n",
    "        #     'acc': acc,\n",
    "        #     'epoch': epoch,\n",
    "        # }\n",
    "        \n",
    "        # torch.save(state, f'./saved_models/resnet_{quantize_nbits}bits_{u.time_stamp()}.pth')\n",
    "        #net.save_state_dict('resnet20.pt')\n",
    "        best_acc = acc\n",
    "        best_count = correct\n",
    "\n",
    "    for k_W, k_G in zip(all_W_kernels, all_G_kernels):\n",
    "        k_W.data, k_G.data = k_G.data, k_W.data\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "save_model(net, \"saved_models\", f'resnet_{quantize_nbits}bits_{u.time_stamp()}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
