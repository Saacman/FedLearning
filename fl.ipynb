{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning Simple Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, layers_shape):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(layers_shape[i], layers_shape[i+1]) for i in range(len(layers_shape) - 1)]) # Define layers list\n",
    "        self.relu = nn.ReLU() # activation function\n",
    "        self.soft = nn.Softmax() # output function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # flatten the input image\n",
    "        for layer in self.linears[:-1]:\n",
    "            x = self.relu(layer(x))\n",
    "        # x = self.soft(self.linears[-1](x))\n",
    "        x = self.linears[-1](x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5\n",
    "comm_cycles = 150\n",
    "num_clients = 10\n",
    "sample_size = int(.3 * num_clients) # Use 30% of available clients\n",
    "net_parameters = [ 28 * 28, # input\n",
    "                512, 256, 128, 64,\n",
    "                10 ] #output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders\n",
    "- Divide the test & training data\n",
    "- Divide the training data among the clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformation to apply to each image in the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # convert the image to a PyTorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)) # normalize the image with mean=0.5 and std=0.5\n",
    "])\n",
    "\n",
    "# load the MNIST training and testing datasets\n",
    "train_dataset = datasets.MNIST(root='data/', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='data/', train=False, transform=transform, download=True)\n",
    "\n",
    "# split the training data\n",
    "train_split = torch.utils.data.random_split(train_dataset, [int(train_dataset.data.shape[0]/num_clients) for i in range(num_clients)])\n",
    "\n",
    "# create data loaders to load the datasets in batches during training and testing\n",
    "train_loader = [torch.utils.data.DataLoader(split, batch_size=64, shuffle=True) for split in train_split]\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Federated Training\n",
    "- `client_update` function train the client model on private client data. This is the local training round that takes place at num_selected clients.\n",
    "- `server_aggregate` function aggregates the model weights received from every client and updates the global model with the updated weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_update(  client_model : torch.nn.Module,\n",
    "                    optimizer,\n",
    "                    criterion,\n",
    "                    data_loader : torch.utils.data.DataLoader,\n",
    "                    device,\n",
    "                    epoch = 5):\n",
    "    \"\"\"\n",
    "    Train the client model on client data\n",
    "    \"\"\"\n",
    "    client_model.train()\n",
    "    for e in range(epoch):\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad() # reset the gradients to zero\n",
    "            output = client_model(images) # forward pass\n",
    "            loss = criterion(output, labels) # compute the loss\n",
    "            loss.backward() # compute the gradients\n",
    "            optimizer.step() # update the parameters\n",
    "\n",
    "    return loss.item() * images.size(0)\n",
    "\n",
    "def server_aggregate(global_model : torch.nn.Module, client_models):\n",
    "    \"\"\"\n",
    "    The means of the weights of the client models are aggregated to the global model\n",
    "    \"\"\"\n",
    "    global_dict = global_model.state_dict() # Get a copy of the global model state_dict\n",
    "    for key in global_dict.keys():\n",
    "        global_dict[key] = torch.stack([client_models[i].state_dict()[key].float for i in range(len(client_models))],0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    \n",
    "    # Update the client models using the global model\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function\n",
    "Similar to previous implementation, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(global_model, criterion, test_loader, device):\n",
    "    \"\"\"This function test the global model on test data and returns test loss and test accuracy \"\"\"\n",
    "    global_model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = global_model(images) # forward pass\n",
    "            loss = criterion(output, labels)\n",
    "            test_loss += loss.item() * images.size(0)# sum up batch loss\n",
    "            _, pred = torch.max(output, 1) # get the predicted labels\n",
    "            test_acc += torch.sum(pred == labels.data).item() # compute the testing accuracy\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc /= len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global & Clients instatiation\n",
    "Implement the same elements as before, but:\n",
    "- We need more instances of the model\n",
    "- An optimizer for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = MLP(net_parameters)\n",
    "client_models = [MLP(net_parameters) for _ in range(num_clients)]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "global_model.to(device)\n",
    "for model in client_models:\n",
    "    model.to(device)\n",
    "    model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # computes the cross-entropy loss between the predicted and true labels\n",
    "optimizers =[optim.Adam(model.parameters(), lr=0.001) for model in client_models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (749670344.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[80], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "# initialize lists to store the training and testing losses and accuracies\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for cycle in range(comm_cycles):\n",
    "    # Select random clients\n",
    "    client_idx = np.random.permutation(num_clients)[:sample_size]\n",
    "    train_loss = 0\n",
    "    for i in range(sample_size):\n",
    "        train_loss += client_update(client_models[client_idx[i]],\n",
    "                                    optimizers[client_idx[i]],\n",
    "                                    criterion, train_loader[client_idx[i]],\n",
    "                                    device, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Aggregate\n",
    "    server_aggregate(global_model, client_models)\n",
    "    test_loss, test_acc = test(global_model, client_models)\n",
    "\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    print('%d-th round' % r)\n",
    "    print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (train_loss / sample_size, test_loss, test_acc))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
