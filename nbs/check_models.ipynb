{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# setting path\n",
    "sys.path.append('../../FedLearning')\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from fedlern.models.mlp import MLP\n",
    "from fedlern.models.resnet_v2 import ResNet18\n",
    "from fedlern.train_utils import *\n",
    "from fedlern.quant_utils import *\n",
    "import fedlern.utils as utils\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cuda_device = torch.device(\"cuda:0\")\n",
    "cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "#stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "stats = (0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784)\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:07<00:00, 23022738.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = prepare_dataloader_cifar(num_workers=8, train_batch_size=batch_size ,eval_batch_size=batch_size, stats=stats)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESNET no quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../saved_models/resnet18_cifar10_92-5.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Test the model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model \u001b[39m=\u001b[39m ResNet18()\n\u001b[0;32m----> 4\u001b[0m load_model(model, \u001b[39m'\u001b[39;49m\u001b[39m../saved_models/resnet18_cifar10_92-5.pt\u001b[39;49m\u001b[39m'\u001b[39;49m, device)\n\u001b[1;32m      6\u001b[0m loss, acc \u001b[39m=\u001b[39m evaluate_model(model, test_loader, device,)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{\u001b[39;00macc\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/FedLearning/fedlern/train_utils.py:189\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model, model_filepath, device)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(model, model_filepath, device):\n\u001b[0;32m--> 189\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(model_filepath, map_location\u001b[39m=\u001b[39;49mdevice))\n\u001b[1;32m    191\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../saved_models/resnet18_cifar10_92-5.pt'"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "\n",
    "model = ResNet18()\n",
    "load_model(model, '../saved_models/resnet18_cifar10_92-5.pt', device)\n",
    "\n",
    "loss, acc = evaluate_model(model, test_loader, device,)\n",
    "print(f'Loss: {loss}, Accuracy: {acc*100}%')\n",
    "print_model_size(model)\n",
    "\n",
    "\n",
    "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
    "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
    "print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0, Accuracy: 92.24999237060547%\n",
      "44.77 MB\n",
      "FP32 CPU Inference Latency: 7.22 ms / sample\n",
      "FP32 CUDA Inference Latency: 5.40 ms / sample\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = ResNet18()\n",
    "load_model(model, '../saved_models/resnet18_cifar10_92-2.pt', device)\n",
    "\n",
    "# Test the model\n",
    "loss, acc = evaluate_model(model, test_loader, device,)\n",
    "print(f'Loss: {loss}, Accuracy: {acc*100}%')\n",
    "print_model_size(model)\n",
    "\n",
    "# Measure inference latency\n",
    "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
    "print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
    "print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0, Accuracy: 81.02999877929688%\n",
      "44.77 MB\n",
      "CPU Inference Latency: 9.04 ms / sample\n",
      "CUDA Inference Latency: 2.93 ms / sample\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = ResNet18()\n",
    "load_model(model, '../saved_models/resnet_fedlern_dyn851bits_2023-08-25_15-32.pth', device)\n",
    "\n",
    "# Test the model\n",
    "loss, acc = evaluate_model(model, test_loader, device,)\n",
    "print(f'Loss: {loss}, Accuracy: {acc*100}%')\n",
    "print_model_size(model)\n",
    "\n",
    "# Measure inference latency\n",
    "fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
    "print(\"CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
    "print(\"CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESNET Quantization 4 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0, Accuracy: 83.5199966430664%\n",
      "44.77 MB\n",
      "CPU Inference Latency: 7.00 ms / sample\n",
      "CUDA Inference Latency: 5.32 ms / sample\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = ResNet18()\n",
    "load_model(model, '../saved_models/resnet_4bits_2023-06-29_22-17.pth', device)\n",
    "\n",
    "# Test the model\n",
    "loss, acc = evaluate_model(model, test_loader, device,)\n",
    "print(f'Loss: {loss}, Accuracy: {acc*100}%')\n",
    "print_model_size(model)\n",
    "\n",
    "# Measure inference latency\n",
    "cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
    "print(\"CPU Inference Latency: {:.2f} ms / sample\".format(cpu_inference_latency * 1000))\n",
    "print(\"CUDA Inference Latency: {:.2f} ms / sample\".format(gpu_inference_latency * 1000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization 8 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0, Accuracy: 81.16999816894531%\n",
      "44.77 MB\n",
      "CPU Inference Latency: 7.01 ms / sample\n",
      "CUDA Inference Latency: 4.90 ms / sample\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = ResNet18()\n",
    "load_model(model, '../saved_models/resnet_8bits_2023-06-30_05-04.pth', device)\n",
    "\n",
    "# Test the model\n",
    "loss, acc = evaluate_model(model, test_loader, device,)\n",
    "print(f'Loss: {loss}, Accuracy: {acc*100}%')\n",
    "print_model_size(model)\n",
    "\n",
    "# Measure inference latency\n",
    "cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
    "print(\"CPU Inference Latency: {:.2f} ms / sample\".format(cpu_inference_latency * 1000))\n",
    "print(\"CUDA Inference Latency: {:.2f} ms / sample\".format(gpu_inference_latency * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0, Accuracy: 83.0199966430664%\n",
      "44.77 MB\n",
      "CPU Inference Latency: 6.21 ms / sample\n",
      "CUDA Inference Latency: 5.09 ms / sample\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = ResNet18()\n",
    "load_model(model, '../saved_models/resnet_8bits_2023-06-30_00-45.pth', device)\n",
    "\n",
    "# Test the model\n",
    "loss, acc = evaluate_model(model, test_loader, device,)\n",
    "print(f'Loss: {loss}, Accuracy: {acc*100}%')\n",
    "print_model_size(model)\n",
    "\n",
    "# Measure inference latency\n",
    "cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
    "print(\"CPU Inference Latency: {:.2f} ms / sample\".format(cpu_inference_latency * 1000))\n",
    "print(\"CUDA Inference Latency: {:.2f} ms / sample\".format(gpu_inference_latency * 1000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization 16 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0, Accuracy: 34.369998931884766%\n",
      "44.77 MB\n",
      "CPU Inference Latency: 7.55 ms / sample\n",
      "CUDA Inference Latency: 2.77 ms / sample\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = ResNet18()\n",
    "load_model(model, '/home/isaac/FedLearning/saved_models/resnet_fedlern_3bits_2023-06-26_01-49.pth', device)\n",
    "\n",
    "# Test the model\n",
    "loss, acc = evaluate_model(model, test_loader, device,)\n",
    "print(f'Loss: {loss}, Accuracy: {acc*100}%')\n",
    "print_model_size(model)\n",
    "\n",
    "# Measure inference latency\n",
    "cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
    "print(\"CPU Inference Latency: {:.2f} ms / sample\".format(cpu_inference_latency * 1000))\n",
    "print(\"CUDA Inference Latency: {:.2f} ms / sample\".format(gpu_inference_latency * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d 0: 1728\n",
      "Conv2d 1: 36864\n",
      "Conv2d 2: 36864\n",
      "Conv2d 3: 36864\n",
      "Conv2d 4: 36864\n",
      "Conv2d 5: 73728\n",
      "Conv2d 6: 147456\n",
      "Conv2d 7: 8192\n",
      "Conv2d 8: 147456\n",
      "Conv2d 9: 147456\n",
      "Conv2d 10: 294912\n",
      "Conv2d 11: 589824\n",
      "Conv2d 12: 32768\n",
      "Conv2d 13: 589824\n",
      "Conv2d 14: 589824\n",
      "Conv2d 15: 1179648\n",
      "Conv2d 16: 2359296\n",
      "Conv2d 17: 131072\n",
      "Conv2d 18: 2359296\n",
      "Conv2d 19: 2359296\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for module in model.modules():\n",
    "    conv_parameters = 0\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "    #3print(module._get_name())\n",
    "        conv_parameters += sum(p.numel() for p in module.parameters())\n",
    "        print(f'{module._get_name()} {i}: {conv_parameters} {module.pa}')\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "def get_model_dict(model):\n",
    "    return model.state_dict().copy()\n",
    "dicta = get_model_dict(model)\n",
    "j = 0\n",
    "for key in dicta:\n",
    "    # if 'weight' in key:\n",
    "    #     print(key)\n",
    "        \n",
    "    if 'conv' in key:\n",
    "        j +=1\n",
    "\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1, 2, 3, 4, 5)\n",
    "print(torch.numel(a))\n",
    "a = torch.zeros(4,4)\n",
    "torch.numel(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1728\n",
      "36864\n",
      "36864\n",
      "36864\n",
      "36864\n",
      "73728\n",
      "147456\n",
      "8192\n",
      "147456\n",
      "147456\n",
      "294912\n",
      "589824\n",
      "32768\n",
      "589824\n",
      "589824\n",
      "1179648\n",
      "2359296\n",
      "131072\n",
      "2359296\n",
      "2359296\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for module in model.modules():\n",
    "    conv_parameters = 0\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "    #3print(module._get_name())\n",
    "        for p in module.parameters():\n",
    "            print(p.numel())\n",
    "        \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 True\n",
      "layer1.0.conv1 True\n",
      "layer1.0.conv2 True\n",
      "layer1.1.conv1 True\n",
      "layer1.1.conv2 True\n",
      "layer2.0.conv1 True\n",
      "layer2.0.conv2 True\n",
      "layer2.1.conv1 True\n",
      "layer2.1.conv2 True\n",
      "layer3.0.conv1 True\n",
      "layer3.0.conv2 True\n",
      "layer3.1.conv1 True\n",
      "layer3.1.conv2 True\n",
      "layer4.0.conv1 True\n",
      "layer4.0.conv2 True\n",
      "layer4.1.conv1 True\n",
      "layer4.1.conv2 True\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "k =0\n",
    "for name, module in model.named_modules():\n",
    "    if 'conv' in name:\n",
    "        print(name, isinstance(module, nn.Conv2d))\n",
    "        k+=1\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1\n",
      "<generator object Module.parameters at 0x7f9b1698e5f0>\n",
      "layer1.0.conv1\n",
      "<generator object Module.parameters at 0x7f9b0b1d4f90>\n",
      "layer1.0.conv2\n",
      "<generator object Module.parameters at 0x7f9b0b1d4cf0>\n",
      "layer1.1.conv1\n",
      "<generator object Module.parameters at 0x7f9b0b1d4c10>\n",
      "layer1.1.conv2\n",
      "<generator object Module.parameters at 0x7f9b0b1d4c10>\n",
      "layer2.0.conv1\n",
      "<generator object Module.parameters at 0x7f9b0b1d4b30>\n",
      "layer2.0.conv2\n",
      "<generator object Module.parameters at 0x7f9b0b1d4b30>\n",
      "layer2.0.shortcut.0\n",
      "<generator object Module.parameters at 0x7f9b0b1d46d0>\n",
      "layer2.1.conv1\n",
      "<generator object Module.parameters at 0x7f9b0b1d4b30>\n",
      "layer2.1.conv2\n",
      "<generator object Module.parameters at 0x7f9b0b1d4b30>\n",
      "layer3.0.conv1\n",
      "<generator object Module.parameters at 0x7f9b0b1d4b30>\n",
      "layer3.0.conv2\n",
      "<generator object Module.parameters at 0x7f9b0b1d4b30>\n",
      "layer3.0.shortcut.0\n",
      "<generator object Module.parameters at 0x7f9b0b1d4580>\n",
      "layer3.1.conv1\n",
      "<generator object Module.parameters at 0x7f9b0b1d4b30>\n",
      "layer3.1.conv2\n",
      "<generator object Module.parameters at 0x7f9b0b1d4b30>\n",
      "layer4.0.conv1\n",
      "<generator object Module.parameters at 0x7f9b0b1d4b30>\n",
      "layer4.0.conv2\n",
      "<generator object Module.parameters at 0x7f9b0b1d4b30>\n",
      "layer4.0.shortcut.0\n",
      "<generator object Module.parameters at 0x7f9b0cf9ddd0>\n",
      "layer4.1.conv1\n",
      "<generator object Module.parameters at 0x7f9b0b1d4b30>\n",
      "layer4.1.conv2\n",
      "<generator object Module.parameters at 0x7f9b0b1d4b30>\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "k =0\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        print(name)\n",
    "        print(module.parameters())\n",
    "        k+=1\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Label:  frog\n",
      "Predicted Label:  deer\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Load CIFAR-10 dataset and extract an element\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(*stats)])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Pass the data through the model to get predictions\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        outputs = model(images.to(device))\n",
    "        s, predicted = torch.max(outputs, 1)\n",
    "        break  # Stop after processing one example\n",
    "\n",
    "# Step 5: Compare the correct label with the predicted label\n",
    "predicted_class = predicted.item()\n",
    "correct_class = labels.item()\n",
    "\n",
    "# Step 6: Print the results\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(\"Correct Label: \", class_names[correct_class])\n",
    "print(\"Predicted Label: \", class_names[predicted_class])\n",
    "\n",
    "# Step 7: Plot the image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize the image\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Plot the test image\n",
    "imshow(images[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
