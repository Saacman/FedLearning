{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import fedlern.utils as u\n",
    "from fedlern.train_utils import *\n",
    "from fedlern.quant_utils import *\n",
    "from fedlern.models.resnet_v2 import ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "stats = (0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784)\n",
    "batch_size_test = 250\n",
    "batch_size_train = 128\n",
    "\n",
    "quantize_nbits = 8\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "eta_rate = 1.05\n",
    "eta = 1\n",
    "#global best_acc\n",
    "best_acc = 0\n",
    "best_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = prepare_dataloader_cifar(num_workers=8, \n",
    "                                                     train_batch_size=batch_size_train, \n",
    "                                                     eval_batch_size=batch_size_test, \n",
    "                                                     stats=stats)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ResNet18()\n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net, criterion, optimizer = get_model2(net, learning_rate=0.1, weight_decay=5e-4)\n",
    "\n",
    "    \n",
    "all_G_kernels = [\n",
    "    Variable(kernel.data.clone(), requires_grad=True)\n",
    "    for kernel in optimizer.param_groups[1]['params']\n",
    "]\n",
    "\n",
    "\n",
    "all_W_kernels = [kernel for kernel in optimizer.param_groups[1]['params']]\n",
    "kernels = [{'params': all_G_kernels}]\n",
    "optimizer_quant = optim.SGD(kernels, lr=0)\n",
    "\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80,120,160], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6061135c549943fe8e412c94e6b8977d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 0 Loss: 0.367 | Acc: 85.938% (110/128)\n",
      "Training 1 Loss: 0.300 | Acc: 89.062% (228/256)\n",
      "Training 2 Loss: 0.300 | Acc: 88.542% (340/384)\n",
      "Training 3 Loss: 0.294 | Acc: 89.453% (458/512)\n",
      "Training 4 Loss: 0.281 | Acc: 90.156% (577/640)\n",
      "Training 5 Loss: 0.269 | Acc: 91.016% (699/768)\n",
      "Training 6 Loss: 0.281 | Acc: 90.513% (811/896)\n",
      "Training 7 Loss: 0.273 | Acc: 90.820% (930/1024)\n",
      "Training 8 Loss: 0.281 | Acc: 90.625% (1044/1152)\n",
      "Training 9 Loss: 0.269 | Acc: 91.172% (1167/1280)\n",
      "Training 10 Loss: 0.279 | Acc: 90.767% (1278/1408)\n",
      "Training 11 Loss: 0.273 | Acc: 91.016% (1398/1536)\n",
      "Training 12 Loss: 0.276 | Acc: 90.805% (1511/1664)\n",
      "Training 13 Loss: 0.274 | Acc: 90.848% (1628/1792)\n",
      "Training 14 Loss: 0.278 | Acc: 90.781% (1743/1920)\n",
      "Training 15 Loss: 0.284 | Acc: 90.576% (1855/2048)\n",
      "Training 16 Loss: 0.281 | Acc: 90.717% (1974/2176)\n",
      "Training 17 Loss: 0.275 | Acc: 90.929% (2095/2304)\n",
      "Training 18 Loss: 0.279 | Acc: 90.666% (2205/2432)\n",
      "Training 19 Loss: 0.280 | Acc: 90.664% (2321/2560)\n",
      "Training 20 Loss: 0.284 | Acc: 90.365% (2429/2688)\n",
      "Training 21 Loss: 0.287 | Acc: 90.199% (2540/2816)\n",
      "Training 22 Loss: 0.284 | Acc: 90.387% (2661/2944)\n",
      "Training 23 Loss: 0.278 | Acc: 90.560% (2782/3072)\n",
      "Training 24 Loss: 0.279 | Acc: 90.531% (2897/3200)\n",
      "Training 25 Loss: 0.281 | Acc: 90.355% (3007/3328)\n",
      "Training 26 Loss: 0.282 | Acc: 90.249% (3119/3456)\n",
      "Training 27 Loss: 0.278 | Acc: 90.318% (3237/3584)\n",
      "Training 28 Loss: 0.276 | Acc: 90.329% (3353/3712)\n",
      "Training 29 Loss: 0.277 | Acc: 90.339% (3469/3840)\n",
      "Training 30 Loss: 0.279 | Acc: 90.247% (3581/3968)\n",
      "Training 31 Loss: 0.279 | Acc: 90.332% (3700/4096)\n",
      "Training 32 Loss: 0.281 | Acc: 90.246% (3812/4224)\n",
      "Training 33 Loss: 0.278 | Acc: 90.326% (3931/4352)\n",
      "Training 34 Loss: 0.277 | Acc: 90.402% (4050/4480)\n",
      "Training 35 Loss: 0.276 | Acc: 90.430% (4167/4608)\n",
      "Training 36 Loss: 0.280 | Acc: 90.351% (4279/4736)\n",
      "Training 37 Loss: 0.284 | Acc: 90.214% (4388/4864)\n",
      "Training 38 Loss: 0.286 | Acc: 90.164% (4501/4992)\n",
      "Training 39 Loss: 0.287 | Acc: 90.059% (4611/5120)\n",
      "Training 40 Loss: 0.287 | Acc: 90.053% (4726/5248)\n",
      "Training 41 Loss: 0.289 | Acc: 90.011% (4839/5376)\n",
      "Training 42 Loss: 0.287 | Acc: 90.080% (4958/5504)\n",
      "Training 43 Loss: 0.289 | Acc: 90.057% (5072/5632)\n",
      "Training 44 Loss: 0.290 | Acc: 89.983% (5183/5760)\n",
      "Training 45 Loss: 0.288 | Acc: 90.099% (5305/5888)\n",
      "Training 46 Loss: 0.288 | Acc: 90.110% (5421/6016)\n",
      "Training 47 Loss: 0.288 | Acc: 90.153% (5539/6144)\n",
      "Training 48 Loss: 0.288 | Acc: 90.195% (5657/6272)\n",
      "Training 49 Loss: 0.288 | Acc: 90.188% (5772/6400)\n",
      "Training 50 Loss: 0.290 | Acc: 90.104% (5882/6528)\n",
      "Training 51 Loss: 0.292 | Acc: 89.979% (5989/6656)\n",
      "Training 52 Loss: 0.292 | Acc: 89.947% (6102/6784)\n",
      "Training 53 Loss: 0.290 | Acc: 90.046% (6224/6912)\n",
      "Training 54 Loss: 0.292 | Acc: 90.028% (6338/7040)\n",
      "Training 55 Loss: 0.296 | Acc: 89.941% (6447/7168)\n",
      "Training 56 Loss: 0.297 | Acc: 89.926% (6561/7296)\n",
      "Training 57 Loss: 0.297 | Acc: 89.952% (6678/7424)\n",
      "Training 58 Loss: 0.298 | Acc: 89.936% (6792/7552)\n",
      "Training 59 Loss: 0.296 | Acc: 89.948% (6908/7680)\n",
      "Training 60 Loss: 0.296 | Acc: 89.985% (7026/7808)\n",
      "Training 61 Loss: 0.295 | Acc: 90.008% (7143/7936)\n",
      "Training 62 Loss: 0.295 | Acc: 90.017% (7259/8064)\n",
      "Training 63 Loss: 0.295 | Acc: 90.015% (7374/8192)\n",
      "Training 64 Loss: 0.294 | Acc: 90.036% (7491/8320)\n",
      "Training 65 Loss: 0.294 | Acc: 90.104% (7612/8448)\n",
      "Training 66 Loss: 0.292 | Acc: 90.182% (7734/8576)\n",
      "Training 67 Loss: 0.293 | Acc: 90.188% (7850/8704)\n",
      "Training 68 Loss: 0.293 | Acc: 90.161% (7963/8832)\n",
      "Training 69 Loss: 0.293 | Acc: 90.167% (8079/8960)\n",
      "Training 70 Loss: 0.292 | Acc: 90.185% (8196/9088)\n",
      "Training 71 Loss: 0.293 | Acc: 90.148% (8308/9216)\n",
      "Training 72 Loss: 0.292 | Acc: 90.133% (8422/9344)\n",
      "Training 73 Loss: 0.290 | Acc: 90.203% (8544/9472)\n",
      "Training 74 Loss: 0.289 | Acc: 90.240% (8663/9600)\n",
      "Training 75 Loss: 0.289 | Acc: 90.265% (8781/9728)\n",
      "Training 76 Loss: 0.288 | Acc: 90.331% (8903/9856)\n",
      "Training 77 Loss: 0.287 | Acc: 90.355% (9021/9984)\n",
      "Training 78 Loss: 0.286 | Acc: 90.398% (9141/10112)\n",
      "Training 79 Loss: 0.286 | Acc: 90.410% (9258/10240)\n",
      "Training 80 Loss: 0.286 | Acc: 90.413% (9374/10368)\n",
      "Training 81 Loss: 0.285 | Acc: 90.387% (9487/10496)\n",
      "Training 82 Loss: 0.286 | Acc: 90.371% (9601/10624)\n",
      "Training 83 Loss: 0.286 | Acc: 90.318% (9711/10752)\n",
      "Training 84 Loss: 0.287 | Acc: 90.322% (9827/10880)\n",
      "Training 85 Loss: 0.286 | Acc: 90.352% (9946/11008)\n",
      "Training 86 Loss: 0.285 | Acc: 90.392% (10066/11136)\n",
      "Training 87 Loss: 0.285 | Acc: 90.403% (10183/11264)\n",
      "Training 88 Loss: 0.284 | Acc: 90.423% (10301/11392)\n",
      "Training 89 Loss: 0.283 | Acc: 90.443% (10419/11520)\n",
      "Training 90 Loss: 0.283 | Acc: 90.462% (10537/11648)\n",
      "Training 91 Loss: 0.283 | Acc: 90.455% (10652/11776)\n",
      "Training 92 Loss: 0.284 | Acc: 90.415% (10763/11904)\n",
      "Training 93 Loss: 0.285 | Acc: 90.376% (10874/12032)\n",
      "Training 94 Loss: 0.284 | Acc: 90.370% (10989/12160)\n",
      "Training 95 Loss: 0.284 | Acc: 90.381% (11106/12288)\n",
      "Training 96 Loss: 0.284 | Acc: 90.375% (11221/12416)\n",
      "Training 97 Loss: 0.285 | Acc: 90.362% (11335/12544)\n",
      "Training 98 Loss: 0.285 | Acc: 90.357% (11450/12672)\n",
      "Training 99 Loss: 0.285 | Acc: 90.344% (11564/12800)\n",
      "Training 100 Loss: 0.285 | Acc: 90.316% (11676/12928)\n",
      "Training 101 Loss: 0.285 | Acc: 90.357% (11797/13056)\n",
      "Training 102 Loss: 0.285 | Acc: 90.344% (11911/13184)\n",
      "Training 103 Loss: 0.284 | Acc: 90.392% (12033/13312)\n",
      "Training 104 Loss: 0.285 | Acc: 90.357% (12144/13440)\n",
      "Training 105 Loss: 0.285 | Acc: 90.330% (12256/13568)\n",
      "Training 106 Loss: 0.285 | Acc: 90.333% (12372/13696)\n",
      "Training 107 Loss: 0.285 | Acc: 90.336% (12488/13824)\n",
      "Training 108 Loss: 0.286 | Acc: 90.310% (12600/13952)\n",
      "Training 109 Loss: 0.286 | Acc: 90.327% (12718/14080)\n",
      "Training 110 Loss: 0.285 | Acc: 90.365% (12839/14208)\n",
      "Training 111 Loss: 0.285 | Acc: 90.388% (12958/14336)\n",
      "Training 112 Loss: 0.284 | Acc: 90.390% (13074/14464)\n",
      "Training 113 Loss: 0.285 | Acc: 90.385% (13189/14592)\n",
      "Training 114 Loss: 0.285 | Acc: 90.374% (13303/14720)\n",
      "Training 115 Loss: 0.288 | Acc: 90.335% (13413/14848)\n",
      "Training 116 Loss: 0.289 | Acc: 90.311% (13525/14976)\n",
      "Training 117 Loss: 0.289 | Acc: 90.314% (13641/15104)\n",
      "Training 118 Loss: 0.289 | Acc: 90.310% (13756/15232)\n",
      "Training 119 Loss: 0.289 | Acc: 90.306% (13871/15360)\n",
      "Training 120 Loss: 0.288 | Acc: 90.315% (13988/15488)\n",
      "Training 121 Loss: 0.288 | Acc: 90.298% (14101/15616)\n",
      "Training 122 Loss: 0.288 | Acc: 90.288% (14215/15744)\n",
      "Training 123 Loss: 0.289 | Acc: 90.291% (14331/15872)\n",
      "Training 124 Loss: 0.289 | Acc: 90.294% (14447/16000)\n",
      "Training 125 Loss: 0.289 | Acc: 90.284% (14561/16128)\n",
      "Training 126 Loss: 0.289 | Acc: 90.274% (14675/16256)\n",
      "Training 127 Loss: 0.290 | Acc: 90.240% (14785/16384)\n",
      "Training 128 Loss: 0.291 | Acc: 90.201% (14894/16512)\n",
      "Training 129 Loss: 0.291 | Acc: 90.216% (15012/16640)\n",
      "Training 130 Loss: 0.291 | Acc: 90.208% (15126/16768)\n",
      "Training 131 Loss: 0.291 | Acc: 90.211% (15242/16896)\n",
      "Training 132 Loss: 0.290 | Acc: 90.220% (15359/17024)\n",
      "Training 133 Loss: 0.290 | Acc: 90.229% (15476/17152)\n",
      "Training 134 Loss: 0.291 | Acc: 90.162% (15580/17280)\n",
      "Training 135 Loss: 0.291 | Acc: 90.148% (15693/17408)\n",
      "Training 136 Loss: 0.291 | Acc: 90.152% (15809/17536)\n",
      "Training 137 Loss: 0.291 | Acc: 90.149% (15924/17664)\n",
      "Training 138 Loss: 0.291 | Acc: 90.147% (16039/17792)\n",
      "Training 139 Loss: 0.290 | Acc: 90.167% (16158/17920)\n",
      "Training 140 Loss: 0.290 | Acc: 90.171% (16274/18048)\n",
      "Training 141 Loss: 0.291 | Acc: 90.141% (16384/18176)\n",
      "Training 142 Loss: 0.292 | Acc: 90.106% (16493/18304)\n",
      "Training 143 Loss: 0.292 | Acc: 90.093% (16606/18432)\n",
      "Training 144 Loss: 0.292 | Acc: 90.097% (16722/18560)\n",
      "Training 145 Loss: 0.291 | Acc: 90.106% (16839/18688)\n",
      "Training 146 Loss: 0.291 | Acc: 90.115% (16956/18816)\n",
      "Training 147 Loss: 0.291 | Acc: 90.124% (17073/18944)\n",
      "Training 148 Loss: 0.291 | Acc: 90.111% (17186/19072)\n",
      "Training 149 Loss: 0.291 | Acc: 90.125% (17304/19200)\n",
      "Training 150 Loss: 0.291 | Acc: 90.092% (17413/19328)\n",
      "Training 151 Loss: 0.291 | Acc: 90.101% (17530/19456)\n",
      "Training 152 Loss: 0.292 | Acc: 90.089% (17643/19584)\n",
      "Training 153 Loss: 0.292 | Acc: 90.072% (17755/19712)\n",
      "Training 154 Loss: 0.292 | Acc: 90.081% (17872/19840)\n",
      "Training 155 Loss: 0.292 | Acc: 90.074% (17986/19968)\n",
      "Training 156 Loss: 0.293 | Acc: 90.063% (18099/20096)\n",
      "Training 157 Loss: 0.292 | Acc: 90.076% (18217/20224)\n",
      "Training 158 Loss: 0.292 | Acc: 90.080% (18333/20352)\n",
      "Training 159 Loss: 0.293 | Acc: 90.034% (18439/20480)\n",
      "Training 160 Loss: 0.293 | Acc: 90.023% (18552/20608)\n",
      "Training 161 Loss: 0.293 | Acc: 90.008% (18664/20736)\n",
      "Training 162 Loss: 0.293 | Acc: 89.983% (18774/20864)\n",
      "Training 163 Loss: 0.293 | Acc: 89.972% (18887/20992)\n",
      "Training 164 Loss: 0.293 | Acc: 90.005% (19009/21120)\n",
      "Training 165 Loss: 0.293 | Acc: 89.990% (19121/21248)\n",
      "Training 166 Loss: 0.293 | Acc: 90.003% (19239/21376)\n",
      "Training 167 Loss: 0.293 | Acc: 89.997% (19353/21504)\n",
      "Training 168 Loss: 0.292 | Acc: 90.006% (19470/21632)\n",
      "Training 169 Loss: 0.293 | Acc: 89.982% (19580/21760)\n",
      "Training 170 Loss: 0.293 | Acc: 89.976% (19694/21888)\n",
      "Training 171 Loss: 0.293 | Acc: 89.971% (19808/22016)\n",
      "Training 172 Loss: 0.293 | Acc: 89.979% (19925/22144)\n",
      "Training 173 Loss: 0.293 | Acc: 89.978% (20040/22272)\n",
      "Training 174 Loss: 0.293 | Acc: 89.987% (20157/22400)\n",
      "Training 175 Loss: 0.292 | Acc: 90.012% (20278/22528)\n",
      "Training 176 Loss: 0.292 | Acc: 90.034% (20398/22656)\n",
      "Training 177 Loss: 0.291 | Acc: 90.059% (20519/22784)\n",
      "Training 178 Loss: 0.291 | Acc: 90.058% (20634/22912)\n",
      "Training 179 Loss: 0.291 | Acc: 90.043% (20746/23040)\n",
      "Training 180 Loss: 0.291 | Acc: 90.042% (20861/23168)\n",
      "Training 181 Loss: 0.291 | Acc: 90.028% (20973/23296)\n",
      "Training 182 Loss: 0.292 | Acc: 90.027% (21088/23424)\n",
      "Training 183 Loss: 0.291 | Acc: 90.035% (21205/23552)\n",
      "Training 184 Loss: 0.291 | Acc: 90.046% (21323/23680)\n",
      "Training 185 Loss: 0.291 | Acc: 90.062% (21442/23808)\n",
      "Training 186 Loss: 0.291 | Acc: 90.078% (21561/23936)\n",
      "Training 187 Loss: 0.291 | Acc: 90.085% (21678/24064)\n",
      "Training 188 Loss: 0.291 | Acc: 90.088% (21794/24192)\n",
      "Training 189 Loss: 0.291 | Acc: 90.095% (21911/24320)\n",
      "Training 190 Loss: 0.292 | Acc: 90.061% (22018/24448)\n",
      "Training 191 Loss: 0.291 | Acc: 90.080% (22138/24576)\n",
      "Training 192 Loss: 0.292 | Acc: 90.074% (22252/24704)\n",
      "Training 193 Loss: 0.292 | Acc: 90.057% (22363/24832)\n",
      "Training 194 Loss: 0.293 | Acc: 90.020% (22469/24960)\n",
      "Training 195 Loss: 0.294 | Acc: 90.003% (22580/25088)\n",
      "Training 196 Loss: 0.293 | Acc: 90.018% (22699/25216)\n",
      "Training 197 Loss: 0.293 | Acc: 89.998% (22809/25344)\n",
      "Training 198 Loss: 0.294 | Acc: 89.985% (22921/25472)\n",
      "Training 199 Loss: 0.293 | Acc: 89.992% (23038/25600)\n",
      "Training 200 Loss: 0.293 | Acc: 89.991% (23153/25728)\n",
      "Training 201 Loss: 0.294 | Acc: 89.975% (23264/25856)\n",
      "Training 202 Loss: 0.294 | Acc: 89.971% (23378/25984)\n",
      "Training 203 Loss: 0.293 | Acc: 89.955% (23489/26112)\n",
      "Training 204 Loss: 0.293 | Acc: 89.958% (23605/26240)\n",
      "Training 205 Loss: 0.293 | Acc: 89.935% (23714/26368)\n",
      "Training 206 Loss: 0.293 | Acc: 89.931% (23828/26496)\n",
      "Training 207 Loss: 0.293 | Acc: 89.938% (23945/26624)\n",
      "Training 208 Loss: 0.294 | Acc: 89.930% (24058/26752)\n",
      "Training 209 Loss: 0.294 | Acc: 89.940% (24176/26880)\n",
      "Training 210 Loss: 0.294 | Acc: 89.933% (24289/27008)\n",
      "Training 211 Loss: 0.294 | Acc: 89.936% (24405/27136)\n",
      "Training 212 Loss: 0.294 | Acc: 89.943% (24522/27264)\n",
      "Training 213 Loss: 0.294 | Acc: 89.942% (24637/27392)\n",
      "Training 214 Loss: 0.294 | Acc: 89.935% (24750/27520)\n",
      "Training 215 Loss: 0.294 | Acc: 89.941% (24867/27648)\n",
      "Training 216 Loss: 0.294 | Acc: 89.945% (24983/27776)\n",
      "Training 217 Loss: 0.294 | Acc: 89.933% (25095/27904)\n",
      "Training 218 Loss: 0.294 | Acc: 89.929% (25209/28032)\n",
      "Training 219 Loss: 0.294 | Acc: 89.911% (25319/28160)\n",
      "Training 220 Loss: 0.294 | Acc: 89.925% (25438/28288)\n",
      "Training 221 Loss: 0.294 | Acc: 89.918% (25551/28416)\n",
      "Training 222 Loss: 0.294 | Acc: 89.921% (25667/28544)\n",
      "Training 223 Loss: 0.294 | Acc: 89.941% (25788/28672)\n",
      "Training 224 Loss: 0.294 | Acc: 89.938% (25902/28800)\n",
      "Training 225 Loss: 0.293 | Acc: 89.951% (26021/28928)\n",
      "Training 226 Loss: 0.293 | Acc: 89.940% (26133/29056)\n",
      "Training 227 Loss: 0.294 | Acc: 89.940% (26248/29184)\n",
      "Training 228 Loss: 0.294 | Acc: 89.912% (26355/29312)\n",
      "Training 229 Loss: 0.294 | Acc: 89.908% (26469/29440)\n",
      "Training 230 Loss: 0.294 | Acc: 89.905% (26583/29568)\n",
      "Training 231 Loss: 0.294 | Acc: 89.908% (26699/29696)\n",
      "Training 232 Loss: 0.294 | Acc: 89.897% (26811/29824)\n",
      "Training 233 Loss: 0.295 | Acc: 89.897% (26926/29952)\n",
      "Training 234 Loss: 0.295 | Acc: 89.907% (27044/30080)\n",
      "Training 235 Loss: 0.294 | Acc: 89.923% (27164/30208)\n",
      "Training 236 Loss: 0.296 | Acc: 89.864% (27261/30336)\n",
      "Training 237 Loss: 0.296 | Acc: 89.854% (27373/30464)\n",
      "Training 238 Loss: 0.296 | Acc: 89.870% (27493/30592)\n",
      "Training 239 Loss: 0.296 | Acc: 89.873% (27609/30720)\n",
      "Training 240 Loss: 0.296 | Acc: 89.866% (27722/30848)\n",
      "Training 241 Loss: 0.297 | Acc: 89.857% (27834/30976)\n",
      "Training 242 Loss: 0.297 | Acc: 89.847% (27946/31104)\n",
      "Training 243 Loss: 0.297 | Acc: 89.850% (28062/31232)\n",
      "Training 244 Loss: 0.298 | Acc: 89.818% (28167/31360)\n",
      "Training 245 Loss: 0.298 | Acc: 89.815% (28281/31488)\n",
      "Training 246 Loss: 0.299 | Acc: 89.806% (28393/31616)\n",
      "Training 247 Loss: 0.299 | Acc: 89.812% (28510/31744)\n",
      "Training 248 Loss: 0.298 | Acc: 89.819% (28627/31872)\n",
      "Training 249 Loss: 0.298 | Acc: 89.816% (28741/32000)\n",
      "Training 250 Loss: 0.299 | Acc: 89.806% (28853/32128)\n",
      "Training 251 Loss: 0.299 | Acc: 89.822% (28973/32256)\n",
      "Training 252 Loss: 0.298 | Acc: 89.834% (29092/32384)\n",
      "Training 253 Loss: 0.299 | Acc: 89.813% (29200/32512)\n",
      "Training 254 Loss: 0.299 | Acc: 89.822% (29318/32640)\n",
      "Training 255 Loss: 0.299 | Acc: 89.795% (29424/32768)\n",
      "Training 256 Loss: 0.299 | Acc: 89.792% (29538/32896)\n",
      "Training 257 Loss: 0.299 | Acc: 89.783% (29650/33024)\n",
      "Training 258 Loss: 0.299 | Acc: 89.768% (29760/33152)\n",
      "Training 259 Loss: 0.299 | Acc: 89.763% (29873/33280)\n",
      "Training 260 Loss: 0.300 | Acc: 89.757% (29986/33408)\n",
      "Training 261 Loss: 0.300 | Acc: 89.745% (30097/33536)\n",
      "Training 262 Loss: 0.300 | Acc: 89.752% (30214/33664)\n",
      "Training 263 Loss: 0.300 | Acc: 89.746% (30327/33792)\n",
      "Training 264 Loss: 0.300 | Acc: 89.749% (30443/33920)\n",
      "Training 265 Loss: 0.300 | Acc: 89.756% (30560/34048)\n",
      "Training 266 Loss: 0.300 | Acc: 89.753% (30674/34176)\n",
      "Training 267 Loss: 0.300 | Acc: 89.730% (30781/34304)\n",
      "Training 268 Loss: 0.300 | Acc: 89.728% (30895/34432)\n",
      "Training 269 Loss: 0.300 | Acc: 89.725% (31009/34560)\n",
      "Training 270 Loss: 0.301 | Acc: 89.714% (31120/34688)\n",
      "Training 271 Loss: 0.301 | Acc: 89.712% (31234/34816)\n",
      "Training 272 Loss: 0.301 | Acc: 89.706% (31347/34944)\n",
      "Training 273 Loss: 0.301 | Acc: 89.704% (31461/35072)\n",
      "Training 274 Loss: 0.301 | Acc: 89.705% (31576/35200)\n",
      "Training 275 Loss: 0.301 | Acc: 89.685% (31684/35328)\n",
      "Training 276 Loss: 0.301 | Acc: 89.694% (31802/35456)\n",
      "Training 277 Loss: 0.301 | Acc: 89.706% (31921/35584)\n",
      "Training 278 Loss: 0.301 | Acc: 89.715% (32039/35712)\n",
      "Training 279 Loss: 0.301 | Acc: 89.721% (32156/35840)\n",
      "Training 280 Loss: 0.300 | Acc: 89.738% (32277/35968)\n",
      "Training 281 Loss: 0.300 | Acc: 89.738% (32392/36096)\n",
      "Training 282 Loss: 0.300 | Acc: 89.731% (32504/36224)\n",
      "Training 283 Loss: 0.300 | Acc: 89.731% (32619/36352)\n",
      "Training 284 Loss: 0.300 | Acc: 89.737% (32736/36480)\n",
      "Training 285 Loss: 0.301 | Acc: 89.734% (32850/36608)\n",
      "Training 286 Loss: 0.301 | Acc: 89.727% (32962/36736)\n",
      "Training 287 Loss: 0.300 | Acc: 89.738% (33081/36864)\n",
      "Training 288 Loss: 0.300 | Acc: 89.757% (33203/36992)\n",
      "Training 289 Loss: 0.300 | Acc: 89.758% (33318/37120)\n",
      "Training 290 Loss: 0.300 | Acc: 89.758% (33433/37248)\n",
      "Training 291 Loss: 0.300 | Acc: 89.769% (33552/37376)\n",
      "Training 292 Loss: 0.300 | Acc: 89.774% (33669/37504)\n",
      "Training 293 Loss: 0.300 | Acc: 89.751% (33775/37632)\n",
      "Training 294 Loss: 0.300 | Acc: 89.759% (33893/37760)\n",
      "Training 295 Loss: 0.300 | Acc: 89.767% (34011/37888)\n",
      "Training 296 Loss: 0.300 | Acc: 89.765% (34125/38016)\n",
      "Training 297 Loss: 0.300 | Acc: 89.760% (34238/38144)\n",
      "Training 298 Loss: 0.301 | Acc: 89.758% (34352/38272)\n",
      "Training 299 Loss: 0.300 | Acc: 89.763% (34469/38400)\n",
      "Training 300 Loss: 0.300 | Acc: 89.774% (34588/38528)\n",
      "Training 301 Loss: 0.300 | Acc: 89.764% (34699/38656)\n",
      "Training 302 Loss: 0.300 | Acc: 89.769% (34816/38784)\n",
      "Training 303 Loss: 0.300 | Acc: 89.780% (34935/38912)\n",
      "Training 304 Loss: 0.300 | Acc: 89.777% (35049/39040)\n",
      "Training 305 Loss: 0.300 | Acc: 89.777% (35164/39168)\n",
      "Training 306 Loss: 0.300 | Acc: 89.783% (35281/39296)\n",
      "Training 307 Loss: 0.300 | Acc: 89.788% (35398/39424)\n",
      "Training 308 Loss: 0.300 | Acc: 89.798% (35517/39552)\n",
      "Training 309 Loss: 0.300 | Acc: 89.796% (35631/39680)\n",
      "Training 310 Loss: 0.300 | Acc: 89.804% (35749/39808)\n",
      "Training 311 Loss: 0.300 | Acc: 89.786% (35857/39936)\n",
      "Training 312 Loss: 0.300 | Acc: 89.786% (35972/40064)\n",
      "Training 313 Loss: 0.300 | Acc: 89.782% (36085/40192)\n",
      "Training 314 Loss: 0.300 | Acc: 89.794% (36205/40320)\n",
      "Training 315 Loss: 0.299 | Acc: 89.794% (36320/40448)\n",
      "Training 316 Loss: 0.299 | Acc: 89.814% (36443/40576)\n",
      "Training 317 Loss: 0.299 | Acc: 89.814% (36558/40704)\n",
      "Training 318 Loss: 0.299 | Acc: 89.812% (36672/40832)\n",
      "Training 319 Loss: 0.299 | Acc: 89.832% (36795/40960)\n",
      "Training 320 Loss: 0.299 | Acc: 89.822% (36906/41088)\n",
      "Training 321 Loss: 0.299 | Acc: 89.834% (37026/41216)\n",
      "Training 322 Loss: 0.299 | Acc: 89.832% (37140/41344)\n",
      "Training 323 Loss: 0.298 | Acc: 89.844% (37260/41472)\n",
      "Training 324 Loss: 0.298 | Acc: 89.851% (37378/41600)\n",
      "Training 325 Loss: 0.298 | Acc: 89.851% (37493/41728)\n",
      "Training 326 Loss: 0.299 | Acc: 89.827% (37598/41856)\n",
      "Training 327 Loss: 0.299 | Acc: 89.820% (37710/41984)\n",
      "Training 328 Loss: 0.299 | Acc: 89.825% (37827/42112)\n",
      "Training 329 Loss: 0.299 | Acc: 89.804% (37933/42240)\n",
      "Training 330 Loss: 0.300 | Acc: 89.780% (38038/42368)\n",
      "Training 331 Loss: 0.300 | Acc: 89.766% (38147/42496)\n",
      "Training 332 Loss: 0.300 | Acc: 89.769% (38263/42624)\n",
      "Training 333 Loss: 0.300 | Acc: 89.771% (38379/42752)\n",
      "Training 334 Loss: 0.300 | Acc: 89.769% (38493/42880)\n",
      "Training 335 Loss: 0.300 | Acc: 89.767% (38607/43008)\n",
      "Training 336 Loss: 0.300 | Acc: 89.774% (38725/43136)\n",
      "Training 337 Loss: 0.300 | Acc: 89.767% (38837/43264)\n",
      "Training 338 Loss: 0.300 | Acc: 89.754% (38946/43392)\n",
      "Training 339 Loss: 0.300 | Acc: 89.754% (39061/43520)\n",
      "Training 340 Loss: 0.300 | Acc: 89.754% (39176/43648)\n",
      "Training 341 Loss: 0.300 | Acc: 89.750% (39289/43776)\n",
      "Training 342 Loss: 0.300 | Acc: 89.748% (39403/43904)\n",
      "Training 343 Loss: 0.300 | Acc: 89.742% (39515/44032)\n",
      "Training 344 Loss: 0.301 | Acc: 89.735% (39627/44160)\n",
      "Training 345 Loss: 0.301 | Acc: 89.726% (39738/44288)\n",
      "Training 346 Loss: 0.301 | Acc: 89.720% (39850/44416)\n",
      "Training 347 Loss: 0.301 | Acc: 89.714% (39962/44544)\n",
      "Training 348 Loss: 0.302 | Acc: 89.705% (40073/44672)\n",
      "Training 349 Loss: 0.301 | Acc: 89.710% (40190/44800)\n",
      "Training 350 Loss: 0.302 | Acc: 89.697% (40299/44928)\n",
      "Training 351 Loss: 0.301 | Acc: 89.693% (40412/45056)\n",
      "Training 352 Loss: 0.302 | Acc: 89.693% (40527/45184)\n",
      "Training 353 Loss: 0.301 | Acc: 89.698% (40644/45312)\n",
      "Training 354 Loss: 0.302 | Acc: 89.674% (40748/45440)\n",
      "Training 355 Loss: 0.302 | Acc: 89.684% (40867/45568)\n",
      "Training 356 Loss: 0.302 | Acc: 89.675% (40978/45696)\n",
      "Training 357 Loss: 0.302 | Acc: 89.680% (41095/45824)\n",
      "Training 358 Loss: 0.302 | Acc: 89.683% (41211/45952)\n",
      "Training 359 Loss: 0.302 | Acc: 89.677% (41323/46080)\n",
      "Training 360 Loss: 0.302 | Acc: 89.666% (41433/46208)\n",
      "Training 361 Loss: 0.303 | Acc: 89.656% (41543/46336)\n",
      "Training 362 Loss: 0.303 | Acc: 89.663% (41661/46464)\n",
      "Training 363 Loss: 0.303 | Acc: 89.661% (41775/46592)\n",
      "Training 364 Loss: 0.303 | Acc: 89.673% (41895/46720)\n",
      "Training 365 Loss: 0.303 | Acc: 89.664% (42006/46848)\n",
      "Training 366 Loss: 0.303 | Acc: 89.656% (42117/46976)\n",
      "Training 367 Loss: 0.303 | Acc: 89.661% (42234/47104)\n",
      "Training 368 Loss: 0.303 | Acc: 89.662% (42349/47232)\n",
      "Training 369 Loss: 0.303 | Acc: 89.658% (42462/47360)\n",
      "Training 370 Loss: 0.303 | Acc: 89.667% (42581/47488)\n",
      "Training 371 Loss: 0.303 | Acc: 89.665% (42695/47616)\n",
      "Training 372 Loss: 0.303 | Acc: 89.668% (42811/47744)\n",
      "Training 373 Loss: 0.303 | Acc: 89.664% (42924/47872)\n",
      "Training 374 Loss: 0.303 | Acc: 89.658% (43036/48000)\n",
      "Training 375 Loss: 0.303 | Acc: 89.665% (43154/48128)\n",
      "Training 376 Loss: 0.303 | Acc: 89.657% (43265/48256)\n",
      "Training 377 Loss: 0.303 | Acc: 89.656% (43379/48384)\n",
      "Training 378 Loss: 0.303 | Acc: 89.650% (43491/48512)\n",
      "Training 379 Loss: 0.303 | Acc: 89.644% (43603/48640)\n",
      "Training 380 Loss: 0.303 | Acc: 89.653% (43722/48768)\n",
      "Training 381 Loss: 0.303 | Acc: 89.670% (43845/48896)\n",
      "Training 382 Loss: 0.303 | Acc: 89.662% (43956/49024)\n",
      "Training 383 Loss: 0.303 | Acc: 89.663% (44071/49152)\n",
      "Training 384 Loss: 0.303 | Acc: 89.659% (44184/49280)\n",
      "Training 385 Loss: 0.303 | Acc: 89.664% (44301/49408)\n",
      "Training 386 Loss: 0.303 | Acc: 89.672% (44420/49536)\n",
      "Training 387 Loss: 0.303 | Acc: 89.679% (44538/49664)\n",
      "Training 388 Loss: 0.302 | Acc: 89.691% (44659/49792)\n",
      "Training 389 Loss: 0.302 | Acc: 89.692% (44774/49920)\n",
      "Training 390 Loss: 0.302 | Acc: 89.688% (44844/50000)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d472b2eca76f4156bfe4f0dc3ef4469d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 0 Loss: 118.244 | Acc: 86.000% (215/250)\n",
      "Testing 1 Loss: 59.122 | Acc: 85.200% (426/500)\n",
      "Testing 2 Loss: 39.415 | Acc: 84.933% (637/750)\n",
      "Testing 3 Loss: 29.561 | Acc: 84.100% (841/1000)\n",
      "Testing 4 Loss: 23.649 | Acc: 83.280% (1041/1250)\n",
      "Testing 5 Loss: 19.707 | Acc: 83.667% (1255/1500)\n",
      "Testing 6 Loss: 16.892 | Acc: 83.600% (1463/1750)\n",
      "Testing 7 Loss: 14.780 | Acc: 83.600% (1672/2000)\n",
      "Testing 8 Loss: 13.138 | Acc: 83.733% (1884/2250)\n",
      "Testing 9 Loss: 11.824 | Acc: 83.280% (2082/2500)\n",
      "Testing 10 Loss: 10.749 | Acc: 83.382% (2293/2750)\n",
      "Testing 11 Loss: 9.854 | Acc: 83.367% (2501/3000)\n",
      "Testing 12 Loss: 9.096 | Acc: 83.508% (2714/3250)\n",
      "Testing 13 Loss: 8.446 | Acc: 83.457% (2921/3500)\n",
      "Testing 14 Loss: 7.883 | Acc: 83.547% (3133/3750)\n",
      "Testing 15 Loss: 7.390 | Acc: 83.675% (3347/4000)\n",
      "Testing 16 Loss: 6.956 | Acc: 83.788% (3561/4250)\n",
      "Testing 17 Loss: 6.569 | Acc: 83.889% (3775/4500)\n",
      "Testing 18 Loss: 6.223 | Acc: 83.874% (3984/4750)\n",
      "Testing 19 Loss: 5.912 | Acc: 83.720% (4186/5000)\n",
      "Testing 20 Loss: 5.631 | Acc: 83.581% (4388/5250)\n",
      "Testing 21 Loss: 5.375 | Acc: 83.727% (4605/5500)\n",
      "Testing 22 Loss: 5.141 | Acc: 83.600% (4807/5750)\n",
      "Testing 23 Loss: 4.927 | Acc: 83.667% (5020/6000)\n",
      "Testing 24 Loss: 4.730 | Acc: 83.664% (5229/6250)\n",
      "Testing 25 Loss: 4.548 | Acc: 83.754% (5444/6500)\n",
      "Testing 26 Loss: 4.379 | Acc: 83.793% (5656/6750)\n",
      "Testing 27 Loss: 4.223 | Acc: 83.786% (5865/7000)\n",
      "Testing 28 Loss: 4.077 | Acc: 83.779% (6074/7250)\n",
      "Testing 29 Loss: 3.941 | Acc: 83.773% (6283/7500)\n",
      "Testing 30 Loss: 3.814 | Acc: 83.794% (6494/7750)\n",
      "Testing 31 Loss: 3.695 | Acc: 83.700% (6696/8000)\n",
      "Testing 32 Loss: 3.583 | Acc: 83.745% (6909/8250)\n",
      "Testing 33 Loss: 3.478 | Acc: 83.741% (7118/8500)\n",
      "Testing 34 Loss: 3.378 | Acc: 83.669% (7321/8750)\n",
      "Testing 35 Loss: 3.285 | Acc: 83.567% (7521/9000)\n",
      "Testing 36 Loss: 3.196 | Acc: 83.632% (7736/9250)\n",
      "Testing 37 Loss: 3.112 | Acc: 83.642% (7946/9500)\n",
      "Testing 38 Loss: 3.032 | Acc: 83.662% (8157/9750)\n",
      "Testing 39 Loss: 2.956 | Acc: 83.660% (8366/10000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for epoch in tqdm(range(num_epochs)):    num_epochs = 200\n",
    "for epoch in (pbar := tqdm(range(num_epochs))):\n",
    "    #print('Epoch ID', epoch)\n",
    "    #----------------------------------------------------------------------\n",
    "    # Training\n",
    "    #----------------------------------------------------------------------\n",
    "    pbar.set_description(f\"Training {epoch}\",refresh=True)\n",
    "    correct = 0; total = 0; train_loss = 0\n",
    "    net.train()\n",
    "    for batch_idx, (x, target) in enumerate(tqdm(train_loader, leave=False)):\n",
    "    #for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        #if batch_idx < 1:\n",
    "        optimizer.zero_grad()\n",
    "        x, target = Variable(x.cuda()), Variable(target.cuda())\n",
    "        all_W_kernels = optimizer.param_groups[1]['params']\n",
    "        all_G_kernels = optimizer_quant.param_groups[0]['params']\n",
    "        \n",
    "        for i in range(len(all_W_kernels)):\n",
    "            k_W = all_W_kernels[i]\n",
    "            k_G = all_G_kernels[i]\n",
    "            V = k_W.data\n",
    "            #print(type(V))\n",
    "            #####Binary Connect#########################\n",
    "            #k_G.data = quantize_bw(V)\n",
    "            ############################################\n",
    "            \n",
    "            ######Binary Relax##########################\n",
    "            if epoch<120:\n",
    "                #k_G.data = (eta*quantize_bw(V)+V)/(1+eta)\n",
    "                k_G.data = (eta*quantize(V,num_bits=quantize_nbits)+V)/(1+eta)\n",
    "                \n",
    "            else:\n",
    "                k_G.data = quantize(V, num_bits=quantize_nbits)\n",
    "            #############################################\n",
    "            \n",
    "            k_W.data, k_G.data = k_G.data, k_W.data\n",
    "            \n",
    "            \n",
    "        score = net(x)\n",
    "        loss = criterion(score, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        for i in range(len(all_W_kernels)):\n",
    "            k_W = all_W_kernels[i]\n",
    "            k_G = all_G_kernels[i]\n",
    "            k_W.data, k_G.data = k_G.data, k_W.data\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data\n",
    "        _, predicted = torch.max(score.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target.data).cpu().sum()\n",
    "        pbar.write(f\"Training {batch_idx} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f}% ({correct}/{total})\")\n",
    "        \n",
    "    #----------------------------------------------------------------------\n",
    "    # Testing\n",
    "    #----------------------------------------------------------------------\n",
    "    pbar.set_description(f\"Testing {epoch}\",refresh=True)\n",
    "    test_loss = 0; correct = 0; total = 0\n",
    "    net.eval()\n",
    "    \n",
    "    for i in range(len(all_W_kernels)):\n",
    "        k_W = all_W_kernels[i]\n",
    "        k_quant = all_G_kernels[i]    \n",
    "        k_W.data, k_quant.data = k_quant.data, k_W.data\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, target) in enumerate(tqdm(test_loader, leave=False)):\n",
    "        #for batch_idx, (x, target) in enumerate(test_loader):\n",
    "            x, target = Variable(x.cuda()), Variable(target.cuda())\n",
    "            score= net(x)\n",
    "            \n",
    "            loss = criterion(score, target)\n",
    "            test_loss += loss.data\n",
    "            _, predicted = torch.max(score.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target.data).cpu().sum()\n",
    "            pbar.write(f\"Testing {batch_idx} Loss: {train_loss/(batch_idx+1):.3f} | Acc: {100.*correct/total:.3f}% ({correct}/{total})\")\n",
    "\n",
    "    \n",
    "    #----------------------------------------------------------------------\n",
    "    # Save the checkpoint\n",
    "    #----------------------------------------------------------------------\n",
    "    '''\n",
    "    for i in range(len(all_W_kernels)):\n",
    "        k_W = all_W_kernels[i]\n",
    "        k_quant = all_G_kernels[i]    \n",
    "        k_W.data, k_quant.data = k_quant.data, k_W.data\n",
    "        '''  \n",
    "    acc = 100.*correct/total\n",
    "    #if acc > best_acc:\n",
    "    if correct > best_count:\n",
    "        # print('Saving model...')\n",
    "        # state = {\n",
    "        #     'state': net.state_dict(), #net,\n",
    "        #     'acc': acc,\n",
    "        #     'epoch': epoch,\n",
    "        # }\n",
    "        \n",
    "        # torch.save(state, f'./saved_models/resnet_{quantize_nbits}bits_{u.time_stamp()}.pth')\n",
    "        #net.save_state_dict('resnet20.pt')\n",
    "        best_acc = acc\n",
    "        best_count = correct\n",
    "\n",
    "    for i in range(len(all_W_kernels)):\n",
    "        k_W=all_W_kernels[i]\n",
    "        k_quant=all_W_kernels[i]\n",
    "        k_W.data, k_quant.data =k_quant.data,k_W.data\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "save_model(net, \"saved_models\", f'resnet_{quantize_nbits}bits_{u.time_stamp()}.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
