{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Quantization for RESNET"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train a floating point model or load a pre-trained floating point model.\n",
    "- Move the model to CPU and switch model to training mode.\n",
    "- Apply layer fusion.\n",
    "- Switch model to evaluation mode, check if the layer fusion results in correct model, and switch back to training mode.\n",
    "- Apply torch.quantization.QuantStub() and torch.quantization.QuantStub() to the inputs and outputs, respectively.\n",
    "- Specify quantization configurations, such as symmetric quantization or asymmetric quantization, etc.\n",
    "- Prepare quantization model for quantization aware training.\n",
    "- Move the model to CUDA and run quantization aware training using CUDA.\n",
    "- Move the model to CPU and convert the quantization aware trained floating point model to quantized integer model.\n",
    "- [Optional] Verify accuracies and inference performance gain.\n",
    "- Save the quantized integer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from fedlern.train_utils import *\n",
    "from fedlern.quant_utils import *\n",
    "from fedlern.models.resnet_v2 import *\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 10\n",
    "stats = (0.49139968, 0.48215841, 0.44653091), (0.24703223, 0.24348513, 0.26158784)\n",
    "\n",
    "cuda_device = torch.device(\"cuda:0\")\n",
    "cpu_device = torch.device(\"cpu:0\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"saved_models\"\n",
    "model_filename = 'resnet18v2_cifar10.pt'\n",
    "quantized_model_filename = \"resnet18_quantized_cifar10pytorch.pt\"\n",
    "model_filepath = os.path.join(model_dir, model_filename)\n",
    "quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = prepare_dataloader_cifar(num_workers=8, train_batch_size=128, eval_batch_size=256, stats=stats)\n",
    "\n",
    "\n",
    "# Prepare the model for static quantization. This inserts observers in\n",
    "# the model that will observe activation tensors during calibration.\n",
    "model_fp32 = QuantizedResNet(model_fp32=ResNet18())\n",
    "model_fp32.eval()\n",
    "\n",
    "model3 = ResNet18()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/.local/lib/python3.8/site-packages/torch/ao/quantization/quantize.py:310: UserWarning: None of the submodule got qconfig applied. Make sure you passed correct configuration through `qconfig_dict` or by assigning the `.qconfig` attribute directly on submodules\n",
      "  warnings.warn(\"None of the submodule got qconfig applied. Make sure you \"\n"
     ]
    }
   ],
   "source": [
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'x86' for server inference and 'qnnpack'\n",
    "# for mobile inference. Other quantization configurations such as selecting\n",
    "# symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques\n",
    "# can be specified here.\n",
    "# Note: the old 'fbgemm' is still available but 'x86' is the recommended default\n",
    "# for server inference.\n",
    "# model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')\n",
    "model_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n",
    "# The model has to be switched to training mode before any layer fusion.\n",
    "# Otherwise the quantization aware training will not work correctly.\n",
    "\n",
    "# Fuse the model in place rather manually.\n",
    "# Fuse the first Conv2d, BatchNorm2d, and ReLU layers\n",
    "model_fp32_fused = torch.quantization.fuse_modules(model_fp32.model_fp32, [[\"conv1\", \"bn1\", \"relu\"]])\n",
    "# Fuse the remaining Conv2d and BatchNorm2d layers in the ResNet blocks\n",
    "for name, module in model_fp32_fused.named_modules():\n",
    "    if isinstance(module, nn.Sequential):\n",
    "        for i in range(len(module)):\n",
    "            if isinstance(module[i], BasicBlock) or isinstance(module[i], Bottleneck):\n",
    "                module[i] = torch.quantization.fuse_modules(module[i], [[\"conv1\", \"bn1\", \"relu\"], [\"conv2\", \"bn2\"]], inplace = True)\n",
    "\n",
    "# Prepare the model for QAT. This inserts observers and fake_quants in\n",
    "# the model needs to be set to train for QAT logic to work\n",
    "# the model that will observe weight and activation tensors during calibration.\n",
    "model_fp32_prepared = torch.ao.quantization.prepare_qat(model_fp32_fused.train())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the training loop (not shown)\n",
    "train_model(model=model_fp32_prepared,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            num_epochs=20,\n",
    "            device=cpu_device)\n",
    "\n",
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, fuses modules where appropriate,\n",
    "# and replaces key operators with quantized implementations.\n",
    "model_fp32_prepared.eval()\n",
    "model_int8 = torch.ao.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, fp32_eval_accuracy \u001b[39m=\u001b[39m evaluate_model(model\u001b[39m=\u001b[39;49mmodel, test_loader\u001b[39m=\u001b[39;49mtest_loader, device\u001b[39m=\u001b[39;49mcpu_device, criterion\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m      2\u001b[0m _, int8_eval_accuracy \u001b[39m=\u001b[39m evaluate_model(model\u001b[39m=\u001b[39mquantized_model, test_loader\u001b[39m=\u001b[39mtest_loader, device\u001b[39m=\u001b[39mcpu_device, criterion\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mINT8 evaluation accuracy: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(int8_eval_accuracy))\n",
      "File \u001b[0;32m~/FedLearning/fedlern/train_utils.py:53\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, test_loader, device, criterion)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_model\u001b[39m(model, test_loader, device, criterion\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 53\u001b[0m     model\u001b[39m.\u001b[39;49meval()\n\u001b[1;32m     54\u001b[0m     model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     56\u001b[0m     running_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "_, int8_eval_accuracy = evaluate_model(model=model_int8, test_loader=test_loader, device=cpu_device, criterion=None)\n",
    "\n",
    "\n",
    "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
    "\n",
    "# int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "# int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "\n",
    "\n",
    "# print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
    "# print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
