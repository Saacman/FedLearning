{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization for MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training Model...\n",
      "Epoch: -1 Eval Loss: 2.325 Eval Acc: 0.098\n",
      "Epoch: 000 Train Loss: 2.090 Train Acc: 0.299 Eval Loss: 1.603 Eval Acc: 0.403\n",
      "Epoch: 001 Train Loss: 1.511 Train Acc: 0.447 Eval Loss: 1.374 Eval Acc: 0.501\n",
      "Epoch: 002 Train Loss: 1.329 Train Acc: 0.521 Eval Loss: 1.163 Eval Acc: 0.583\n",
      "Epoch: 003 Train Loss: 1.179 Train Acc: 0.579 Eval Loss: 1.142 Eval Acc: 0.596\n",
      "Epoch: 004 Train Loss: 1.071 Train Acc: 0.620 Eval Loss: 1.010 Eval Acc: 0.636\n",
      "Epoch: 005 Train Loss: 0.986 Train Acc: 0.651 Eval Loss: 0.920 Eval Acc: 0.675\n",
      "Epoch: 006 Train Loss: 0.915 Train Acc: 0.679 Eval Loss: 0.885 Eval Acc: 0.697\n",
      "Epoch: 007 Train Loss: 0.866 Train Acc: 0.695 Eval Loss: 0.789 Eval Acc: 0.726\n",
      "Epoch: 008 Train Loss: 0.806 Train Acc: 0.718 Eval Loss: 0.816 Eval Acc: 0.717\n",
      "Epoch: 009 Train Loss: 0.775 Train Acc: 0.729 Eval Loss: 0.795 Eval Acc: 0.729\n",
      "Epoch: 010 Train Loss: 0.741 Train Acc: 0.743 Eval Loss: 0.797 Eval Acc: 0.726\n",
      "Epoch: 011 Train Loss: 0.713 Train Acc: 0.754 Eval Loss: 0.754 Eval Acc: 0.739\n",
      "Epoch: 012 Train Loss: 0.693 Train Acc: 0.760 Eval Loss: 0.733 Eval Acc: 0.745\n",
      "Epoch: 013 Train Loss: 0.669 Train Acc: 0.767 Eval Loss: 0.686 Eval Acc: 0.767\n",
      "Epoch: 014 Train Loss: 0.653 Train Acc: 0.772 Eval Loss: 0.675 Eval Acc: 0.771\n",
      "Epoch: 015 Train Loss: 0.631 Train Acc: 0.780 Eval Loss: 0.631 Eval Acc: 0.786\n",
      "Epoch: 016 Train Loss: 0.617 Train Acc: 0.784 Eval Loss: 0.662 Eval Acc: 0.772\n",
      "Epoch: 017 Train Loss: 0.596 Train Acc: 0.794 Eval Loss: 0.698 Eval Acc: 0.768\n",
      "Epoch: 018 Train Loss: 0.589 Train Acc: 0.794 Eval Loss: 0.657 Eval Acc: 0.783\n",
      "Epoch: 019 Train Loss: 0.580 Train Acc: 0.798 Eval Loss: 0.661 Eval Acc: 0.779\n",
      "Epoch: 020 Train Loss: 0.570 Train Acc: 0.801 Eval Loss: 0.664 Eval Acc: 0.778\n",
      "Epoch: 021 Train Loss: 0.554 Train Acc: 0.807 Eval Loss: 0.629 Eval Acc: 0.785\n",
      "Epoch: 022 Train Loss: 0.545 Train Acc: 0.811 Eval Loss: 0.628 Eval Acc: 0.792\n",
      "Epoch: 023 Train Loss: 0.533 Train Acc: 0.814 Eval Loss: 0.637 Eval Acc: 0.782\n",
      "Epoch: 024 Train Loss: 0.530 Train Acc: 0.815 Eval Loss: 0.597 Eval Acc: 0.802\n",
      "Epoch: 025 Train Loss: 0.523 Train Acc: 0.819 Eval Loss: 0.606 Eval Acc: 0.799\n",
      "Epoch: 026 Train Loss: 0.517 Train Acc: 0.820 Eval Loss: 0.590 Eval Acc: 0.799\n",
      "Epoch: 027 Train Loss: 0.505 Train Acc: 0.826 Eval Loss: 0.604 Eval Acc: 0.793\n",
      "Epoch: 028 Train Loss: 0.497 Train Acc: 0.827 Eval Loss: 0.640 Eval Acc: 0.784\n",
      "Epoch: 029 Train Loss: 0.493 Train Acc: 0.829 Eval Loss: 0.583 Eval Acc: 0.802\n",
      "Epoch: 030 Train Loss: 0.483 Train Acc: 0.832 Eval Loss: 0.627 Eval Acc: 0.791\n",
      "Epoch: 031 Train Loss: 0.479 Train Acc: 0.834 Eval Loss: 0.589 Eval Acc: 0.801\n",
      "Epoch: 032 Train Loss: 0.475 Train Acc: 0.834 Eval Loss: 0.567 Eval Acc: 0.811\n",
      "Epoch: 033 Train Loss: 0.469 Train Acc: 0.836 Eval Loss: 0.589 Eval Acc: 0.807\n",
      "Epoch: 034 Train Loss: 0.460 Train Acc: 0.839 Eval Loss: 0.582 Eval Acc: 0.811\n",
      "Epoch: 035 Train Loss: 0.455 Train Acc: 0.842 Eval Loss: 0.588 Eval Acc: 0.811\n",
      "Epoch: 036 Train Loss: 0.451 Train Acc: 0.844 Eval Loss: 0.578 Eval Acc: 0.813\n",
      "Epoch: 037 Train Loss: 0.445 Train Acc: 0.844 Eval Loss: 0.567 Eval Acc: 0.812\n",
      "Epoch: 038 Train Loss: 0.439 Train Acc: 0.845 Eval Loss: 0.577 Eval Acc: 0.809\n",
      "Epoch: 039 Train Loss: 0.439 Train Acc: 0.846 Eval Loss: 0.598 Eval Acc: 0.808\n",
      "Epoch: 040 Train Loss: 0.436 Train Acc: 0.848 Eval Loss: 0.591 Eval Acc: 0.809\n",
      "Epoch: 041 Train Loss: 0.436 Train Acc: 0.850 Eval Loss: 0.560 Eval Acc: 0.811\n",
      "Epoch: 042 Train Loss: 0.425 Train Acc: 0.852 Eval Loss: 0.565 Eval Acc: 0.814\n",
      "Epoch: 043 Train Loss: 0.426 Train Acc: 0.852 Eval Loss: 0.548 Eval Acc: 0.815\n",
      "Epoch: 044 Train Loss: 0.423 Train Acc: 0.852 Eval Loss: 0.558 Eval Acc: 0.817\n",
      "Epoch: 045 Train Loss: 0.418 Train Acc: 0.852 Eval Loss: 0.600 Eval Acc: 0.805\n",
      "Epoch: 046 Train Loss: 0.417 Train Acc: 0.854 Eval Loss: 0.596 Eval Acc: 0.804\n",
      "Epoch: 047 Train Loss: 0.413 Train Acc: 0.857 Eval Loss: 0.636 Eval Acc: 0.792\n",
      "Epoch: 048 Train Loss: 0.405 Train Acc: 0.858 Eval Loss: 0.568 Eval Acc: 0.815\n",
      "Epoch: 049 Train Loss: 0.405 Train Acc: 0.859 Eval Loss: 0.529 Eval Acc: 0.824\n",
      "Epoch: 050 Train Loss: 0.402 Train Acc: 0.860 Eval Loss: 0.574 Eval Acc: 0.811\n",
      "Epoch: 051 Train Loss: 0.400 Train Acc: 0.859 Eval Loss: 0.551 Eval Acc: 0.820\n",
      "Epoch: 052 Train Loss: 0.398 Train Acc: 0.862 Eval Loss: 0.599 Eval Acc: 0.808\n",
      "Epoch: 053 Train Loss: 0.391 Train Acc: 0.863 Eval Loss: 0.529 Eval Acc: 0.825\n",
      "Epoch: 054 Train Loss: 0.391 Train Acc: 0.863 Eval Loss: 0.575 Eval Acc: 0.811\n",
      "Epoch: 055 Train Loss: 0.393 Train Acc: 0.863 Eval Loss: 0.577 Eval Acc: 0.812\n",
      "Epoch: 056 Train Loss: 0.384 Train Acc: 0.865 Eval Loss: 0.600 Eval Acc: 0.805\n",
      "Epoch: 057 Train Loss: 0.387 Train Acc: 0.865 Eval Loss: 0.542 Eval Acc: 0.821\n",
      "Epoch: 058 Train Loss: 0.383 Train Acc: 0.865 Eval Loss: 0.531 Eval Acc: 0.829\n",
      "Epoch: 059 Train Loss: 0.384 Train Acc: 0.865 Eval Loss: 0.586 Eval Acc: 0.808\n",
      "Epoch: 060 Train Loss: 0.382 Train Acc: 0.866 Eval Loss: 0.534 Eval Acc: 0.830\n",
      "Epoch: 061 Train Loss: 0.382 Train Acc: 0.866 Eval Loss: 0.542 Eval Acc: 0.824\n",
      "Epoch: 062 Train Loss: 0.375 Train Acc: 0.868 Eval Loss: 0.529 Eval Acc: 0.825\n",
      "Epoch: 063 Train Loss: 0.370 Train Acc: 0.870 Eval Loss: 0.550 Eval Acc: 0.822\n",
      "Epoch: 064 Train Loss: 0.371 Train Acc: 0.868 Eval Loss: 0.598 Eval Acc: 0.810\n",
      "Epoch: 065 Train Loss: 0.376 Train Acc: 0.868 Eval Loss: 0.594 Eval Acc: 0.811\n",
      "Epoch: 066 Train Loss: 0.373 Train Acc: 0.870 Eval Loss: 0.592 Eval Acc: 0.811\n",
      "Epoch: 067 Train Loss: 0.370 Train Acc: 0.871 Eval Loss: 0.538 Eval Acc: 0.826\n",
      "Epoch: 068 Train Loss: 0.364 Train Acc: 0.872 Eval Loss: 0.545 Eval Acc: 0.825\n",
      "Epoch: 069 Train Loss: 0.360 Train Acc: 0.874 Eval Loss: 0.556 Eval Acc: 0.820\n",
      "Epoch: 070 Train Loss: 0.361 Train Acc: 0.874 Eval Loss: 0.561 Eval Acc: 0.827\n",
      "Epoch: 071 Train Loss: 0.359 Train Acc: 0.875 Eval Loss: 0.564 Eval Acc: 0.820\n",
      "Epoch: 072 Train Loss: 0.359 Train Acc: 0.874 Eval Loss: 0.533 Eval Acc: 0.828\n",
      "Epoch: 073 Train Loss: 0.359 Train Acc: 0.874 Eval Loss: 0.568 Eval Acc: 0.819\n",
      "Epoch: 074 Train Loss: 0.355 Train Acc: 0.876 Eval Loss: 0.567 Eval Acc: 0.821\n",
      "Epoch: 075 Train Loss: 0.358 Train Acc: 0.876 Eval Loss: 0.526 Eval Acc: 0.830\n",
      "Epoch: 076 Train Loss: 0.355 Train Acc: 0.876 Eval Loss: 0.544 Eval Acc: 0.827\n",
      "Epoch: 077 Train Loss: 0.355 Train Acc: 0.875 Eval Loss: 0.526 Eval Acc: 0.831\n",
      "Epoch: 078 Train Loss: 0.355 Train Acc: 0.875 Eval Loss: 0.544 Eval Acc: 0.829\n",
      "Epoch: 079 Train Loss: 0.353 Train Acc: 0.876 Eval Loss: 0.603 Eval Acc: 0.808\n",
      "Epoch: 080 Train Loss: 0.351 Train Acc: 0.876 Eval Loss: 0.580 Eval Acc: 0.816\n",
      "Epoch: 081 Train Loss: 0.342 Train Acc: 0.879 Eval Loss: 0.541 Eval Acc: 0.826\n",
      "Epoch: 082 Train Loss: 0.343 Train Acc: 0.879 Eval Loss: 0.509 Eval Acc: 0.835\n",
      "Epoch: 083 Train Loss: 0.344 Train Acc: 0.879 Eval Loss: 0.541 Eval Acc: 0.826\n",
      "Epoch: 084 Train Loss: 0.347 Train Acc: 0.879 Eval Loss: 0.525 Eval Acc: 0.833\n",
      "Epoch: 085 Train Loss: 0.346 Train Acc: 0.878 Eval Loss: 0.543 Eval Acc: 0.824\n",
      "Epoch: 086 Train Loss: 0.345 Train Acc: 0.878 Eval Loss: 0.558 Eval Acc: 0.823\n",
      "Epoch: 087 Train Loss: 0.339 Train Acc: 0.882 Eval Loss: 0.526 Eval Acc: 0.831\n",
      "Epoch: 088 Train Loss: 0.343 Train Acc: 0.880 Eval Loss: 0.575 Eval Acc: 0.817\n",
      "Epoch: 089 Train Loss: 0.337 Train Acc: 0.883 Eval Loss: 0.520 Eval Acc: 0.833\n",
      "Epoch: 090 Train Loss: 0.337 Train Acc: 0.882 Eval Loss: 0.527 Eval Acc: 0.839\n",
      "Epoch: 091 Train Loss: 0.336 Train Acc: 0.882 Eval Loss: 0.560 Eval Acc: 0.825\n",
      "Epoch: 092 Train Loss: 0.334 Train Acc: 0.883 Eval Loss: 0.657 Eval Acc: 0.797\n",
      "Epoch: 093 Train Loss: 0.339 Train Acc: 0.882 Eval Loss: 0.571 Eval Acc: 0.819\n",
      "Epoch: 094 Train Loss: 0.333 Train Acc: 0.883 Eval Loss: 0.519 Eval Acc: 0.832\n",
      "Epoch: 095 Train Loss: 0.333 Train Acc: 0.883 Eval Loss: 0.532 Eval Acc: 0.828\n",
      "Epoch: 096 Train Loss: 0.330 Train Acc: 0.885 Eval Loss: 0.544 Eval Acc: 0.822\n",
      "Epoch: 097 Train Loss: 0.337 Train Acc: 0.882 Eval Loss: 0.592 Eval Acc: 0.815\n",
      "Epoch: 098 Train Loss: 0.332 Train Acc: 0.884 Eval Loss: 0.514 Eval Acc: 0.837\n",
      "Epoch: 099 Train Loss: 0.335 Train Acc: 0.883 Eval Loss: 0.533 Eval Acc: 0.831\n",
      "Epoch: 100 Train Loss: 0.221 Train Acc: 0.922 Eval Loss: 0.439 Eval Acc: 0.863\n",
      "Epoch: 101 Train Loss: 0.177 Train Acc: 0.938 Eval Loss: 0.442 Eval Acc: 0.865\n",
      "Epoch: 102 Train Loss: 0.163 Train Acc: 0.943 Eval Loss: 0.445 Eval Acc: 0.866\n",
      "Epoch: 103 Train Loss: 0.151 Train Acc: 0.947 Eval Loss: 0.439 Eval Acc: 0.873\n",
      "Epoch: 104 Train Loss: 0.140 Train Acc: 0.951 Eval Loss: 0.447 Eval Acc: 0.872\n",
      "Epoch: 105 Train Loss: 0.133 Train Acc: 0.953 Eval Loss: 0.452 Eval Acc: 0.872\n",
      "Epoch: 106 Train Loss: 0.129 Train Acc: 0.955 Eval Loss: 0.452 Eval Acc: 0.871\n",
      "Epoch: 107 Train Loss: 0.126 Train Acc: 0.956 Eval Loss: 0.454 Eval Acc: 0.873\n",
      "Epoch: 108 Train Loss: 0.119 Train Acc: 0.959 Eval Loss: 0.464 Eval Acc: 0.872\n",
      "Epoch: 109 Train Loss: 0.113 Train Acc: 0.960 Eval Loss: 0.462 Eval Acc: 0.872\n",
      "Epoch: 110 Train Loss: 0.110 Train Acc: 0.961 Eval Loss: 0.465 Eval Acc: 0.873\n",
      "Epoch: 111 Train Loss: 0.109 Train Acc: 0.962 Eval Loss: 0.467 Eval Acc: 0.874\n",
      "Epoch: 112 Train Loss: 0.103 Train Acc: 0.963 Eval Loss: 0.474 Eval Acc: 0.873\n",
      "Epoch: 113 Train Loss: 0.102 Train Acc: 0.964 Eval Loss: 0.477 Eval Acc: 0.873\n",
      "Epoch: 114 Train Loss: 0.098 Train Acc: 0.965 Eval Loss: 0.483 Eval Acc: 0.875\n",
      "Epoch: 115 Train Loss: 0.092 Train Acc: 0.967 Eval Loss: 0.481 Eval Acc: 0.873\n",
      "Epoch: 116 Train Loss: 0.092 Train Acc: 0.968 Eval Loss: 0.491 Eval Acc: 0.873\n",
      "Epoch: 117 Train Loss: 0.091 Train Acc: 0.967 Eval Loss: 0.501 Eval Acc: 0.869\n",
      "Epoch: 118 Train Loss: 0.087 Train Acc: 0.969 Eval Loss: 0.512 Eval Acc: 0.868\n",
      "Epoch: 119 Train Loss: 0.088 Train Acc: 0.969 Eval Loss: 0.506 Eval Acc: 0.872\n",
      "Epoch: 120 Train Loss: 0.080 Train Acc: 0.972 Eval Loss: 0.512 Eval Acc: 0.873\n",
      "Epoch: 121 Train Loss: 0.081 Train Acc: 0.972 Eval Loss: 0.518 Eval Acc: 0.873\n",
      "Epoch: 122 Train Loss: 0.079 Train Acc: 0.973 Eval Loss: 0.525 Eval Acc: 0.871\n",
      "Epoch: 123 Train Loss: 0.078 Train Acc: 0.973 Eval Loss: 0.520 Eval Acc: 0.872\n",
      "Epoch: 124 Train Loss: 0.074 Train Acc: 0.974 Eval Loss: 0.526 Eval Acc: 0.871\n",
      "Epoch: 125 Train Loss: 0.073 Train Acc: 0.974 Eval Loss: 0.526 Eval Acc: 0.871\n",
      "Epoch: 126 Train Loss: 0.069 Train Acc: 0.975 Eval Loss: 0.535 Eval Acc: 0.870\n",
      "Epoch: 127 Train Loss: 0.071 Train Acc: 0.975 Eval Loss: 0.534 Eval Acc: 0.871\n",
      "Epoch: 128 Train Loss: 0.072 Train Acc: 0.974 Eval Loss: 0.537 Eval Acc: 0.869\n",
      "Epoch: 129 Train Loss: 0.069 Train Acc: 0.976 Eval Loss: 0.544 Eval Acc: 0.873\n",
      "Epoch: 130 Train Loss: 0.068 Train Acc: 0.975 Eval Loss: 0.552 Eval Acc: 0.868\n",
      "Epoch: 131 Train Loss: 0.069 Train Acc: 0.976 Eval Loss: 0.555 Eval Acc: 0.869\n",
      "Epoch: 132 Train Loss: 0.065 Train Acc: 0.977 Eval Loss: 0.549 Eval Acc: 0.871\n",
      "Epoch: 133 Train Loss: 0.063 Train Acc: 0.978 Eval Loss: 0.553 Eval Acc: 0.869\n",
      "Epoch: 134 Train Loss: 0.062 Train Acc: 0.979 Eval Loss: 0.554 Eval Acc: 0.874\n",
      "Epoch: 135 Train Loss: 0.059 Train Acc: 0.979 Eval Loss: 0.559 Eval Acc: 0.872\n",
      "Epoch: 136 Train Loss: 0.063 Train Acc: 0.977 Eval Loss: 0.567 Eval Acc: 0.869\n",
      "Epoch: 137 Train Loss: 0.059 Train Acc: 0.979 Eval Loss: 0.561 Eval Acc: 0.873\n",
      "Epoch: 138 Train Loss: 0.059 Train Acc: 0.980 Eval Loss: 0.566 Eval Acc: 0.871\n",
      "Epoch: 139 Train Loss: 0.060 Train Acc: 0.979 Eval Loss: 0.568 Eval Acc: 0.874\n",
      "Epoch: 140 Train Loss: 0.055 Train Acc: 0.981 Eval Loss: 0.580 Eval Acc: 0.868\n",
      "Epoch: 141 Train Loss: 0.053 Train Acc: 0.981 Eval Loss: 0.578 Eval Acc: 0.872\n",
      "Epoch: 142 Train Loss: 0.056 Train Acc: 0.980 Eval Loss: 0.579 Eval Acc: 0.868\n",
      "Epoch: 143 Train Loss: 0.053 Train Acc: 0.981 Eval Loss: 0.592 Eval Acc: 0.870\n",
      "Epoch: 144 Train Loss: 0.054 Train Acc: 0.980 Eval Loss: 0.588 Eval Acc: 0.871\n",
      "Epoch: 145 Train Loss: 0.053 Train Acc: 0.982 Eval Loss: 0.592 Eval Acc: 0.869\n",
      "Epoch: 146 Train Loss: 0.053 Train Acc: 0.982 Eval Loss: 0.589 Eval Acc: 0.870\n",
      "Epoch: 147 Train Loss: 0.050 Train Acc: 0.983 Eval Loss: 0.598 Eval Acc: 0.869\n",
      "Epoch: 148 Train Loss: 0.053 Train Acc: 0.982 Eval Loss: 0.606 Eval Acc: 0.868\n",
      "Epoch: 149 Train Loss: 0.049 Train Acc: 0.983 Eval Loss: 0.610 Eval Acc: 0.867\n",
      "Epoch: 150 Train Loss: 0.043 Train Acc: 0.986 Eval Loss: 0.587 Eval Acc: 0.872\n",
      "Epoch: 151 Train Loss: 0.040 Train Acc: 0.986 Eval Loss: 0.584 Eval Acc: 0.874\n",
      "Epoch: 152 Train Loss: 0.036 Train Acc: 0.988 Eval Loss: 0.586 Eval Acc: 0.873\n",
      "Epoch: 153 Train Loss: 0.037 Train Acc: 0.987 Eval Loss: 0.585 Eval Acc: 0.874\n",
      "Epoch: 154 Train Loss: 0.033 Train Acc: 0.989 Eval Loss: 0.591 Eval Acc: 0.872\n",
      "Epoch: 155 Train Loss: 0.033 Train Acc: 0.989 Eval Loss: 0.585 Eval Acc: 0.872\n",
      "Epoch: 156 Train Loss: 0.033 Train Acc: 0.989 Eval Loss: 0.590 Eval Acc: 0.872\n",
      "Epoch: 157 Train Loss: 0.032 Train Acc: 0.989 Eval Loss: 0.588 Eval Acc: 0.873\n",
      "Epoch: 158 Train Loss: 0.032 Train Acc: 0.989 Eval Loss: 0.588 Eval Acc: 0.873\n",
      "Epoch: 159 Train Loss: 0.031 Train Acc: 0.990 Eval Loss: 0.588 Eval Acc: 0.873\n",
      "Epoch: 160 Train Loss: 0.030 Train Acc: 0.990 Eval Loss: 0.594 Eval Acc: 0.873\n",
      "Epoch: 161 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.596 Eval Acc: 0.873\n",
      "Epoch: 162 Train Loss: 0.030 Train Acc: 0.990 Eval Loss: 0.595 Eval Acc: 0.873\n",
      "Epoch: 163 Train Loss: 0.029 Train Acc: 0.991 Eval Loss: 0.598 Eval Acc: 0.873\n",
      "Epoch: 164 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.602 Eval Acc: 0.873\n",
      "Epoch: 165 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.600 Eval Acc: 0.873\n",
      "Epoch: 166 Train Loss: 0.028 Train Acc: 0.991 Eval Loss: 0.599 Eval Acc: 0.873\n",
      "Epoch: 167 Train Loss: 0.029 Train Acc: 0.991 Eval Loss: 0.602 Eval Acc: 0.872\n",
      "Epoch: 168 Train Loss: 0.028 Train Acc: 0.991 Eval Loss: 0.600 Eval Acc: 0.873\n",
      "Epoch: 169 Train Loss: 0.027 Train Acc: 0.991 Eval Loss: 0.604 Eval Acc: 0.876\n",
      "Epoch: 170 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.606 Eval Acc: 0.874\n",
      "Epoch: 171 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.605 Eval Acc: 0.874\n",
      "Epoch: 172 Train Loss: 0.026 Train Acc: 0.991 Eval Loss: 0.603 Eval Acc: 0.875\n",
      "Epoch: 173 Train Loss: 0.026 Train Acc: 0.991 Eval Loss: 0.610 Eval Acc: 0.874\n",
      "Epoch: 174 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.609 Eval Acc: 0.874\n",
      "Epoch: 175 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.613 Eval Acc: 0.874\n",
      "Epoch: 176 Train Loss: 0.026 Train Acc: 0.991 Eval Loss: 0.615 Eval Acc: 0.874\n",
      "Epoch: 177 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.619 Eval Acc: 0.873\n",
      "Epoch: 178 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.613 Eval Acc: 0.874\n",
      "Epoch: 179 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.612 Eval Acc: 0.874\n",
      "Epoch: 180 Train Loss: 0.026 Train Acc: 0.992 Eval Loss: 0.617 Eval Acc: 0.873\n",
      "Epoch: 181 Train Loss: 0.024 Train Acc: 0.992 Eval Loss: 0.623 Eval Acc: 0.874\n",
      "Epoch: 182 Train Loss: 0.023 Train Acc: 0.992 Eval Loss: 0.621 Eval Acc: 0.874\n",
      "Epoch: 183 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.623 Eval Acc: 0.874\n",
      "Epoch: 184 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.626 Eval Acc: 0.873\n",
      "Epoch: 185 Train Loss: 0.023 Train Acc: 0.993 Eval Loss: 0.625 Eval Acc: 0.875\n",
      "Epoch: 186 Train Loss: 0.023 Train Acc: 0.992 Eval Loss: 0.625 Eval Acc: 0.875\n",
      "Epoch: 187 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.631 Eval Acc: 0.873\n",
      "Epoch: 188 Train Loss: 0.021 Train Acc: 0.993 Eval Loss: 0.633 Eval Acc: 0.873\n",
      "Epoch: 189 Train Loss: 0.021 Train Acc: 0.993 Eval Loss: 0.631 Eval Acc: 0.873\n",
      "Epoch: 190 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.635 Eval Acc: 0.874\n",
      "Epoch: 191 Train Loss: 0.021 Train Acc: 0.993 Eval Loss: 0.636 Eval Acc: 0.873\n",
      "Epoch: 192 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.639 Eval Acc: 0.874\n",
      "Epoch: 193 Train Loss: 0.023 Train Acc: 0.993 Eval Loss: 0.636 Eval Acc: 0.874\n",
      "Epoch: 194 Train Loss: 0.023 Train Acc: 0.993 Eval Loss: 0.640 Eval Acc: 0.874\n",
      "Epoch: 195 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.645 Eval Acc: 0.873\n",
      "Epoch: 196 Train Loss: 0.021 Train Acc: 0.993 Eval Loss: 0.645 Eval Acc: 0.875\n",
      "Epoch: 197 Train Loss: 0.020 Train Acc: 0.993 Eval Loss: 0.644 Eval Acc: 0.874\n",
      "Epoch: 198 Train Loss: 0.020 Train Acc: 0.993 Eval Loss: 0.649 Eval Acc: 0.875\n",
      "Epoch: 199 Train Loss: 0.022 Train Acc: 0.993 Eval Loss: 0.648 Eval Acc: 0.874\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Fusion only for eval!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 389\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mINT8 JIT CPU Inference Latency: \u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m ms / sample\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(int8_jit_cpu_inference_latency \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m))\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 389\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[1], line 305\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    302\u001b[0m fused_model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    304\u001b[0m \u001b[39m# Fuse the model in place rather manually.\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m fused_model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mquantization\u001b[39m.\u001b[39;49mfuse_modules(fused_model, [[\u001b[39m\"\u001b[39;49m\u001b[39mconv1\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mbn1\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m\"\u001b[39;49m]], inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    306\u001b[0m \u001b[39mfor\u001b[39;00m module_name, module \u001b[39min\u001b[39;00m fused_model\u001b[39m.\u001b[39mnamed_children():\n\u001b[1;32m    307\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlayer\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m module_name:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/ao/quantization/fuse_modules.py:158\u001b[0m, in \u001b[0;36mfuse_modules\u001b[0;34m(model, modules_to_fuse, inplace, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfuse_modules\u001b[39m(model, modules_to_fuse, inplace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, fuser_func\u001b[39m=\u001b[39mfuse_known_modules, fuse_custom_config_dict\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    104\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Fuses a list of modules into a single module\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39m    Fuses only the following sequence of modules:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \n\u001b[1;32m    157\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m _fuse_modules(\n\u001b[1;32m    159\u001b[0m         model,\n\u001b[1;32m    160\u001b[0m         modules_to_fuse,\n\u001b[1;32m    161\u001b[0m         is_qat\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    162\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m    163\u001b[0m         fuser_func\u001b[39m=\u001b[39;49mfuser_func,\n\u001b[1;32m    164\u001b[0m         fuse_custom_config_dict\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/ao/quantization/fuse_modules.py:100\u001b[0m, in \u001b[0;36m_fuse_modules\u001b[0;34m(model, modules_to_fuse, is_qat, inplace, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m     \u001b[39m# Handle case of modules_to_fuse being a list of lists\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[39mfor\u001b[39;00m module_list \u001b[39min\u001b[39;00m modules_to_fuse:\n\u001b[0;32m--> 100\u001b[0m         _fuse_modules_helper(model, module_list, is_qat, fuser_func, fuse_custom_config_dict)\n\u001b[1;32m    101\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/ao/quantization/fuse_modules.py:84\u001b[0m, in \u001b[0;36m_fuse_modules_helper\u001b[0;34m(model, modules_to_fuse, is_qat, fuser_func, fuse_custom_config_dict)\u001b[0m\n\u001b[1;32m     81\u001b[0m     mod_list\u001b[39m.\u001b[39mappend(_get_module(model, item))\n\u001b[1;32m     83\u001b[0m \u001b[39m# Fuse list of modules\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m new_mod_list \u001b[39m=\u001b[39m fuser_func(mod_list, is_qat, additional_fuser_method_mapping)\n\u001b[1;32m     86\u001b[0m \u001b[39m# Replace original module list with fused module list\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m i, item \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(modules_to_fuse):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/ao/quantization/fuse_modules.py:56\u001b[0m, in \u001b[0;36mfuse_known_modules\u001b[0;34m(mod_list, is_qat, additional_fuser_method_mapping)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot fuse modules: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(types))\n\u001b[1;32m     55\u001b[0m new_mod : List[Optional[nn\u001b[39m.\u001b[39mModule]] \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(mod_list)\n\u001b[0;32m---> 56\u001b[0m fused \u001b[39m=\u001b[39m fuser_method(is_qat, \u001b[39m*\u001b[39;49mmod_list)\n\u001b[1;32m     57\u001b[0m \u001b[39m# NOTE: forward hooks not processed in the two following for loops will be lost after the fusion\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39m# Move pre forward hooks of the base module to resulting fused module\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m handle_id, pre_hook_fn \u001b[39min\u001b[39;00m mod_list[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39m_forward_pre_hooks\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/ao/quantization/fuser_method_mappings.py:96\u001b[0m, in \u001b[0;36mfuse_conv_bn_relu\u001b[0;34m(is_qat, conv, bn, relu)\u001b[0m\n\u001b[1;32m     94\u001b[0m fused_module \u001b[39m=\u001b[39m map_to_fused_module_eval\u001b[39m.\u001b[39mget(\u001b[39mtype\u001b[39m(conv), \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     95\u001b[0m \u001b[39mif\u001b[39;00m fused_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     fused_conv \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mfusion\u001b[39m.\u001b[39;49mfuse_conv_bn_eval(conv, bn)\n\u001b[1;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m fused_module(fused_conv, relu)\n\u001b[1;32m     98\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/utils/fusion.py:7\u001b[0m, in \u001b[0;36mfuse_conv_bn_eval\u001b[0;34m(conv, bn, transpose)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfuse_conv_bn_eval\u001b[39m(conv, bn, transpose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[39massert\u001b[39;00m(\u001b[39mnot\u001b[39;00m (conv\u001b[39m.\u001b[39mtraining \u001b[39mor\u001b[39;00m bn\u001b[39m.\u001b[39mtraining)), \u001b[39m\"\u001b[39m\u001b[39mFusion only for eval!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m     fused_conv \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(conv)\n\u001b[1;32m     10\u001b[0m     fused_conv\u001b[39m.\u001b[39mweight, fused_conv\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m \\\n\u001b[1;32m     11\u001b[0m         fuse_conv_bn_weights(fused_conv\u001b[39m.\u001b[39mweight, fused_conv\u001b[39m.\u001b[39mbias,\n\u001b[1;32m     12\u001b[0m                              bn\u001b[39m.\u001b[39mrunning_mean, bn\u001b[39m.\u001b[39mrunning_var, bn\u001b[39m.\u001b[39meps, bn\u001b[39m.\u001b[39mweight, bn\u001b[39m.\u001b[39mbias, transpose)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Fusion only for eval!"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from src.resnet18 import resnet18\n",
    "\n",
    "def set_random_seeds(random_seed=0):\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "def prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256):\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=train_transform) \n",
    "    # We will use test set for validation and test in this project.\n",
    "    # Do not use test set for validation in practice!\n",
    "    test_set = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set, batch_size=train_batch_size,\n",
    "        sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def evaluate_model(model, test_loader, device, criterion=None):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        if criterion is not None:\n",
    "            loss = criterion(outputs, labels).item()\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    eval_loss = running_loss / len(test_loader.dataset)\n",
    "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
    "\n",
    "    return eval_loss, eval_accuracy\n",
    "\n",
    "def train_model(model, train_loader, test_loader, device, learning_rate=1e-1, num_epochs=200):\n",
    "\n",
    "    # The training configurations were not carefully selected.\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=500)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1, last_epoch=-1)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
    "    print(\"Epoch: {:02d} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(-1, eval_loss, eval_accuracy))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
    "\n",
    "        # Set learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        print(\"Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
    "\n",
    "    return model\n",
    "\n",
    "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        _ = model(inputs)\n",
    "\n",
    "def measure_inference_latency(model,\n",
    "                              device,\n",
    "                              input_size=(1, 3, 32, 32),\n",
    "                              num_samples=100,\n",
    "                              num_warmups=10):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.rand(size=input_size).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmups):\n",
    "            _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_samples):\n",
    "            _ = model(x)\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_time_ave = elapsed_time / num_samples\n",
    "\n",
    "    return elapsed_time_ave\n",
    "\n",
    "def save_model(model, model_dir, model_filename):\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.save(model.state_dict(), model_filepath)\n",
    "\n",
    "def load_model(model, model_filepath, device):\n",
    "\n",
    "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
    "\n",
    "    return model\n",
    "\n",
    "def save_torchscript_model(model, model_dir, model_filename):\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
    "\n",
    "def load_torchscript_model(model_filepath, device):\n",
    "\n",
    "    model = torch.jit.load(model_filepath, map_location=device)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model(num_classes=10):\n",
    "\n",
    "    # The number of channels in ResNet18 is divisible by 8.\n",
    "    # This is required for fast GEMM integer matrix multiplication.\n",
    "    # model = torchvision.models.resnet18(pretrained=False)\n",
    "    model = resnet18(num_classes=num_classes, pretrained=False)\n",
    "\n",
    "    # We would use the pretrained ResNet18 as a feature extractor.\n",
    "    # for param in model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # Modify the last FC layer\n",
    "    # num_features = model.fc.in_features\n",
    "    # model.fc = nn.Linear(num_features, 10)\n",
    "\n",
    "    return model\n",
    "\n",
    "class QuantizedResNet18(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(QuantizedResNet18, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized.\n",
    "        # This will only be used for inputs.\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        # DeQuantStub converts tensors from quantized to floating point.\n",
    "        # This will only be used for outputs.\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        # FP32 model\n",
    "        self.model_fp32 = model_fp32\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
    "\n",
    "    model_1.to(device)\n",
    "    model_2.to(device)\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model_1(x).detach().cpu().numpy()\n",
    "        y2 = model_2(x).detach().cpu().numpy()\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
    "            print(\"Model equivalence test sample failed: \")\n",
    "            print(y1)\n",
    "            print(y2)\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "\n",
    "    random_seed = 0\n",
    "    num_classes = 10\n",
    "    cuda_device = torch.device(\"cuda:0\")\n",
    "    cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "    model_dir = \"saved_models\"\n",
    "    model_filename = \"resnet18_cifar10.pt\"\n",
    "    quantized_model_filename = \"resnet18_quantized_cifar10.pt\"\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
    "\n",
    "    set_random_seeds(random_seed=random_seed)\n",
    "\n",
    "    # Create an untrained model.\n",
    "    model = create_model(num_classes=num_classes)\n",
    "\n",
    "    train_loader, test_loader = prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)\n",
    "    \n",
    "    # Train model.\n",
    "    print(\"Training Model...\")\n",
    "    model = train_model(model=model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-1, num_epochs=200)\n",
    "    # Save model.\n",
    "    save_model(model=model, model_dir=model_dir, model_filename=model_filename)\n",
    "    # Load a pretrained model.\n",
    "    model = load_model(model=model, model_filepath=model_filepath, device=cuda_device)\n",
    "    # Move the model to CPU since static quantization does not support CUDA currently.\n",
    "    model.to(cpu_device)\n",
    "    # Make a copy of the model for layer fusion\n",
    "    fused_model = copy.deepcopy(model)\n",
    "\n",
    "    model.train()\n",
    "    # The model has to be switched to training mode before any layer fusion.\n",
    "    # Otherwise the quantization aware training will not work correctly.\n",
    "    fused_model.train()\n",
    "\n",
    "    # Fuse the model in place rather manually.\n",
    "    fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
    "    for module_name, module in fused_model.named_children():\n",
    "        if \"layer\" in module_name:\n",
    "            for basic_block_name, basic_block in module.named_children():\n",
    "                torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
    "                for sub_block_name, sub_block in basic_block.named_children():\n",
    "                    if sub_block_name == \"downsample\":\n",
    "                        torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
    "\n",
    "    # Print FP32 model.\n",
    "    print(model)\n",
    "    # Print fused model.\n",
    "    print(fused_model)\n",
    "\n",
    "    # Model and fused model should be equivalent.\n",
    "    model.eval()\n",
    "    fused_model.eval()\n",
    "    assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "    # Prepare the model for quantization aware training. This inserts observers in\n",
    "    # the model that will observe activation tensors during calibration.\n",
    "    quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
    "    # Using un-fused model will fail.\n",
    "    # Because there is no quantized layer implementation for a single batch normalization layer.\n",
    "    # quantized_model = QuantizedResNet18(model_fp32=model)\n",
    "    # Select quantization schemes from \n",
    "    # https://pytorch.org/docs/stable/quantization-support.html\n",
    "    quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "    # Custom quantization configurations\n",
    "    # quantization_config = torch.quantization.default_qconfig\n",
    "    # quantization_config = torch.quantization.QConfig(activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.quint8), weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
    "\n",
    "    quantized_model.qconfig = quantization_config\n",
    "    \n",
    "    # Print quantization configurations\n",
    "    print(quantized_model.qconfig)\n",
    "\n",
    "    # https://pytorch.org/docs/stable/_modules/torch/quantization/quantize.html#prepare_qat\n",
    "    torch.quantization.prepare_qat(quantized_model, inplace=True)\n",
    "\n",
    "    # # Use training data for calibration.\n",
    "    print(\"Training QAT Model...\")\n",
    "    quantized_model.train()\n",
    "    train_model(model=quantized_model, train_loader=train_loader, test_loader=test_loader, device=cuda_device, learning_rate=1e-3, num_epochs=10)\n",
    "    quantized_model.to(cpu_device)\n",
    "\n",
    "    # Using high-level static quantization wrapper\n",
    "    # The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
    "    # quantized_model = torch.quantization.quantize_qat(model=quantized_model, run_fn=train_model, run_args=[train_loader, test_loader, cuda_device], mapping=None, inplace=False)\n",
    "\n",
    "    quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
    "\n",
    "    quantized_model.eval()\n",
    "\n",
    "    # Print quantized model.\n",
    "    print(quantized_model)\n",
    "\n",
    "    # Save quantized model.\n",
    "    save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)\n",
    "\n",
    "    # Load quantized model.\n",
    "    quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
    "\n",
    "    _, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
    "    _, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
    "\n",
    "    # Skip this assertion since the values might deviate a lot.\n",
    "    # assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
    "\n",
    "    print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
    "    print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
    "\n",
    "    fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "    int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "    int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "    fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
    "    \n",
    "    print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
    "    print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
    "    print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
    "    print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from\n",
    "# https://github.com/pytorch/vision/blob/release/0.8.0/torchvision/models/resnet.py\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from typing import Type, Any, Callable, Union, List, Optional\n",
    "\n",
    "\n",
    "__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',\n",
    "           'wide_resnet50_2', 'wide_resnet101_2']\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        # Rename relu to relu1\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "        # Remember to use two independent ReLU for layer fusion.\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        # Use FloatFunctional for addition for quantization compatibility\n",
    "        # out += identity\n",
    "        out = self.skip_add.add(identity, out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        # out += identity\n",
    "        out = self.skip_add.add(identity, out)\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,\n",
    "                    stride: int = 1, dilate: bool = False) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    arch: str,\n",
    "    block: Type[Union[BasicBlock, Bottleneck]],\n",
    "    layers: List[int],\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any\n",
    ") -> ResNet:\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-34 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-50 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-101 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNet-152 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n",
    "                   **kwargs)\n",
    "\n",
    "\n",
    "def resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNeXt-50 32x4d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 4\n",
    "    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def resnext101_32x8d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"ResNeXt-101 32x8d model from\n",
    "    `\"Aggregated Residual Transformation for Deep Neural Networks\" <https://arxiv.org/pdf/1611.05431.pdf>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['groups'] = 32\n",
    "    kwargs['width_per_group'] = 8\n",
    "    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet50_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"Wide ResNet-50-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n",
    "\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],\n",
    "                   pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def wide_resnet101_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    r\"\"\"Wide ResNet-101-2 model from\n",
    "    `\"Wide Residual Networks\" <https://arxiv.org/pdf/1605.07146.pdf>`_.\n",
    "\n",
    "    The model is the same as ResNet except for the bottleneck number of channels\n",
    "    which is twice larger in every block. The number of channels in outer 1x1\n",
    "    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n",
    "    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    kwargs['width_per_group'] = 64 * 2\n",
    "    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],\n",
    "                   pretrained, progress, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch: 00 Train Loss: 1.692 Train Acc: 0.386 Eval Loss: 1.439 Eval Acc: 0.463\n",
      "Epoch: 01 Train Loss: 1.324 Train Acc: 0.521 Eval Loss: 1.140 Eval Acc: 0.590\n",
      "Epoch: 02 Train Loss: 1.154 Train Acc: 0.591 Eval Loss: 1.118 Eval Acc: 0.609\n",
      "Epoch: 03 Train Loss: 1.036 Train Acc: 0.634 Eval Loss: 0.940 Eval Acc: 0.673\n",
      "Epoch: 04 Train Loss: 0.960 Train Acc: 0.661 Eval Loss: 0.902 Eval Acc: 0.680\n",
      "Epoch: 05 Train Loss: 0.897 Train Acc: 0.680 Eval Loss: 0.831 Eval Acc: 0.711\n",
      "Epoch: 06 Train Loss: 0.847 Train Acc: 0.699 Eval Loss: 0.831 Eval Acc: 0.712\n",
      "Epoch: 07 Train Loss: 0.801 Train Acc: 0.716 Eval Loss: 0.779 Eval Acc: 0.729\n",
      "Epoch: 08 Train Loss: 0.771 Train Acc: 0.728 Eval Loss: 0.748 Eval Acc: 0.737\n",
      "Epoch: 09 Train Loss: 0.735 Train Acc: 0.743 Eval Loss: 0.749 Eval Acc: 0.743\n",
      "Epoch: 10 Train Loss: 0.709 Train Acc: 0.751 Eval Loss: 0.737 Eval Acc: 0.754\n",
      "Epoch: 11 Train Loss: 0.685 Train Acc: 0.755 Eval Loss: 0.749 Eval Acc: 0.746\n",
      "Epoch: 12 Train Loss: 0.665 Train Acc: 0.765 Eval Loss: 0.692 Eval Acc: 0.761\n",
      "Epoch: 13 Train Loss: 0.636 Train Acc: 0.775 Eval Loss: 0.680 Eval Acc: 0.767\n",
      "Epoch: 14 Train Loss: 0.619 Train Acc: 0.781 Eval Loss: 0.674 Eval Acc: 0.771\n",
      "Epoch: 15 Train Loss: 0.594 Train Acc: 0.791 Eval Loss: 0.648 Eval Acc: 0.775\n",
      "Epoch: 16 Train Loss: 0.583 Train Acc: 0.793 Eval Loss: 0.636 Eval Acc: 0.780\n",
      "Epoch: 17 Train Loss: 0.562 Train Acc: 0.802 Eval Loss: 0.649 Eval Acc: 0.780\n",
      "Epoch: 18 Train Loss: 0.548 Train Acc: 0.806 Eval Loss: 0.624 Eval Acc: 0.787\n",
      "Epoch: 19 Train Loss: 0.535 Train Acc: 0.811 Eval Loss: 0.620 Eval Acc: 0.794\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu1): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "ResNet(\n",
      "  (conv1): ConvReLU2d(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): ReLU(inplace=True)\n",
      "  )\n",
      "  (bn1): Identity()\n",
      "  (relu): Identity()\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): ConvReLU2d(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu1): Identity()\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (bn2): Identity()\n",
      "      (skip_add): FloatFunctional(\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (relu2): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/.local/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantizedResNet18(\n",
      "  (quant): Quantize(scale=tensor([0.0408]), zero_point=tensor([60]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      "  (model_fp32): ResNet(\n",
      "    (conv1): QuantizedConvReLU2d(3, 64, kernel_size=(7, 7), stride=(2, 2), scale=0.06529717892408371, zero_point=0, padding=(3, 3))\n",
      "    (bn1): Identity()\n",
      "    (relu): Identity()\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.04520025849342346, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.08520650118589401, zero_point=67, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.13368791341781616, zero_point=40\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.035310257226228714, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.07435113191604614, zero_point=63, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.15373508632183075, zero_point=32\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(64, 128, kernel_size=(3, 3), stride=(2, 2), scale=0.03359081968665123, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.0837714672088623, zero_point=61, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (downsample): Sequential(\n",
      "          (0): QuantizedConv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), scale=0.07370596379041672, zero_point=63)\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.1200343668460846, zero_point=61\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.04080863296985626, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=0.07895044982433319, zero_point=61, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.1209041029214859, zero_point=42\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(128, 256, kernel_size=(3, 3), stride=(2, 2), scale=0.05282394587993622, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.08143730461597443, zero_point=55, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (downsample): Sequential(\n",
      "          (0): QuantizedConv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), scale=0.08892849832773209, zero_point=62)\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.11932578682899475, zero_point=59\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.05982227250933647, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=0.13656507432460785, zero_point=52, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.16041679680347443, zero_point=40\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(256, 512, kernel_size=(3, 3), stride=(2, 2), scale=0.062010716646909714, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.1446390300989151, zero_point=61, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (downsample): Sequential(\n",
      "          (0): QuantizedConv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), scale=0.11083267629146576, zero_point=63)\n",
      "          (1): Identity()\n",
      "        )\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.2057337611913681, zero_point=59\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu2): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): QuantizedConvReLU2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.09508950263261795, zero_point=0, padding=(1, 1))\n",
      "        (bn1): Identity()\n",
      "        (relu1): Identity()\n",
      "        (conv2): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=0.1981619894504547, zero_point=65, padding=(1, 1))\n",
      "        (bn2): Identity()\n",
      "        (skip_add): QFunctional(\n",
      "          scale=0.2328041046857834, zero_point=55\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (relu2): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): QuantizedLinear(in_features=512, out_features=10, scale=0.28894153237342834, zero_point=42, qscheme=torch.per_channel_affine)\n",
      "  )\n",
      ")\n",
      "FP32 evaluation accuracy: 0.794\n",
      "INT8 evaluation accuracy: 0.791\n",
      "FP32 CPU Inference Latency: 6.37 ms / sample\n",
      "FP32 CUDA Inference Latency: 2.98 ms / sample\n",
      "INT8 CPU Inference Latency: 2.57 ms / sample\n",
      "INT8 JIT CPU Inference Latency: 1.63 ms / sample\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "from src.resnet18 import resnet18\n",
    "\n",
    "def set_random_seeds(random_seed=0):\n",
    "\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "def prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256):\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    train_set = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=train_transform) \n",
    "    # We will use test set for validation and test in this project.\n",
    "    # Do not use test set for validation in practice!\n",
    "    test_set = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transform)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(test_set)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set, batch_size=train_batch_size,\n",
    "        sampler=train_sampler, num_workers=num_workers)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def evaluate_model(model, test_loader, device, criterion=None):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        if criterion is not None:\n",
    "            loss = criterion(outputs, labels).item()\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    eval_loss = running_loss / len(test_loader.dataset)\n",
    "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
    "\n",
    "    return eval_loss, eval_accuracy\n",
    "\n",
    "def train_model(model, train_loader, test_loader, device):\n",
    "\n",
    "    # The training configurations were not carefully selected.\n",
    "    learning_rate = 1e-2\n",
    "    num_epochs = 20\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
    "\n",
    "        print(\"Epoch: {:02d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
    "\n",
    "    return model\n",
    "\n",
    "def calibrate_model(model, loader, device=torch.device(\"cpu:0\")):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        _ = model(inputs)\n",
    "\n",
    "def measure_inference_latency(model,\n",
    "                              device,\n",
    "                              input_size=(1, 3, 32, 32),\n",
    "                              num_samples=100,\n",
    "                              num_warmups=10):\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    x = torch.rand(size=input_size).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_warmups):\n",
    "            _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_samples):\n",
    "            _ = model(x)\n",
    "            torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_time_ave = elapsed_time / num_samples\n",
    "\n",
    "    return elapsed_time_ave\n",
    "\n",
    "def save_model(model, model_dir, model_filename):\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.save(model.state_dict(), model_filepath)\n",
    "\n",
    "def load_model(model, model_filepath, device):\n",
    "\n",
    "    model.load_state_dict(torch.load(model_filepath, map_location=device))\n",
    "\n",
    "    return model\n",
    "\n",
    "def save_torchscript_model(model, model_dir, model_filename):\n",
    "\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    torch.jit.save(torch.jit.script(model), model_filepath)\n",
    "\n",
    "def load_torchscript_model(model_filepath, device):\n",
    "\n",
    "    model = torch.jit.load(model_filepath, map_location=device)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model(num_classes=10):\n",
    "\n",
    "    model = resnet18(num_classes=num_classes, pretrained=False)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "class QuantizedResNet18(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(QuantizedResNet18, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized.\n",
    "        # This will only be used for inputs.\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        # DeQuantStub converts tensors from quantized to floating point.\n",
    "        # This will only be used for outputs.\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        # FP32 model\n",
    "        self.model_fp32 = model_fp32\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "def model_equivalence(model_1, model_2, device, rtol=1e-05, atol=1e-08, num_tests=100, input_size=(1,3,32,32)):\n",
    "\n",
    "    model_1.to(device)\n",
    "    model_2.to(device)\n",
    "\n",
    "    for _ in range(num_tests):\n",
    "        x = torch.rand(size=input_size).to(device)\n",
    "        y1 = model_1(x).detach().cpu().numpy()\n",
    "        y2 = model_2(x).detach().cpu().numpy()\n",
    "        if np.allclose(a=y1, b=y2, rtol=rtol, atol=atol, equal_nan=False) == False:\n",
    "            print(\"Model equivalence test sample failed: \")\n",
    "            print(y1)\n",
    "            print(y2)\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def main():\n",
    "\n",
    "    random_seed = 0\n",
    "    num_classes = 10\n",
    "    cuda_device = torch.device(\"cuda:0\")\n",
    "    cpu_device = torch.device(\"cpu:0\")\n",
    "\n",
    "    model_dir = \"saved_models\"\n",
    "    model_filename = \"resnet18_cifar10.pt\"\n",
    "    quantized_model_filename = \"resnet18_quantized_cifar10.pt\"\n",
    "    model_filepath = os.path.join(model_dir, model_filename)\n",
    "    quantized_model_filepath = os.path.join(model_dir, quantized_model_filename)\n",
    "\n",
    "    set_random_seeds(random_seed=random_seed)\n",
    "\n",
    "    # Create an untrained model.\n",
    "    model = create_model(num_classes=num_classes)\n",
    "\n",
    "    train_loader, test_loader = prepare_dataloader(num_workers=8, train_batch_size=128, eval_batch_size=256)\n",
    "    \n",
    "    # Train model.\n",
    "    model = train_model(model=model, train_loader=train_loader, test_loader=test_loader, device=cuda_device)\n",
    "    # Save model.\n",
    "    save_model(model=model, model_dir=model_dir, model_filename=model_filename)\n",
    "    # Load a pretrained model.\n",
    "    model = load_model(model=model, model_filepath=model_filepath, device=cuda_device)\n",
    "    # Move the model to CPU since static quantization does not support CUDA currently.\n",
    "    model.to(cpu_device)\n",
    "    # Make a copy of the model for layer fusion\n",
    "    fused_model = copy.deepcopy(model)\n",
    "\n",
    "    model.eval()\n",
    "    # The model has to be switched to evaluation mode before any layer fusion.\n",
    "    # Otherwise the quantization will not work correctly.\n",
    "    fused_model.eval()\n",
    "\n",
    "    # Fuse the model in place rather manually.\n",
    "    fused_model = torch.quantization.fuse_modules(fused_model, [[\"conv1\", \"bn1\", \"relu\"]], inplace=True)\n",
    "    for module_name, module in fused_model.named_children():\n",
    "        if \"layer\" in module_name:\n",
    "            for basic_block_name, basic_block in module.named_children():\n",
    "                torch.quantization.fuse_modules(basic_block, [[\"conv1\", \"bn1\", \"relu1\"], [\"conv2\", \"bn2\"]], inplace=True)\n",
    "                for sub_block_name, sub_block in basic_block.named_children():\n",
    "                    if sub_block_name == \"downsample\":\n",
    "                        torch.quantization.fuse_modules(sub_block, [[\"0\", \"1\"]], inplace=True)\n",
    "\n",
    "    # Print FP32 model.\n",
    "    print(model)\n",
    "    # Print fused model.\n",
    "    print(fused_model)\n",
    "\n",
    "    # Model and fused model should be equivalent.\n",
    "    assert model_equivalence(model_1=model, model_2=fused_model, device=cpu_device, rtol=1e-03, atol=1e-06, num_tests=100, input_size=(1,3,32,32)), \"Fused model is not equivalent to the original model!\"\n",
    "\n",
    "    # Prepare the model for static quantization. This inserts observers in\n",
    "    # the model that will observe activation tensors during calibration.\n",
    "    quantized_model = QuantizedResNet18(model_fp32=fused_model)\n",
    "    # Using un-fused model will fail.\n",
    "    # Because there is no quantized layer implementation for a single batch normalization layer.\n",
    "    # quantized_model = QuantizedResNet18(model_fp32=model)\n",
    "    # Select quantization schemes from \n",
    "    # https://pytorch.org/docs/stable/quantization-support.html\n",
    "    quantization_config = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "    # Custom quantization configurations\n",
    "    # quantization_config = torch.quantization.default_qconfig\n",
    "    # quantization_config = torch.quantization.QConfig(activation=torch.quantization.MinMaxObserver.with_args(dtype=torch.quint8), weight=torch.quantization.MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
    "\n",
    "    quantized_model.qconfig = quantization_config\n",
    "    \n",
    "    # Print quantization configurations\n",
    "    print(quantized_model.qconfig)\n",
    "\n",
    "    torch.quantization.prepare(quantized_model, inplace=True)\n",
    "\n",
    "    # Use training data for calibration.\n",
    "    calibrate_model(model=quantized_model, loader=train_loader, device=cpu_device)\n",
    "\n",
    "    quantized_model = torch.quantization.convert(quantized_model, inplace=True)\n",
    "\n",
    "    # Using high-level static quantization wrapper\n",
    "    # The above steps, including torch.quantization.prepare, calibrate_model, and torch.quantization.convert, are also equivalent to\n",
    "    # quantized_model = torch.quantization.quantize(model=quantized_model, run_fn=calibrate_model, run_args=[train_loader], mapping=None, inplace=False)\n",
    "\n",
    "    quantized_model.eval()\n",
    "\n",
    "    # Print quantized model.\n",
    "    print(quantized_model)\n",
    "\n",
    "    # Save quantized model.\n",
    "    save_torchscript_model(model=quantized_model, model_dir=model_dir, model_filename=quantized_model_filename)\n",
    "\n",
    "    # Load quantized model.\n",
    "    quantized_jit_model = load_torchscript_model(model_filepath=quantized_model_filepath, device=cpu_device)\n",
    "\n",
    "    _, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
    "    _, int8_eval_accuracy = evaluate_model(model=quantized_jit_model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
    "\n",
    "    # Skip this assertion since the values might deviate a lot.\n",
    "    # assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
    "\n",
    "    print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
    "    print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))\n",
    "\n",
    "    fp32_cpu_inference_latency = measure_inference_latency(model=model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "    int8_cpu_inference_latency = measure_inference_latency(model=quantized_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "    int8_jit_cpu_inference_latency = measure_inference_latency(model=quantized_jit_model, device=cpu_device, input_size=(1,3,32,32), num_samples=100)\n",
    "    fp32_gpu_inference_latency = measure_inference_latency(model=model, device=cuda_device, input_size=(1,3,32,32), num_samples=100)\n",
    "    \n",
    "    print(\"FP32 CPU Inference Latency: {:.2f} ms / sample\".format(fp32_cpu_inference_latency * 1000))\n",
    "    print(\"FP32 CUDA Inference Latency: {:.2f} ms / sample\".format(fp32_gpu_inference_latency * 1000))\n",
    "    print(\"INT8 CPU Inference Latency: {:.2f} ms / sample\".format(int8_cpu_inference_latency * 1000))\n",
    "    print(\"INT8 JIT CPU Inference Latency: {:.2f} ms / sample\".format(int8_jit_cpu_inference_latency * 1000))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(20, 30),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(30, 10),\n",
    ")\n",
    "\n",
    "quantized_model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
