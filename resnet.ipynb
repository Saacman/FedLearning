{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from src.models import ResNet\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 80\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "#stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*stats)\n",
    "])\n",
    "\n",
    "# Normalization for testing\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(*stats)\n",
    "])\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform_train, download=True)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform_test)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize some of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (3, 32, 32) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m denorm_image \u001b[39m=\u001b[39m denormalize(image, \u001b[39m*\u001b[39mstats)\n\u001b[1;32m     16\u001b[0m \u001b[39m#ax.imshow(make_grid(denorm_image, nrow=8).permute(1, 2, 0).clamp(0,1))\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m plt\u001b[39m.\u001b[39;49mimshow(denorm_image\u001b[39m.\u001b[39;49mpermute(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m,\u001b[39m3\u001b[39;49m)\u001b[39m.\u001b[39;49mnumpy()\u001b[39m.\u001b[39;49msqueeze())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/pyplot.py:2695\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2689\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mimshow)\n\u001b[1;32m   2690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimshow\u001b[39m(\n\u001b[1;32m   2691\u001b[0m         X, cmap\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, norm\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m, aspect\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, interpolation\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2692\u001b[0m         alpha\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, vmax\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, origin\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, extent\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2693\u001b[0m         interpolation_stage\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, filternorm\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, filterrad\u001b[39m=\u001b[39m\u001b[39m4.0\u001b[39m,\n\u001b[1;32m   2694\u001b[0m         resample\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, url\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2695\u001b[0m     __ret \u001b[39m=\u001b[39m gca()\u001b[39m.\u001b[39;49mimshow(\n\u001b[1;32m   2696\u001b[0m         X, cmap\u001b[39m=\u001b[39;49mcmap, norm\u001b[39m=\u001b[39;49mnorm, aspect\u001b[39m=\u001b[39;49maspect,\n\u001b[1;32m   2697\u001b[0m         interpolation\u001b[39m=\u001b[39;49minterpolation, alpha\u001b[39m=\u001b[39;49malpha, vmin\u001b[39m=\u001b[39;49mvmin,\n\u001b[1;32m   2698\u001b[0m         vmax\u001b[39m=\u001b[39;49mvmax, origin\u001b[39m=\u001b[39;49morigin, extent\u001b[39m=\u001b[39;49mextent,\n\u001b[1;32m   2699\u001b[0m         interpolation_stage\u001b[39m=\u001b[39;49minterpolation_stage,\n\u001b[1;32m   2700\u001b[0m         filternorm\u001b[39m=\u001b[39;49mfilternorm, filterrad\u001b[39m=\u001b[39;49mfilterrad, resample\u001b[39m=\u001b[39;49mresample,\n\u001b[1;32m   2701\u001b[0m         url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}),\n\u001b[1;32m   2702\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2703\u001b[0m     sci(__ret)\n\u001b[1;32m   2704\u001b[0m     \u001b[39mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/__init__.py:1442\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1441\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1442\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1444\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1445\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1446\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/axes/_axes.py:5665\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5657\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5658\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[1;32m   5659\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[1;32m   5660\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[1;32m   5661\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   5662\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5663\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 5665\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[1;32m   5666\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5667\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5668\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/image.py:710\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n\u001b[1;32m    708\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]):\n\u001b[0;32m--> 710\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m for image data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m                     \u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape))\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m    714\u001b[0m     \u001b[39m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[39m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[39m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    717\u001b[0m     \u001b[39m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     high \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (3, 32, 32) for image data"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABkAAAAZCAYAAADE6YVjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAAJ0lEQVR4nO3NMQEAAAiAMLR/Z0zhxwpsVHm230FJSUlJSUlJSQkAB0VgBC7fKownAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def denormalize(images, means, stds):\n",
    "    means = torch.tensor(means).reshape(1, 3, 1, 1)\n",
    "    stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n",
    "    return images * stds + means\n",
    "\n",
    "\n",
    "figure = plt.figure()\n",
    "num_of_images = 20\n",
    "for index in range(1, num_of_images + 1):\n",
    "        image, labels = train_dataset[index]\n",
    "        plt.subplot(60, 20, index)\n",
    "        plt.axis('off')\n",
    "        #fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        #ax.set_xticks([]); ax.set_yticks([])\n",
    "        denorm_image = denormalize(image, *stats)\n",
    "        #ax.imshow(make_grid(denorm_image, nrow=8).permute(1, 2, 0).clamp(0,1))\n",
    "        plt.imshow(denorm_image.permute(1, 2, 0,3).numpy().squeeze())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): ResidualBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avg_pool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet = ResNet(in_channels=16, num_classes=10)\n",
    "resnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/80], Step [100/500], Loss: 1.6597\n",
      "Epoch [1/80], Step [200/500], Loss: 1.5143\n",
      "Epoch [1/80], Step [300/500], Loss: 1.3115\n",
      "Epoch [1/80], Step [400/500], Loss: 1.1933\n",
      "Epoch [1/80], Step [500/500], Loss: 1.2585\n",
      "Epoch [2/80], Step [100/500], Loss: 1.0379\n",
      "Epoch [2/80], Step [200/500], Loss: 1.2477\n",
      "Epoch [2/80], Step [300/500], Loss: 0.9151\n",
      "Epoch [2/80], Step [400/500], Loss: 1.1285\n",
      "Epoch [2/80], Step [500/500], Loss: 0.9303\n",
      "Epoch [3/80], Step [100/500], Loss: 1.0973\n",
      "Epoch [3/80], Step [200/500], Loss: 0.7247\n",
      "Epoch [3/80], Step [300/500], Loss: 0.9803\n",
      "Epoch [3/80], Step [400/500], Loss: 0.8689\n",
      "Epoch [3/80], Step [500/500], Loss: 0.6966\n",
      "Epoch [4/80], Step [100/500], Loss: 0.6708\n",
      "Epoch [4/80], Step [200/500], Loss: 0.7304\n",
      "Epoch [4/80], Step [300/500], Loss: 0.5850\n",
      "Epoch [4/80], Step [400/500], Loss: 0.7302\n",
      "Epoch [4/80], Step [500/500], Loss: 0.7242\n",
      "Epoch [5/80], Step [100/500], Loss: 0.6566\n",
      "Epoch [5/80], Step [200/500], Loss: 0.6212\n",
      "Epoch [5/80], Step [300/500], Loss: 0.7382\n",
      "Epoch [5/80], Step [400/500], Loss: 0.7502\n",
      "Epoch [5/80], Step [500/500], Loss: 0.5433\n",
      "Epoch [6/80], Step [100/500], Loss: 0.5257\n",
      "Epoch [6/80], Step [200/500], Loss: 0.6099\n",
      "Epoch [6/80], Step [300/500], Loss: 0.5466\n",
      "Epoch [6/80], Step [400/500], Loss: 0.5726\n",
      "Epoch [6/80], Step [500/500], Loss: 0.6673\n",
      "Epoch [7/80], Step [100/500], Loss: 0.4383\n",
      "Epoch [7/80], Step [200/500], Loss: 0.6449\n",
      "Epoch [7/80], Step [300/500], Loss: 0.5768\n",
      "Epoch [7/80], Step [400/500], Loss: 0.6425\n",
      "Epoch [7/80], Step [500/500], Loss: 0.5674\n",
      "Epoch [8/80], Step [100/500], Loss: 0.3861\n",
      "Epoch [8/80], Step [200/500], Loss: 0.4984\n",
      "Epoch [8/80], Step [300/500], Loss: 0.6673\n",
      "Epoch [8/80], Step [400/500], Loss: 0.5929\n",
      "Epoch [8/80], Step [500/500], Loss: 0.4786\n",
      "Epoch [9/80], Step [100/500], Loss: 0.6286\n",
      "Epoch [9/80], Step [200/500], Loss: 0.6594\n",
      "Epoch [9/80], Step [300/500], Loss: 0.4942\n",
      "Epoch [9/80], Step [400/500], Loss: 0.5732\n",
      "Epoch [9/80], Step [500/500], Loss: 0.5358\n",
      "Epoch [10/80], Step [100/500], Loss: 0.4096\n",
      "Epoch [10/80], Step [200/500], Loss: 0.5718\n",
      "Epoch [10/80], Step [300/500], Loss: 0.6898\n",
      "Epoch [10/80], Step [400/500], Loss: 0.3776\n",
      "Epoch [10/80], Step [500/500], Loss: 0.3411\n",
      "Epoch [11/80], Step [100/500], Loss: 0.4060\n",
      "Epoch [11/80], Step [200/500], Loss: 0.4445\n",
      "Epoch [11/80], Step [300/500], Loss: 0.5250\n",
      "Epoch [11/80], Step [400/500], Loss: 0.5967\n",
      "Epoch [11/80], Step [500/500], Loss: 0.5610\n",
      "Epoch [12/80], Step [100/500], Loss: 0.4354\n",
      "Epoch [12/80], Step [200/500], Loss: 0.5218\n",
      "Epoch [12/80], Step [300/500], Loss: 0.3573\n",
      "Epoch [12/80], Step [400/500], Loss: 0.6333\n",
      "Epoch [12/80], Step [500/500], Loss: 0.4569\n",
      "Epoch [13/80], Step [100/500], Loss: 0.4928\n",
      "Epoch [13/80], Step [200/500], Loss: 0.6082\n",
      "Epoch [13/80], Step [300/500], Loss: 0.3464\n",
      "Epoch [13/80], Step [400/500], Loss: 0.5174\n",
      "Epoch [13/80], Step [500/500], Loss: 0.3380\n",
      "Epoch [14/80], Step [100/500], Loss: 0.6152\n",
      "Epoch [14/80], Step [200/500], Loss: 0.2986\n",
      "Epoch [14/80], Step [300/500], Loss: 0.3676\n",
      "Epoch [14/80], Step [400/500], Loss: 0.2955\n",
      "Epoch [14/80], Step [500/500], Loss: 0.4288\n",
      "Epoch [15/80], Step [100/500], Loss: 0.4411\n",
      "Epoch [15/80], Step [200/500], Loss: 0.2793\n",
      "Epoch [15/80], Step [300/500], Loss: 0.3710\n",
      "Epoch [15/80], Step [400/500], Loss: 0.4539\n",
      "Epoch [15/80], Step [500/500], Loss: 0.5181\n",
      "Epoch [16/80], Step [100/500], Loss: 0.2879\n",
      "Epoch [16/80], Step [200/500], Loss: 0.4396\n",
      "Epoch [16/80], Step [300/500], Loss: 0.3258\n",
      "Epoch [16/80], Step [400/500], Loss: 0.3402\n",
      "Epoch [16/80], Step [500/500], Loss: 0.4209\n",
      "Epoch [17/80], Step [100/500], Loss: 0.2974\n",
      "Epoch [17/80], Step [200/500], Loss: 0.5500\n",
      "Epoch [17/80], Step [300/500], Loss: 0.3027\n",
      "Epoch [17/80], Step [400/500], Loss: 0.3653\n",
      "Epoch [17/80], Step [500/500], Loss: 0.3160\n",
      "Epoch [18/80], Step [100/500], Loss: 0.3548\n",
      "Epoch [18/80], Step [200/500], Loss: 0.3965\n",
      "Epoch [18/80], Step [300/500], Loss: 0.3141\n",
      "Epoch [18/80], Step [400/500], Loss: 0.4203\n",
      "Epoch [18/80], Step [500/500], Loss: 0.3361\n",
      "Epoch [19/80], Step [100/500], Loss: 0.3180\n",
      "Epoch [19/80], Step [200/500], Loss: 0.3016\n",
      "Epoch [19/80], Step [300/500], Loss: 0.2843\n",
      "Epoch [19/80], Step [400/500], Loss: 0.3319\n",
      "Epoch [19/80], Step [500/500], Loss: 0.3840\n",
      "Epoch [20/80], Step [100/500], Loss: 0.2502\n",
      "Epoch [20/80], Step [200/500], Loss: 0.2160\n",
      "Epoch [20/80], Step [300/500], Loss: 0.5800\n",
      "Epoch [20/80], Step [400/500], Loss: 0.4151\n",
      "Epoch [20/80], Step [500/500], Loss: 0.3328\n",
      "Epoch [21/80], Step [100/500], Loss: 0.3400\n",
      "Epoch [21/80], Step [200/500], Loss: 0.3882\n",
      "Epoch [21/80], Step [300/500], Loss: 0.3399\n",
      "Epoch [21/80], Step [400/500], Loss: 0.3020\n",
      "Epoch [21/80], Step [500/500], Loss: 0.4104\n",
      "Epoch [22/80], Step [100/500], Loss: 0.2461\n",
      "Epoch [22/80], Step [200/500], Loss: 0.2317\n",
      "Epoch [22/80], Step [300/500], Loss: 0.4618\n",
      "Epoch [22/80], Step [400/500], Loss: 0.3355\n",
      "Epoch [22/80], Step [500/500], Loss: 0.2814\n",
      "Epoch [23/80], Step [100/500], Loss: 0.3496\n",
      "Epoch [23/80], Step [200/500], Loss: 0.3651\n",
      "Epoch [23/80], Step [300/500], Loss: 0.2855\n",
      "Epoch [23/80], Step [400/500], Loss: 0.3319\n",
      "Epoch [23/80], Step [500/500], Loss: 0.3775\n",
      "Epoch [24/80], Step [100/500], Loss: 0.3512\n",
      "Epoch [24/80], Step [200/500], Loss: 0.3307\n",
      "Epoch [24/80], Step [300/500], Loss: 0.2661\n",
      "Epoch [24/80], Step [400/500], Loss: 0.2567\n",
      "Epoch [24/80], Step [500/500], Loss: 0.4137\n",
      "Epoch [25/80], Step [100/500], Loss: 0.2305\n",
      "Epoch [25/80], Step [200/500], Loss: 0.4048\n",
      "Epoch [25/80], Step [300/500], Loss: 0.3765\n",
      "Epoch [25/80], Step [400/500], Loss: 0.3391\n",
      "Epoch [25/80], Step [500/500], Loss: 0.2334\n",
      "Epoch [26/80], Step [100/500], Loss: 0.3289\n",
      "Epoch [26/80], Step [200/500], Loss: 0.3285\n",
      "Epoch [26/80], Step [300/500], Loss: 0.2520\n",
      "Epoch [26/80], Step [400/500], Loss: 0.3428\n",
      "Epoch [26/80], Step [500/500], Loss: 0.2913\n",
      "Epoch [27/80], Step [100/500], Loss: 0.2111\n",
      "Epoch [27/80], Step [200/500], Loss: 0.4403\n",
      "Epoch [27/80], Step [300/500], Loss: 0.3389\n",
      "Epoch [27/80], Step [400/500], Loss: 0.4156\n",
      "Epoch [27/80], Step [500/500], Loss: 0.2385\n",
      "Epoch [28/80], Step [100/500], Loss: 0.5157\n",
      "Epoch [28/80], Step [200/500], Loss: 0.3063\n",
      "Epoch [28/80], Step [300/500], Loss: 0.3184\n",
      "Epoch [28/80], Step [400/500], Loss: 0.2578\n",
      "Epoch [28/80], Step [500/500], Loss: 0.2904\n",
      "Epoch [29/80], Step [100/500], Loss: 0.4045\n",
      "Epoch [29/80], Step [200/500], Loss: 0.3584\n",
      "Epoch [29/80], Step [300/500], Loss: 0.3080\n",
      "Epoch [29/80], Step [400/500], Loss: 0.1962\n",
      "Epoch [29/80], Step [500/500], Loss: 0.3781\n",
      "Epoch [30/80], Step [100/500], Loss: 0.2223\n",
      "Epoch [30/80], Step [200/500], Loss: 0.2946\n",
      "Epoch [30/80], Step [300/500], Loss: 0.2323\n",
      "Epoch [30/80], Step [400/500], Loss: 0.3206\n",
      "Epoch [30/80], Step [500/500], Loss: 0.3056\n",
      "Epoch [31/80], Step [100/500], Loss: 0.3130\n",
      "Epoch [31/80], Step [200/500], Loss: 0.3155\n",
      "Epoch [31/80], Step [300/500], Loss: 0.3966\n",
      "Epoch [31/80], Step [400/500], Loss: 0.2423\n",
      "Epoch [31/80], Step [500/500], Loss: 0.3948\n",
      "Epoch [32/80], Step [100/500], Loss: 0.1817\n",
      "Epoch [32/80], Step [200/500], Loss: 0.2600\n",
      "Epoch [32/80], Step [300/500], Loss: 0.3670\n",
      "Epoch [32/80], Step [400/500], Loss: 0.2772\n",
      "Epoch [32/80], Step [500/500], Loss: 0.2328\n",
      "Epoch [33/80], Step [100/500], Loss: 0.1693\n",
      "Epoch [33/80], Step [200/500], Loss: 0.2994\n",
      "Epoch [33/80], Step [300/500], Loss: 0.3249\n",
      "Epoch [33/80], Step [400/500], Loss: 0.3134\n",
      "Epoch [33/80], Step [500/500], Loss: 0.1399\n",
      "Epoch [34/80], Step [100/500], Loss: 0.2849\n",
      "Epoch [34/80], Step [200/500], Loss: 0.2745\n",
      "Epoch [34/80], Step [300/500], Loss: 0.2532\n",
      "Epoch [34/80], Step [400/500], Loss: 0.4536\n",
      "Epoch [34/80], Step [500/500], Loss: 0.2566\n",
      "Epoch [35/80], Step [100/500], Loss: 0.3676\n",
      "Epoch [35/80], Step [200/500], Loss: 0.2323\n",
      "Epoch [35/80], Step [300/500], Loss: 0.2943\n",
      "Epoch [35/80], Step [400/500], Loss: 0.2117\n",
      "Epoch [35/80], Step [500/500], Loss: 0.3576\n",
      "Epoch [36/80], Step [100/500], Loss: 0.2398\n",
      "Epoch [36/80], Step [200/500], Loss: 0.1650\n",
      "Epoch [36/80], Step [300/500], Loss: 0.2043\n",
      "Epoch [36/80], Step [400/500], Loss: 0.2022\n",
      "Epoch [36/80], Step [500/500], Loss: 0.1308\n",
      "Epoch [37/80], Step [100/500], Loss: 0.1661\n",
      "Epoch [37/80], Step [200/500], Loss: 0.2713\n",
      "Epoch [37/80], Step [300/500], Loss: 0.3255\n",
      "Epoch [37/80], Step [400/500], Loss: 0.1906\n",
      "Epoch [37/80], Step [500/500], Loss: 0.1891\n",
      "Epoch [38/80], Step [100/500], Loss: 0.2253\n",
      "Epoch [38/80], Step [200/500], Loss: 0.1941\n",
      "Epoch [38/80], Step [300/500], Loss: 0.2292\n",
      "Epoch [38/80], Step [400/500], Loss: 0.1751\n",
      "Epoch [38/80], Step [500/500], Loss: 0.3215\n",
      "Epoch [39/80], Step [100/500], Loss: 0.2171\n",
      "Epoch [39/80], Step [200/500], Loss: 0.1802\n",
      "Epoch [39/80], Step [300/500], Loss: 0.2402\n",
      "Epoch [39/80], Step [400/500], Loss: 0.2990\n",
      "Epoch [39/80], Step [500/500], Loss: 0.2013\n",
      "Epoch [40/80], Step [100/500], Loss: 0.3086\n",
      "Epoch [40/80], Step [200/500], Loss: 0.1700\n",
      "Epoch [40/80], Step [300/500], Loss: 0.2211\n",
      "Epoch [40/80], Step [400/500], Loss: 0.3059\n",
      "Epoch [40/80], Step [500/500], Loss: 0.3650\n",
      "Epoch [41/80], Step [100/500], Loss: 0.3410\n",
      "Epoch [41/80], Step [200/500], Loss: 0.2201\n",
      "Epoch [41/80], Step [300/500], Loss: 0.2753\n",
      "Epoch [41/80], Step [400/500], Loss: 0.1553\n",
      "Epoch [41/80], Step [500/500], Loss: 0.2787\n",
      "Epoch [42/80], Step [100/500], Loss: 0.2563\n",
      "Epoch [42/80], Step [200/500], Loss: 0.1283\n",
      "Epoch [42/80], Step [300/500], Loss: 0.2023\n",
      "Epoch [42/80], Step [400/500], Loss: 0.1929\n",
      "Epoch [42/80], Step [500/500], Loss: 0.2893\n",
      "Epoch [43/80], Step [100/500], Loss: 0.2688\n",
      "Epoch [43/80], Step [200/500], Loss: 0.2849\n",
      "Epoch [43/80], Step [300/500], Loss: 0.2328\n",
      "Epoch [43/80], Step [400/500], Loss: 0.1390\n",
      "Epoch [43/80], Step [500/500], Loss: 0.2869\n",
      "Epoch [44/80], Step [100/500], Loss: 0.0988\n",
      "Epoch [44/80], Step [200/500], Loss: 0.2644\n",
      "Epoch [44/80], Step [300/500], Loss: 0.1958\n",
      "Epoch [44/80], Step [400/500], Loss: 0.3369\n",
      "Epoch [44/80], Step [500/500], Loss: 0.2329\n",
      "Epoch [45/80], Step [100/500], Loss: 0.2048\n",
      "Epoch [45/80], Step [200/500], Loss: 0.1198\n",
      "Epoch [45/80], Step [300/500], Loss: 0.2354\n",
      "Epoch [45/80], Step [400/500], Loss: 0.1836\n",
      "Epoch [45/80], Step [500/500], Loss: 0.2319\n",
      "Epoch [46/80], Step [100/500], Loss: 0.1225\n",
      "Epoch [46/80], Step [200/500], Loss: 0.1460\n",
      "Epoch [46/80], Step [300/500], Loss: 0.2587\n",
      "Epoch [46/80], Step [400/500], Loss: 0.2646\n",
      "Epoch [46/80], Step [500/500], Loss: 0.2925\n",
      "Epoch [47/80], Step [100/500], Loss: 0.2282\n",
      "Epoch [47/80], Step [200/500], Loss: 0.2466\n",
      "Epoch [47/80], Step [300/500], Loss: 0.1341\n",
      "Epoch [47/80], Step [400/500], Loss: 0.2297\n",
      "Epoch [47/80], Step [500/500], Loss: 0.1908\n",
      "Epoch [48/80], Step [100/500], Loss: 0.1507\n",
      "Epoch [48/80], Step [200/500], Loss: 0.1883\n",
      "Epoch [48/80], Step [300/500], Loss: 0.1915\n",
      "Epoch [48/80], Step [400/500], Loss: 0.1560\n",
      "Epoch [48/80], Step [500/500], Loss: 0.3384\n",
      "Epoch [49/80], Step [100/500], Loss: 0.1892\n",
      "Epoch [49/80], Step [200/500], Loss: 0.2092\n",
      "Epoch [49/80], Step [300/500], Loss: 0.2421\n",
      "Epoch [49/80], Step [400/500], Loss: 0.2312\n",
      "Epoch [49/80], Step [500/500], Loss: 0.2275\n",
      "Epoch [50/80], Step [100/500], Loss: 0.1434\n",
      "Epoch [50/80], Step [200/500], Loss: 0.0784\n",
      "Epoch [50/80], Step [300/500], Loss: 0.1305\n",
      "Epoch [50/80], Step [400/500], Loss: 0.2089\n",
      "Epoch [50/80], Step [500/500], Loss: 0.2608\n",
      "Epoch [51/80], Step [100/500], Loss: 0.1609\n",
      "Epoch [51/80], Step [200/500], Loss: 0.2016\n",
      "Epoch [51/80], Step [300/500], Loss: 0.1880\n",
      "Epoch [51/80], Step [400/500], Loss: 0.1591\n",
      "Epoch [51/80], Step [500/500], Loss: 0.2930\n",
      "Epoch [52/80], Step [100/500], Loss: 0.2778\n",
      "Epoch [52/80], Step [200/500], Loss: 0.0618\n",
      "Epoch [52/80], Step [300/500], Loss: 0.1761\n",
      "Epoch [52/80], Step [400/500], Loss: 0.1341\n",
      "Epoch [52/80], Step [500/500], Loss: 0.1818\n",
      "Epoch [53/80], Step [100/500], Loss: 0.1609\n",
      "Epoch [53/80], Step [200/500], Loss: 0.2162\n",
      "Epoch [53/80], Step [300/500], Loss: 0.1464\n",
      "Epoch [53/80], Step [400/500], Loss: 0.2471\n",
      "Epoch [53/80], Step [500/500], Loss: 0.2350\n",
      "Epoch [54/80], Step [100/500], Loss: 0.2262\n",
      "Epoch [54/80], Step [200/500], Loss: 0.0993\n",
      "Epoch [54/80], Step [300/500], Loss: 0.2797\n",
      "Epoch [54/80], Step [400/500], Loss: 0.2570\n",
      "Epoch [54/80], Step [500/500], Loss: 0.2149\n",
      "Epoch [55/80], Step [100/500], Loss: 0.1742\n",
      "Epoch [55/80], Step [200/500], Loss: 0.1705\n",
      "Epoch [55/80], Step [300/500], Loss: 0.1888\n",
      "Epoch [55/80], Step [400/500], Loss: 0.1011\n",
      "Epoch [55/80], Step [500/500], Loss: 0.1822\n",
      "Epoch [56/80], Step [100/500], Loss: 0.1642\n",
      "Epoch [56/80], Step [200/500], Loss: 0.3339\n",
      "Epoch [56/80], Step [300/500], Loss: 0.1662\n",
      "Epoch [56/80], Step [400/500], Loss: 0.2576\n",
      "Epoch [56/80], Step [500/500], Loss: 0.0702\n",
      "Epoch [57/80], Step [100/500], Loss: 0.2529\n",
      "Epoch [57/80], Step [200/500], Loss: 0.1495\n",
      "Epoch [57/80], Step [300/500], Loss: 0.3330\n",
      "Epoch [57/80], Step [400/500], Loss: 0.2272\n",
      "Epoch [57/80], Step [500/500], Loss: 0.1748\n",
      "Epoch [58/80], Step [100/500], Loss: 0.3144\n",
      "Epoch [58/80], Step [200/500], Loss: 0.1827\n",
      "Epoch [58/80], Step [300/500], Loss: 0.2100\n",
      "Epoch [58/80], Step [400/500], Loss: 0.0933\n",
      "Epoch [58/80], Step [500/500], Loss: 0.1465\n",
      "Epoch [59/80], Step [100/500], Loss: 0.1338\n",
      "Epoch [59/80], Step [200/500], Loss: 0.1695\n",
      "Epoch [59/80], Step [300/500], Loss: 0.3047\n",
      "Epoch [59/80], Step [400/500], Loss: 0.1915\n",
      "Epoch [59/80], Step [500/500], Loss: 0.1495\n",
      "Epoch [60/80], Step [100/500], Loss: 0.1068\n",
      "Epoch [60/80], Step [200/500], Loss: 0.1222\n",
      "Epoch [60/80], Step [300/500], Loss: 0.2687\n",
      "Epoch [60/80], Step [400/500], Loss: 0.1382\n",
      "Epoch [60/80], Step [500/500], Loss: 0.1223\n",
      "Epoch [61/80], Step [100/500], Loss: 0.1236\n",
      "Epoch [61/80], Step [200/500], Loss: 0.2762\n",
      "Epoch [61/80], Step [300/500], Loss: 0.1718\n",
      "Epoch [61/80], Step [400/500], Loss: 0.1333\n",
      "Epoch [61/80], Step [500/500], Loss: 0.1876\n",
      "Epoch [62/80], Step [100/500], Loss: 0.0896\n",
      "Epoch [62/80], Step [200/500], Loss: 0.2237\n",
      "Epoch [62/80], Step [300/500], Loss: 0.1549\n",
      "Epoch [62/80], Step [400/500], Loss: 0.2138\n",
      "Epoch [62/80], Step [500/500], Loss: 0.1250\n",
      "Epoch [63/80], Step [100/500], Loss: 0.1433\n",
      "Epoch [63/80], Step [200/500], Loss: 0.2167\n",
      "Epoch [63/80], Step [300/500], Loss: 0.2180\n",
      "Epoch [63/80], Step [400/500], Loss: 0.1668\n",
      "Epoch [63/80], Step [500/500], Loss: 0.2521\n",
      "Epoch [64/80], Step [100/500], Loss: 0.2031\n",
      "Epoch [64/80], Step [200/500], Loss: 0.1604\n",
      "Epoch [64/80], Step [300/500], Loss: 0.1924\n",
      "Epoch [64/80], Step [400/500], Loss: 0.1655\n",
      "Epoch [64/80], Step [500/500], Loss: 0.2049\n",
      "Epoch [65/80], Step [100/500], Loss: 0.1786\n",
      "Epoch [65/80], Step [200/500], Loss: 0.1633\n",
      "Epoch [65/80], Step [300/500], Loss: 0.2058\n",
      "Epoch [65/80], Step [400/500], Loss: 0.1581\n",
      "Epoch [65/80], Step [500/500], Loss: 0.1728\n",
      "Epoch [66/80], Step [100/500], Loss: 0.2147\n",
      "Epoch [66/80], Step [200/500], Loss: 0.2247\n",
      "Epoch [66/80], Step [300/500], Loss: 0.2027\n",
      "Epoch [66/80], Step [400/500], Loss: 0.1693\n",
      "Epoch [66/80], Step [500/500], Loss: 0.1556\n",
      "Epoch [67/80], Step [100/500], Loss: 0.1769\n",
      "Epoch [67/80], Step [200/500], Loss: 0.1116\n",
      "Epoch [67/80], Step [300/500], Loss: 0.1408\n",
      "Epoch [67/80], Step [400/500], Loss: 0.2581\n",
      "Epoch [67/80], Step [500/500], Loss: 0.1788\n",
      "Epoch [68/80], Step [100/500], Loss: 0.1476\n",
      "Epoch [68/80], Step [200/500], Loss: 0.1430\n",
      "Epoch [68/80], Step [300/500], Loss: 0.1746\n",
      "Epoch [68/80], Step [400/500], Loss: 0.3095\n",
      "Epoch [68/80], Step [500/500], Loss: 0.1669\n",
      "Epoch [69/80], Step [100/500], Loss: 0.0814\n",
      "Epoch [69/80], Step [200/500], Loss: 0.1114\n",
      "Epoch [69/80], Step [300/500], Loss: 0.1323\n",
      "Epoch [69/80], Step [400/500], Loss: 0.1196\n",
      "Epoch [69/80], Step [500/500], Loss: 0.1974\n",
      "Epoch [70/80], Step [100/500], Loss: 0.1486\n",
      "Epoch [70/80], Step [200/500], Loss: 0.1790\n",
      "Epoch [70/80], Step [300/500], Loss: 0.2025\n",
      "Epoch [70/80], Step [400/500], Loss: 0.1112\n",
      "Epoch [70/80], Step [500/500], Loss: 0.1451\n",
      "Epoch [71/80], Step [100/500], Loss: 0.1126\n",
      "Epoch [71/80], Step [200/500], Loss: 0.1418\n",
      "Epoch [71/80], Step [300/500], Loss: 0.1413\n",
      "Epoch [71/80], Step [400/500], Loss: 0.1020\n",
      "Epoch [71/80], Step [500/500], Loss: 0.1892\n",
      "Epoch [72/80], Step [100/500], Loss: 0.0906\n",
      "Epoch [72/80], Step [200/500], Loss: 0.1622\n",
      "Epoch [72/80], Step [300/500], Loss: 0.0854\n",
      "Epoch [72/80], Step [400/500], Loss: 0.1918\n",
      "Epoch [72/80], Step [500/500], Loss: 0.1109\n",
      "Epoch [73/80], Step [100/500], Loss: 0.1361\n",
      "Epoch [73/80], Step [200/500], Loss: 0.2102\n",
      "Epoch [73/80], Step [300/500], Loss: 0.1847\n",
      "Epoch [73/80], Step [400/500], Loss: 0.2135\n",
      "Epoch [73/80], Step [500/500], Loss: 0.1698\n",
      "Epoch [74/80], Step [100/500], Loss: 0.0645\n",
      "Epoch [74/80], Step [200/500], Loss: 0.2007\n",
      "Epoch [74/80], Step [300/500], Loss: 0.1251\n",
      "Epoch [74/80], Step [400/500], Loss: 0.0985\n",
      "Epoch [74/80], Step [500/500], Loss: 0.1702\n",
      "Epoch [75/80], Step [100/500], Loss: 0.2318\n",
      "Epoch [75/80], Step [200/500], Loss: 0.1222\n",
      "Epoch [75/80], Step [300/500], Loss: 0.1905\n",
      "Epoch [75/80], Step [400/500], Loss: 0.2268\n",
      "Epoch [75/80], Step [500/500], Loss: 0.1752\n",
      "Epoch [76/80], Step [100/500], Loss: 0.1910\n",
      "Epoch [76/80], Step [200/500], Loss: 0.1593\n",
      "Epoch [76/80], Step [300/500], Loss: 0.1476\n",
      "Epoch [76/80], Step [400/500], Loss: 0.1830\n",
      "Epoch [76/80], Step [500/500], Loss: 0.1359\n",
      "Epoch [77/80], Step [100/500], Loss: 0.0941\n",
      "Epoch [77/80], Step [200/500], Loss: 0.1142\n",
      "Epoch [77/80], Step [300/500], Loss: 0.1084\n",
      "Epoch [77/80], Step [400/500], Loss: 0.0876\n",
      "Epoch [77/80], Step [500/500], Loss: 0.1244\n",
      "Epoch [78/80], Step [100/500], Loss: 0.1269\n",
      "Epoch [78/80], Step [200/500], Loss: 0.2127\n",
      "Epoch [78/80], Step [300/500], Loss: 0.0999\n",
      "Epoch [78/80], Step [400/500], Loss: 0.1659\n",
      "Epoch [78/80], Step [500/500], Loss: 0.0527\n",
      "Epoch [79/80], Step [100/500], Loss: 0.0857\n",
      "Epoch [79/80], Step [200/500], Loss: 0.1332\n",
      "Epoch [79/80], Step [300/500], Loss: 0.1789\n",
      "Epoch [79/80], Step [400/500], Loss: 0.2239\n",
      "Epoch [79/80], Step [500/500], Loss: 0.0891\n",
      "Epoch [80/80], Step [100/500], Loss: 0.1258\n",
      "Epoch [80/80], Step [200/500], Loss: 0.2321\n",
      "Epoch [80/80], Step [300/500], Loss: 0.1341\n",
      "Epoch [80/80], Step [400/500], Loss: 0.0440\n",
      "Epoch [80/80], Step [500/500], Loss: 0.2642\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = resnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(resnet.state_dict(), 'resnet.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 88.26 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model_dict = torch.load('resnet.ckpt')\n",
    "model = ResNet(in_channels=16, num_classes=10)\n",
    "model.load_state_dict(model_dict)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
