{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QAT for MLP\n",
    "### Quantization aware training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from src.models import MLP\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedMLP(nn.Module):\n",
    "    def __init__(self, model_fp32):\n",
    "        super(QuantizedMLP, self).__init__()\n",
    "        # QuantStub converts tensors from floating point to quantized.\n",
    "        # This will only be used for inputs.\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        # DeQuantStub converts tensors from quantized to floating point.\n",
    "        # This will only be used for outputs.\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "        # FP32 model\n",
    "        self.model_fp32 = model_fp32\n",
    "\n",
    "    def forward(self, x):\n",
    "        # manually specify where tensors will be converted from floating\n",
    "        # point to quantized in the quantized model\n",
    "        x = self.quant(x)\n",
    "        x = self.model_fp32(x)\n",
    "        # manually specify where tensors will be converted from quantized\n",
    "        # to floating point in the quantized model\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "    \n",
    "def evaluate_model(model, test_loader, device, criterion=None):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    running_loss = 0\n",
    "    running_corrects = 0\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        if criterion is not None:\n",
    "            loss = criterion(outputs, labels).item()\n",
    "        else:\n",
    "            loss = 0\n",
    "\n",
    "        # statistics\n",
    "        running_loss += loss * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    eval_loss = running_loss / len(test_loader.dataset)\n",
    "    eval_accuracy = running_corrects / len(test_loader.dataset)\n",
    "\n",
    "    return eval_loss, eval_accuracy\n",
    "\n",
    "def train_model(model, train_loader, test_loader, device):\n",
    "\n",
    "    # The training configurations were not carefully selected.\n",
    "    learning_rate = 1e-2\n",
    "    num_epochs = 20\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # It seems that SGD optimizer is better than Adam optimizer for ResNet18 training on CIFAR10.\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = running_corrects / len(train_loader.dataset)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)\n",
    "\n",
    "        print(\"Epoch: {:02d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}\".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformation to apply to each image in the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # convert the image to a PyTorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)) # normalize the image with mean=0.5 and std=0.5\n",
    "])\n",
    "\n",
    "# load the MNIST training and testing datasets\n",
    "train_dataset = datasets.MNIST(root='data/', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='data/', train=False, transform=transform, download=True)\n",
    "\n",
    "# create data loaders to load the datasets in batches during training and testing\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedMLP(\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       "  (model_fp32): MLP(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "      (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (3): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (4): Linear(in_features=64, out_features=10, bias=True)\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "    (soft): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = [ 28 * 28, # input\n",
    "                512, 256, 128, 64,\n",
    "                10 ] #output\n",
    "# instantiate the model\n",
    "\n",
    "model = MLP(parameters)\n",
    "# define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # computes the cross-entropy loss between the predicted and true labels\n",
    "#criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) # performs stochastic gradient descent with adaptive learning rate\n",
    "\n",
    "# set the device to run the model on\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "cpu_device = torch.device(\"cpu:0\")\n",
    "model.to(cpu_device)\n",
    "# create a model instance\n",
    "model_fp32 = QuantizedMLP(model)\n",
    "\n",
    "# model must be set to eval for fusion to work\n",
    "model_fp32.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quant\n",
      "dequant\n",
      "model_fp32\n",
      "\tlinears\n",
      "\t\t0\n",
      "\t\t1\n",
      "\t\t2\n",
      "\t\t3\n",
      "\t\t4\n",
      "\trelu\n",
      "\tsoft\n"
     ]
    }
   ],
   "source": [
    "for module_name, module in model_fp32.named_children():\n",
    "        print(module_name)\n",
    "        if \"model\" in module_name:\n",
    "            for basic_block_name, basic_block in module.named_children():\n",
    "\n",
    "                print(f\"\\t{basic_block_name}\")\n",
    "                if \"linears\" in basic_block_name:\n",
    "                    for block_name, block in basic_block.named_children():\n",
    "                        print(f\"\\t\\t{block_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/.local/lib/python3.8/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 Train Loss: 1.001 Train Acc: 0.797 Eval Loss: 0.276 Eval Acc: 0.909\n",
      "Epoch: 01 Train Loss: 0.199 Train Acc: 0.938 Eval Loss: 0.234 Eval Acc: 0.923\n",
      "Epoch: 02 Train Loss: 0.137 Train Acc: 0.957 Eval Loss: 0.121 Eval Acc: 0.963\n",
      "Epoch: 03 Train Loss: 0.104 Train Acc: 0.967 Eval Loss: 0.098 Eval Acc: 0.969\n",
      "Epoch: 04 Train Loss: 0.086 Train Acc: 0.973 Eval Loss: 0.090 Eval Acc: 0.970\n",
      "Epoch: 05 Train Loss: 0.070 Train Acc: 0.978 Eval Loss: 0.091 Eval Acc: 0.972\n",
      "Epoch: 06 Train Loss: 0.060 Train Acc: 0.981 Eval Loss: 0.080 Eval Acc: 0.975\n",
      "Epoch: 07 Train Loss: 0.054 Train Acc: 0.982 Eval Loss: 0.077 Eval Acc: 0.977\n",
      "Epoch: 08 Train Loss: 0.046 Train Acc: 0.985 Eval Loss: 0.084 Eval Acc: 0.975\n",
      "Epoch: 09 Train Loss: 0.042 Train Acc: 0.986 Eval Loss: 0.081 Eval Acc: 0.977\n",
      "Epoch: 10 Train Loss: 0.037 Train Acc: 0.988 Eval Loss: 0.071 Eval Acc: 0.979\n",
      "Epoch: 11 Train Loss: 0.031 Train Acc: 0.990 Eval Loss: 0.076 Eval Acc: 0.976\n",
      "Epoch: 12 Train Loss: 0.029 Train Acc: 0.990 Eval Loss: 0.078 Eval Acc: 0.979\n",
      "Epoch: 13 Train Loss: 0.025 Train Acc: 0.992 Eval Loss: 0.070 Eval Acc: 0.982\n",
      "Epoch: 14 Train Loss: 0.023 Train Acc: 0.992 Eval Loss: 0.072 Eval Acc: 0.980\n",
      "Epoch: 15 Train Loss: 0.020 Train Acc: 0.993 Eval Loss: 0.069 Eval Acc: 0.981\n",
      "Epoch: 16 Train Loss: 0.019 Train Acc: 0.994 Eval Loss: 0.090 Eval Acc: 0.977\n",
      "Epoch: 17 Train Loss: 0.020 Train Acc: 0.994 Eval Loss: 0.076 Eval Acc: 0.982\n",
      "Epoch: 18 Train Loss: 0.017 Train Acc: 0.995 Eval Loss: 0.087 Eval Acc: 0.978\n",
      "Epoch: 19 Train Loss: 0.012 Train Acc: 0.996 Eval Loss: 0.074 Eval Acc: 0.982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuantizedMLP(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0157]), zero_point=tensor([64], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-1.0, max_val=1.0)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       "  (model_fp32): MLP(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(\n",
       "        in_features=784, out_features=512, bias=True\n",
       "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0009, 0.0009, 0.0013, 0.0016, 0.0003, 0.0013, 0.0008, 0.0004, 0.0013,\n",
       "                  0.0008, 0.0010, 0.0008, 0.0008, 0.0010, 0.0006, 0.0008, 0.0010, 0.0007,\n",
       "                  0.0007, 0.0012, 0.0003, 0.0012, 0.0005, 0.0013, 0.0003, 0.0010, 0.0009,\n",
       "                  0.0011, 0.0009, 0.0009, 0.0008, 0.0010, 0.0007, 0.0005, 0.0011, 0.0004,\n",
       "                  0.0003, 0.0003, 0.0013, 0.0011, 0.0016, 0.0006, 0.0010, 0.0007, 0.0007,\n",
       "                  0.0008, 0.0008, 0.0003, 0.0005, 0.0009, 0.0011, 0.0006, 0.0004, 0.0004,\n",
       "                  0.0008, 0.0009, 0.0003, 0.0011, 0.0010, 0.0013, 0.0009, 0.0005, 0.0003,\n",
       "                  0.0011, 0.0015, 0.0013, 0.0004, 0.0010, 0.0015, 0.0010, 0.0006, 0.0006,\n",
       "                  0.0004, 0.0008, 0.0011, 0.0009, 0.0007, 0.0004, 0.0007, 0.0013, 0.0006,\n",
       "                  0.0010, 0.0009, 0.0010, 0.0003, 0.0008, 0.0005, 0.0009, 0.0016, 0.0007,\n",
       "                  0.0009, 0.0009, 0.0008, 0.0009, 0.0011, 0.0007, 0.0008, 0.0009, 0.0010,\n",
       "                  0.0003, 0.0010, 0.0006, 0.0010, 0.0004, 0.0010, 0.0008, 0.0008, 0.0013,\n",
       "                  0.0009, 0.0012, 0.0011, 0.0009, 0.0008, 0.0014, 0.0008, 0.0010, 0.0005,\n",
       "                  0.0016, 0.0006, 0.0003, 0.0007, 0.0010, 0.0012, 0.0004, 0.0003, 0.0008,\n",
       "                  0.0004, 0.0012, 0.0012, 0.0007, 0.0009, 0.0006, 0.0004, 0.0006, 0.0010,\n",
       "                  0.0010, 0.0008, 0.0005, 0.0009, 0.0007, 0.0008, 0.0003, 0.0008, 0.0010,\n",
       "                  0.0011, 0.0010, 0.0009, 0.0008, 0.0007, 0.0013, 0.0009, 0.0008, 0.0008,\n",
       "                  0.0008, 0.0008, 0.0010, 0.0006, 0.0011, 0.0007, 0.0005, 0.0012, 0.0005,\n",
       "                  0.0006, 0.0010, 0.0012, 0.0011, 0.0011, 0.0003, 0.0008, 0.0007, 0.0007,\n",
       "                  0.0005, 0.0003, 0.0007, 0.0007, 0.0007, 0.0011, 0.0011, 0.0009, 0.0007,\n",
       "                  0.0009, 0.0003, 0.0004, 0.0012, 0.0015, 0.0010, 0.0013, 0.0009, 0.0010,\n",
       "                  0.0009, 0.0013, 0.0003, 0.0010, 0.0011, 0.0005, 0.0014, 0.0011, 0.0007,\n",
       "                  0.0008, 0.0010, 0.0009, 0.0013, 0.0009, 0.0009, 0.0008, 0.0010, 0.0013,\n",
       "                  0.0009, 0.0013, 0.0011, 0.0011, 0.0011, 0.0005, 0.0010, 0.0007, 0.0009,\n",
       "                  0.0010, 0.0008, 0.0004, 0.0008, 0.0007, 0.0011, 0.0008, 0.0010, 0.0006,\n",
       "                  0.0009, 0.0003, 0.0009, 0.0006, 0.0007, 0.0004, 0.0004, 0.0012, 0.0003,\n",
       "                  0.0008, 0.0012, 0.0003, 0.0005, 0.0003, 0.0010, 0.0007, 0.0003, 0.0008,\n",
       "                  0.0011, 0.0003, 0.0009, 0.0010, 0.0009, 0.0010, 0.0009, 0.0008, 0.0009,\n",
       "                  0.0007, 0.0009, 0.0007, 0.0010, 0.0008, 0.0003, 0.0008, 0.0008, 0.0009,\n",
       "                  0.0009, 0.0009, 0.0008, 0.0008, 0.0012, 0.0007, 0.0008, 0.0006, 0.0011,\n",
       "                  0.0008, 0.0008, 0.0009, 0.0008, 0.0009, 0.0006, 0.0009, 0.0003, 0.0008,\n",
       "                  0.0008, 0.0006, 0.0008, 0.0003, 0.0008, 0.0009, 0.0008, 0.0014, 0.0009,\n",
       "                  0.0006, 0.0010, 0.0012, 0.0008, 0.0012, 0.0006, 0.0011, 0.0006, 0.0003,\n",
       "                  0.0003, 0.0008, 0.0010, 0.0003, 0.0013, 0.0008, 0.0006, 0.0010, 0.0009,\n",
       "                  0.0013, 0.0012, 0.0009, 0.0014, 0.0008, 0.0011, 0.0010, 0.0008, 0.0012,\n",
       "                  0.0008, 0.0009, 0.0010, 0.0004, 0.0009, 0.0009, 0.0005, 0.0013, 0.0008,\n",
       "                  0.0009, 0.0005, 0.0013, 0.0007, 0.0003, 0.0006, 0.0005, 0.0005, 0.0010,\n",
       "                  0.0009, 0.0011, 0.0006, 0.0003, 0.0012, 0.0010, 0.0012, 0.0009, 0.0007,\n",
       "                  0.0014, 0.0006, 0.0003, 0.0003, 0.0010, 0.0008, 0.0013, 0.0006, 0.0004,\n",
       "                  0.0008, 0.0009, 0.0011, 0.0013, 0.0005, 0.0007, 0.0009, 0.0012, 0.0009,\n",
       "                  0.0007, 0.0008, 0.0007, 0.0007, 0.0003, 0.0010, 0.0012, 0.0007, 0.0004,\n",
       "                  0.0013, 0.0008, 0.0008, 0.0005, 0.0008, 0.0009, 0.0008, 0.0007, 0.0010,\n",
       "                  0.0016, 0.0012, 0.0011, 0.0009, 0.0012, 0.0003, 0.0014, 0.0003, 0.0006,\n",
       "                  0.0004, 0.0008, 0.0013, 0.0013, 0.0003, 0.0007, 0.0003, 0.0003, 0.0014,\n",
       "                  0.0008, 0.0011, 0.0003, 0.0012, 0.0008, 0.0004, 0.0008, 0.0011, 0.0009,\n",
       "                  0.0012, 0.0005, 0.0007, 0.0008, 0.0003, 0.0007, 0.0010, 0.0016, 0.0008,\n",
       "                  0.0014, 0.0011, 0.0011, 0.0011, 0.0009, 0.0003, 0.0003, 0.0008, 0.0008,\n",
       "                  0.0003, 0.0003, 0.0010, 0.0010, 0.0003, 0.0014, 0.0010, 0.0003, 0.0003,\n",
       "                  0.0005, 0.0009, 0.0008, 0.0004, 0.0007, 0.0009, 0.0006, 0.0011, 0.0005,\n",
       "                  0.0012, 0.0003, 0.0012, 0.0013, 0.0007, 0.0009, 0.0010, 0.0005, 0.0013,\n",
       "                  0.0003, 0.0003, 0.0008, 0.0011, 0.0012, 0.0006, 0.0004, 0.0003, 0.0007,\n",
       "                  0.0007, 0.0008, 0.0011, 0.0010, 0.0006, 0.0003, 0.0011, 0.0009, 0.0003,\n",
       "                  0.0010, 0.0009, 0.0009, 0.0005, 0.0009, 0.0009, 0.0006, 0.0004, 0.0006,\n",
       "                  0.0008, 0.0010, 0.0018, 0.0007, 0.0003, 0.0009, 0.0008, 0.0004, 0.0008,\n",
       "                  0.0010, 0.0003, 0.0010, 0.0004, 0.0006, 0.0007, 0.0009, 0.0003, 0.0008,\n",
       "                  0.0008, 0.0003, 0.0008, 0.0011, 0.0010, 0.0007, 0.0014, 0.0011, 0.0004,\n",
       "                  0.0008, 0.0003, 0.0003, 0.0006, 0.0007, 0.0003, 0.0011, 0.0003]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.1098, -0.0990, -0.1628, -0.2072, -0.0348, -0.1604, -0.0866, -0.0418,\n",
       "                    -0.1458, -0.1060, -0.1033, -0.0834, -0.0796, -0.1323, -0.0713, -0.1032,\n",
       "                    -0.1296, -0.0893, -0.0758, -0.1483, -0.0348, -0.1547, -0.0592, -0.1680,\n",
       "                    -0.0352, -0.1343, -0.0883, -0.1108, -0.1183, -0.1094, -0.1057, -0.1305,\n",
       "                    -0.0922, -0.0659, -0.1422, -0.0473, -0.0385, -0.0434, -0.1616, -0.1107,\n",
       "                    -0.1996, -0.0744, -0.1264, -0.0828, -0.0896, -0.1000, -0.1041, -0.0339,\n",
       "                    -0.0558, -0.1201, -0.1013, -0.0822, -0.0519, -0.0529, -0.0997, -0.0981,\n",
       "                    -0.0355, -0.1274, -0.1291, -0.1191, -0.0965, -0.0618, -0.0344, -0.1424,\n",
       "                    -0.1716, -0.1647, -0.0432, -0.1161, -0.1953, -0.1236, -0.0692, -0.0716,\n",
       "                    -0.0563, -0.1035, -0.1391, -0.1101, -0.0542, -0.0390, -0.0885, -0.1688,\n",
       "                    -0.0738, -0.1227, -0.1004, -0.1072, -0.0351, -0.0999, -0.0513, -0.1133,\n",
       "                    -0.2003, -0.0895, -0.1174, -0.0989, -0.0854, -0.1040, -0.1355, -0.0845,\n",
       "                    -0.1018, -0.1116, -0.1302, -0.0348, -0.1233, -0.0728, -0.1314, -0.0483,\n",
       "                    -0.1271, -0.0733, -0.1007, -0.1602, -0.1045, -0.1482, -0.1297, -0.1165,\n",
       "                    -0.1028, -0.1742, -0.0983, -0.1224, -0.0610, -0.2002, -0.0783, -0.0348,\n",
       "                    -0.0916, -0.1244, -0.1507, -0.0500, -0.0344, -0.0741, -0.0418, -0.1139,\n",
       "                    -0.1588, -0.0880, -0.1076, -0.0699, -0.0473, -0.0598, -0.1257, -0.1131,\n",
       "                    -0.1075, -0.0693, -0.1131, -0.0948, -0.0998, -0.0368, -0.1007, -0.1237,\n",
       "                    -0.1067, -0.1343, -0.1183, -0.1037, -0.0910, -0.1654, -0.1103, -0.0967,\n",
       "                    -0.0988, -0.1028, -0.0913, -0.1321, -0.0729, -0.1322, -0.0673, -0.0579,\n",
       "                    -0.1548, -0.0591, -0.0747, -0.1246, -0.1580, -0.1471, -0.1410, -0.0349,\n",
       "                    -0.1036, -0.0897, -0.0799, -0.0602, -0.0387, -0.0884, -0.0865, -0.0897,\n",
       "                    -0.1356, -0.1450, -0.1178, -0.0849, -0.1095, -0.0358, -0.0493, -0.1525,\n",
       "                    -0.1688, -0.1231, -0.1329, -0.1110, -0.1105, -0.1034, -0.1717, -0.0350,\n",
       "                    -0.1225, -0.1345, -0.0683, -0.1562, -0.1026, -0.0927, -0.0907, -0.1266,\n",
       "                    -0.1156, -0.1634, -0.1164, -0.0931, -0.1007, -0.1333, -0.1695, -0.0925,\n",
       "                    -0.1378, -0.1279, -0.1371, -0.1357, -0.0566, -0.1142, -0.0800, -0.0895,\n",
       "                    -0.1234, -0.0888, -0.0477, -0.1080, -0.0725, -0.1026, -0.0978, -0.1190,\n",
       "                    -0.0597, -0.1017, -0.0351, -0.0911, -0.0742, -0.0924, -0.0420, -0.0530,\n",
       "                    -0.1350, -0.0350, -0.0980, -0.1507, -0.0339, -0.0699, -0.0349, -0.1323,\n",
       "                    -0.0755, -0.0427, -0.0873, -0.1439, -0.0354, -0.0939, -0.1110, -0.1115,\n",
       "                    -0.1258, -0.0945, -0.1061, -0.1021, -0.0585, -0.1128, -0.0776, -0.1318,\n",
       "                    -0.1045, -0.0351, -0.1021, -0.1039, -0.1089, -0.1091, -0.0907, -0.0999,\n",
       "                    -0.1033, -0.1230, -0.0897, -0.0914, -0.0663, -0.1453, -0.1030, -0.0986,\n",
       "                    -0.0906, -0.1075, -0.1089, -0.0772, -0.1101, -0.0351, -0.1075, -0.1014,\n",
       "                    -0.0759, -0.1022, -0.0359, -0.1005, -0.1069, -0.1000, -0.1728, -0.1174,\n",
       "                    -0.0730, -0.1176, -0.1493, -0.1012, -0.1488, -0.0778, -0.1437, -0.0792,\n",
       "                    -0.0349, -0.0396, -0.0936, -0.1293, -0.0396, -0.1717, -0.0764, -0.0695,\n",
       "                    -0.1119, -0.1205, -0.1695, -0.1552, -0.1021, -0.1681, -0.1031, -0.1351,\n",
       "                    -0.1280, -0.0990, -0.1547, -0.1004, -0.1207, -0.1160, -0.0565, -0.1154,\n",
       "                    -0.1116, -0.0594, -0.1676, -0.1003, -0.1118, -0.0629, -0.1718, -0.0871,\n",
       "                    -0.0350, -0.0770, -0.0639, -0.0576, -0.1237, -0.1118, -0.1422, -0.0769,\n",
       "                    -0.0384, -0.1557, -0.1297, -0.1563, -0.1102, -0.0842, -0.1764, -0.0556,\n",
       "                    -0.0348, -0.0396, -0.1234, -0.0981, -0.1625, -0.0625, -0.0568, -0.1070,\n",
       "                    -0.1117, -0.1460, -0.1300, -0.0506, -0.0779, -0.0965, -0.1327, -0.1132,\n",
       "                    -0.0921, -0.0923, -0.0808, -0.0755, -0.0350, -0.1287, -0.1527, -0.0917,\n",
       "                    -0.0439, -0.1487, -0.1004, -0.0937, -0.0608, -0.0948, -0.1007, -0.0935,\n",
       "                    -0.0706, -0.1323, -0.1992, -0.1007, -0.1331, -0.1138, -0.1551, -0.0348,\n",
       "                    -0.1761, -0.0346, -0.0762, -0.0491, -0.1037, -0.1357, -0.1588, -0.0425,\n",
       "                    -0.0806, -0.0399, -0.0347, -0.1479, -0.0847, -0.1360, -0.0350, -0.1534,\n",
       "                    -0.0982, -0.0393, -0.0990, -0.1424, -0.1107, -0.1405, -0.0607, -0.0771,\n",
       "                    -0.1053, -0.0350, -0.0901, -0.1169, -0.2102, -0.0927, -0.1739, -0.1397,\n",
       "                    -0.1387, -0.1264, -0.1000, -0.0350, -0.0378, -0.0977, -0.1035, -0.0362,\n",
       "                    -0.0349, -0.1229, -0.1274, -0.0352, -0.1766, -0.1334, -0.0378, -0.0389,\n",
       "                    -0.0672, -0.1157, -0.1069, -0.0426, -0.0707, -0.0922, -0.0775, -0.1468,\n",
       "                    -0.0630, -0.1599, -0.0407, -0.1526, -0.1613, -0.0955, -0.1068, -0.1242,\n",
       "                    -0.0521, -0.1555, -0.0346, -0.0434, -0.0886, -0.1378, -0.1558, -0.0716,\n",
       "                    -0.0454, -0.0354, -0.0956, -0.0880, -0.0992, -0.1060, -0.1093, -0.0701,\n",
       "                    -0.0348, -0.1382, -0.0748, -0.0357, -0.1326, -0.1052, -0.1209, -0.0672,\n",
       "                    -0.1158, -0.0893, -0.0723, -0.0457, -0.0790, -0.1012, -0.1107, -0.2084,\n",
       "                    -0.0839, -0.0350, -0.1156, -0.0839, -0.0562, -0.0789, -0.1204, -0.0349,\n",
       "                    -0.1112, -0.0572, -0.0813, -0.0824, -0.1067, -0.0348, -0.1071, -0.0760,\n",
       "                    -0.0435, -0.1036, -0.1440, -0.1321, -0.0886, -0.1778, -0.1460, -0.0454,\n",
       "                    -0.1020, -0.0348, -0.0350, -0.0595, -0.0843, -0.0346, -0.1441, -0.0365]), max_val=tensor([0.1184, 0.1083, 0.1401, 0.1900, 0.0350, 0.1123, 0.1019, 0.0447, 0.1610,\n",
       "                    0.0994, 0.1324, 0.1026, 0.0960, 0.1036, 0.0581, 0.0990, 0.1279, 0.0783,\n",
       "                    0.0898, 0.1446, 0.0357, 0.1183, 0.0641, 0.1502, 0.0355, 0.1237, 0.1198,\n",
       "                    0.1424, 0.1019, 0.0867, 0.0985, 0.1073, 0.0778, 0.0604, 0.1448, 0.0371,\n",
       "                    0.0368, 0.0418, 0.1407, 0.1456, 0.1663, 0.0595, 0.1226, 0.0928, 0.0824,\n",
       "                    0.1024, 0.1030, 0.0365, 0.0573, 0.0968, 0.1426, 0.0555, 0.0513, 0.0504,\n",
       "                    0.0936, 0.1171, 0.0358, 0.1417, 0.1164, 0.1705, 0.1086, 0.0625, 0.0357,\n",
       "                    0.1364, 0.1903, 0.1511, 0.0516, 0.1276, 0.1698, 0.1112, 0.0813, 0.0662,\n",
       "                    0.0559, 0.1010, 0.1149, 0.1012, 0.0867, 0.0446, 0.0881, 0.1416, 0.0681,\n",
       "                    0.1216, 0.1160, 0.1243, 0.0365, 0.0921, 0.0579, 0.0901, 0.1467, 0.0872,\n",
       "                    0.1120, 0.1146, 0.1024, 0.1165, 0.1349, 0.0826, 0.1063, 0.1099, 0.1272,\n",
       "                    0.0353, 0.0952, 0.0703, 0.0988, 0.0473, 0.1259, 0.1008, 0.0924, 0.1419,\n",
       "                    0.1161, 0.1263, 0.1430, 0.0881, 0.1013, 0.1714, 0.0894, 0.1312, 0.0539,\n",
       "                    0.2070, 0.0824, 0.0351, 0.0911, 0.1271, 0.1028, 0.0481, 0.0354, 0.1010,\n",
       "                    0.0448, 0.1461, 0.1547, 0.0822, 0.1163, 0.0777, 0.0539, 0.0728, 0.1027,\n",
       "                    0.1229, 0.0989, 0.0697, 0.1117, 0.0861, 0.1007, 0.0409, 0.0969, 0.1175,\n",
       "                    0.1405, 0.1126, 0.1192, 0.0924, 0.0820, 0.0881, 0.1206, 0.0878, 0.1041,\n",
       "                    0.0942, 0.0959, 0.0955, 0.0794, 0.1435, 0.0935, 0.0560, 0.1352, 0.0596,\n",
       "                    0.0743, 0.1069, 0.1420, 0.1284, 0.1457, 0.0348, 0.1047, 0.0872, 0.0862,\n",
       "                    0.0588, 0.0408, 0.0769, 0.0861, 0.0950, 0.1251, 0.1308, 0.1063, 0.0758,\n",
       "                    0.1143, 0.0374, 0.0487, 0.1455, 0.1913, 0.1301, 0.1701, 0.1051, 0.1258,\n",
       "                    0.1142, 0.1449, 0.0348, 0.0828, 0.1142, 0.0614, 0.1776, 0.1396, 0.0889,\n",
       "                    0.0981, 0.1220, 0.1038, 0.1029, 0.1047, 0.1136, 0.0882, 0.1239, 0.1260,\n",
       "                    0.1124, 0.1705, 0.1450, 0.1181, 0.1151, 0.0595, 0.1331, 0.0887, 0.1128,\n",
       "                    0.1061, 0.0965, 0.0543, 0.0955, 0.0883, 0.1390, 0.0828, 0.1250, 0.0806,\n",
       "                    0.1097, 0.0357, 0.1120, 0.0737, 0.0818, 0.0471, 0.0553, 0.1477, 0.0356,\n",
       "                    0.1070, 0.1265, 0.0359, 0.0598, 0.0357, 0.0910, 0.0829, 0.0404, 0.0954,\n",
       "                    0.1304, 0.0365, 0.1086, 0.1293, 0.0948, 0.1286, 0.1093, 0.0888, 0.1080,\n",
       "                    0.0937, 0.1106, 0.0884, 0.1119, 0.0818, 0.0356, 0.0904, 0.0760, 0.0894,\n",
       "                    0.1162, 0.1190, 0.0976, 0.0884, 0.1578, 0.0757, 0.0954, 0.0737, 0.1365,\n",
       "                    0.1010, 0.0894, 0.1137, 0.0938, 0.1130, 0.0786, 0.0950, 0.0388, 0.0940,\n",
       "                    0.1078, 0.0686, 0.0991, 0.0376, 0.0970, 0.1093, 0.1009, 0.1502, 0.0916,\n",
       "                    0.0762, 0.1273, 0.1263, 0.1013, 0.1388, 0.0797, 0.1440, 0.0700, 0.0350,\n",
       "                    0.0398, 0.0987, 0.1260, 0.0409, 0.1067, 0.1002, 0.0781, 0.1215, 0.0908,\n",
       "                    0.1403, 0.1502, 0.1085, 0.1802, 0.1051, 0.1162, 0.0981, 0.0933, 0.1189,\n",
       "                    0.0980, 0.1100, 0.1254, 0.0553, 0.1189, 0.1187, 0.0600, 0.1344, 0.1056,\n",
       "                    0.1104, 0.0614, 0.1374, 0.0748, 0.0350, 0.0644, 0.0602, 0.0585, 0.1288,\n",
       "                    0.1124, 0.1403, 0.0815, 0.0377, 0.0856, 0.1049, 0.1387, 0.0967, 0.0797,\n",
       "                    0.1648, 0.0760, 0.0352, 0.0416, 0.1012, 0.1036, 0.1242, 0.0759, 0.0534,\n",
       "                    0.0787, 0.0982, 0.1332, 0.1693, 0.0576, 0.0922, 0.1162, 0.1497, 0.1073,\n",
       "                    0.0788, 0.0957, 0.0902, 0.0952, 0.0351, 0.1216, 0.0917, 0.0823, 0.0469,\n",
       "                    0.1613, 0.0923, 0.1010, 0.0640, 0.1054, 0.1100, 0.0986, 0.0850, 0.1229,\n",
       "                    0.1999, 0.1473, 0.1433, 0.1003, 0.1403, 0.0367, 0.1610, 0.0355, 0.0815,\n",
       "                    0.0505, 0.0823, 0.1662, 0.1612, 0.0435, 0.0917, 0.0421, 0.0362, 0.1780,\n",
       "                    0.1064, 0.0816, 0.0350, 0.1460, 0.1006, 0.0502, 0.1013, 0.1399, 0.1067,\n",
       "                    0.1581, 0.0663, 0.0901, 0.0912, 0.0349, 0.0846, 0.1227, 0.1891, 0.1026,\n",
       "                    0.1628, 0.1043, 0.1344, 0.1353, 0.1132, 0.0350, 0.0380, 0.0949, 0.0820,\n",
       "                    0.0395, 0.0358, 0.1255, 0.1167, 0.0361, 0.1470, 0.1266, 0.0383, 0.0409,\n",
       "                    0.0693, 0.0964, 0.1021, 0.0460, 0.0885, 0.1117, 0.0743, 0.1339, 0.0693,\n",
       "                    0.1389, 0.0426, 0.1107, 0.1471, 0.0891, 0.1096, 0.1242, 0.0603, 0.1608,\n",
       "                    0.0354, 0.0412, 0.1011, 0.1288, 0.1482, 0.0758, 0.0490, 0.0349, 0.0932,\n",
       "                    0.0656, 0.0935, 0.1348, 0.1244, 0.0750, 0.0355, 0.1311, 0.1153, 0.0356,\n",
       "                    0.1252, 0.1200, 0.1132, 0.0545, 0.1154, 0.1145, 0.0819, 0.0473, 0.0791,\n",
       "                    0.0800, 0.1248, 0.2308, 0.0702, 0.0350, 0.0980, 0.1061, 0.0551, 0.1061,\n",
       "                    0.1241, 0.0350, 0.1305, 0.0496, 0.0769, 0.0861, 0.1089, 0.0358, 0.0861,\n",
       "                    0.1009, 0.0438, 0.0948, 0.1282, 0.0893, 0.0724, 0.1774, 0.1101, 0.0457,\n",
       "                    0.1031, 0.0353, 0.0351, 0.0767, 0.0849, 0.0354, 0.1454, 0.0393])\n",
       "          )\n",
       "        )\n",
       "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1461]), zero_point=tensor([74], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-10.748093605041504, max_val=7.803608417510986)\n",
       "        )\n",
       "      )\n",
       "      (1): Linear(\n",
       "        in_features=512, out_features=256, bias=True\n",
       "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0011, 0.0005, 0.0009, 0.0012, 0.0005, 0.0012, 0.0008, 0.0007, 0.0012,\n",
       "                  0.0010, 0.0010, 0.0010, 0.0004, 0.0010, 0.0009, 0.0005, 0.0015, 0.0005,\n",
       "                  0.0010, 0.0005, 0.0012, 0.0010, 0.0013, 0.0008, 0.0005, 0.0016, 0.0005,\n",
       "                  0.0008, 0.0007, 0.0005, 0.0009, 0.0010, 0.0012, 0.0012, 0.0010, 0.0005,\n",
       "                  0.0009, 0.0006, 0.0010, 0.0011, 0.0009, 0.0006, 0.0009, 0.0004, 0.0011,\n",
       "                  0.0005, 0.0006, 0.0008, 0.0013, 0.0012, 0.0011, 0.0009, 0.0004, 0.0004,\n",
       "                  0.0011, 0.0013, 0.0005, 0.0014, 0.0006, 0.0017, 0.0011, 0.0016, 0.0012,\n",
       "                  0.0007, 0.0014, 0.0010, 0.0009, 0.0007, 0.0007, 0.0007, 0.0004, 0.0013,\n",
       "                  0.0011, 0.0005, 0.0015, 0.0005, 0.0008, 0.0004, 0.0006, 0.0004, 0.0012,\n",
       "                  0.0012, 0.0006, 0.0006, 0.0014, 0.0010, 0.0010, 0.0013, 0.0009, 0.0004,\n",
       "                  0.0008, 0.0009, 0.0008, 0.0009, 0.0009, 0.0004, 0.0005, 0.0006, 0.0005,\n",
       "                  0.0008, 0.0010, 0.0012, 0.0010, 0.0012, 0.0009, 0.0012, 0.0016, 0.0008,\n",
       "                  0.0010, 0.0009, 0.0006, 0.0010, 0.0008, 0.0013, 0.0014, 0.0011, 0.0005,\n",
       "                  0.0009, 0.0012, 0.0012, 0.0011, 0.0008, 0.0007, 0.0004, 0.0013, 0.0013,\n",
       "                  0.0013, 0.0014, 0.0006, 0.0008, 0.0009, 0.0010, 0.0011, 0.0009, 0.0008,\n",
       "                  0.0011, 0.0005, 0.0005, 0.0006, 0.0005, 0.0010, 0.0006, 0.0010, 0.0008,\n",
       "                  0.0010, 0.0010, 0.0012, 0.0011, 0.0008, 0.0012, 0.0014, 0.0006, 0.0011,\n",
       "                  0.0009, 0.0014, 0.0006, 0.0004, 0.0009, 0.0005, 0.0015, 0.0006, 0.0009,\n",
       "                  0.0013, 0.0012, 0.0008, 0.0004, 0.0010, 0.0010, 0.0008, 0.0010, 0.0010,\n",
       "                  0.0004, 0.0011, 0.0012, 0.0008, 0.0004, 0.0008, 0.0009, 0.0010, 0.0010,\n",
       "                  0.0009, 0.0011, 0.0005, 0.0012, 0.0008, 0.0005, 0.0004, 0.0008, 0.0010,\n",
       "                  0.0007, 0.0009, 0.0010, 0.0008, 0.0005, 0.0009, 0.0008, 0.0009, 0.0013,\n",
       "                  0.0008, 0.0010, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0006, 0.0005,\n",
       "                  0.0007, 0.0009, 0.0007, 0.0012, 0.0009, 0.0006, 0.0009, 0.0012, 0.0010,\n",
       "                  0.0006, 0.0008, 0.0010, 0.0007, 0.0004, 0.0005, 0.0004, 0.0005, 0.0005,\n",
       "                  0.0009, 0.0009, 0.0010, 0.0007, 0.0010, 0.0007, 0.0009, 0.0011, 0.0005,\n",
       "                  0.0011, 0.0009, 0.0006, 0.0008, 0.0008, 0.0010, 0.0008, 0.0012, 0.0014,\n",
       "                  0.0011, 0.0011, 0.0004, 0.0004, 0.0010, 0.0004, 0.0008, 0.0008, 0.0009,\n",
       "                  0.0010, 0.0010, 0.0009, 0.0008]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.0950, -0.0571, -0.1128, -0.1260, -0.0627, -0.1549, -0.0986, -0.0877,\n",
       "                    -0.1522, -0.1264, -0.1113, -0.1249, -0.0542, -0.1239, -0.1060, -0.0596,\n",
       "                    -0.1131, -0.0528, -0.1124, -0.0649, -0.1523, -0.0952, -0.1636, -0.0852,\n",
       "                    -0.0530, -0.2011, -0.0691, -0.0749, -0.0623, -0.0629, -0.1161, -0.1252,\n",
       "                    -0.1494, -0.1591, -0.1130, -0.0487, -0.0805, -0.0518, -0.1295, -0.1128,\n",
       "                    -0.0839, -0.0640, -0.0848, -0.0465, -0.0995, -0.0557, -0.0560, -0.1000,\n",
       "                    -0.1696, -0.0846, -0.0994, -0.0997, -0.0456, -0.0514, -0.1350, -0.1653,\n",
       "                    -0.0636, -0.1271, -0.0625, -0.1419, -0.1256, -0.1371, -0.1559, -0.0783,\n",
       "                    -0.1322, -0.1134, -0.1149, -0.0615, -0.0703, -0.0608, -0.0465, -0.1203,\n",
       "                    -0.1106, -0.0450, -0.1074, -0.0492, -0.0997, -0.0488, -0.0593, -0.0522,\n",
       "                    -0.1104, -0.1134, -0.0763, -0.0587, -0.1232, -0.1180, -0.0900, -0.1664,\n",
       "                    -0.1156, -0.0497, -0.0855, -0.0976, -0.1033, -0.1122, -0.0745, -0.0540,\n",
       "                    -0.0611, -0.0633, -0.0542, -0.0862, -0.1220, -0.1216, -0.0877, -0.0882,\n",
       "                    -0.1114, -0.1145, -0.1453, -0.0817, -0.1172, -0.0818, -0.0636, -0.1331,\n",
       "                    -0.0964, -0.1197, -0.1303, -0.1436, -0.0596, -0.0849, -0.1580, -0.1423,\n",
       "                    -0.1278, -0.0950, -0.0859, -0.0504, -0.1496, -0.1240, -0.1277, -0.1270,\n",
       "                    -0.0630, -0.0846, -0.0801, -0.1299, -0.1428, -0.0969, -0.0980, -0.1026,\n",
       "                    -0.0561, -0.0540, -0.0666, -0.0524, -0.1203, -0.0648, -0.1167, -0.0778,\n",
       "                    -0.1325, -0.1021, -0.1132, -0.1426, -0.0776, -0.1278, -0.1385, -0.0627,\n",
       "                    -0.1460, -0.1095, -0.1358, -0.0828, -0.0551, -0.0722, -0.0603, -0.1901,\n",
       "                    -0.0747, -0.0785, -0.1442, -0.1323, -0.0854, -0.0481, -0.1291, -0.1264,\n",
       "                    -0.0819, -0.1340, -0.0852, -0.0488, -0.1021, -0.1152, -0.1087, -0.0488,\n",
       "                    -0.0900, -0.0776, -0.1143, -0.1292, -0.1169, -0.1244, -0.0485, -0.1544,\n",
       "                    -0.0946, -0.0596, -0.0472, -0.0976, -0.1281, -0.0634, -0.0804, -0.1249,\n",
       "                    -0.1052, -0.0572, -0.1184, -0.1068, -0.1118, -0.1173, -0.0961, -0.0910,\n",
       "                    -0.0527, -0.0524, -0.0691, -0.0546, -0.0603, -0.0515, -0.0607, -0.0640,\n",
       "                    -0.0786, -0.0677, -0.1301, -0.1208, -0.0788, -0.1034, -0.1160, -0.1172,\n",
       "                    -0.0595, -0.0955, -0.1132, -0.0832, -0.0473, -0.0512, -0.0483, -0.0574,\n",
       "                    -0.0512, -0.1166, -0.0944, -0.1054, -0.0878, -0.1275, -0.0898, -0.1021,\n",
       "                    -0.1369, -0.0533, -0.1350, -0.1215, -0.0589, -0.1045, -0.0778, -0.1028,\n",
       "                    -0.0849, -0.1500, -0.1098, -0.1266, -0.1038, -0.0453, -0.0451, -0.0937,\n",
       "                    -0.0505, -0.0799, -0.0997, -0.0947, -0.1138, -0.1023, -0.1199, -0.0626]), max_val=tensor([0.1392, 0.0698, 0.0905, 0.1512, 0.0603, 0.1129, 0.0808, 0.0922, 0.1329,\n",
       "                    0.1061, 0.1325, 0.0942, 0.0568, 0.1009, 0.1134, 0.0679, 0.1909, 0.0642,\n",
       "                    0.1222, 0.0644, 0.0942, 0.1249, 0.1643, 0.0972, 0.0629, 0.1443, 0.0629,\n",
       "                    0.0994, 0.0831, 0.0678, 0.0958, 0.1042, 0.1097, 0.1117, 0.1227, 0.0618,\n",
       "                    0.1174, 0.0715, 0.1105, 0.1359, 0.1144, 0.0715, 0.1098, 0.0547, 0.1356,\n",
       "                    0.0658, 0.0714, 0.0931, 0.1275, 0.1494, 0.1358, 0.1087, 0.0481, 0.0488,\n",
       "                    0.1346, 0.1239, 0.0624, 0.1802, 0.0810, 0.2201, 0.1450, 0.2076, 0.1287,\n",
       "                    0.0869, 0.1777, 0.1214, 0.1203, 0.0889, 0.0945, 0.0837, 0.0540, 0.1591,\n",
       "                    0.1365, 0.0598, 0.1866, 0.0614, 0.0777, 0.0489, 0.0704, 0.0460, 0.1557,\n",
       "                    0.1503, 0.0710, 0.0778, 0.1794, 0.1295, 0.1285, 0.1279, 0.1040, 0.0555,\n",
       "                    0.0991, 0.1131, 0.0955, 0.1040, 0.1115, 0.0490, 0.0597, 0.0726, 0.0600,\n",
       "                    0.1018, 0.0890, 0.1491, 0.1289, 0.1468, 0.1034, 0.1542, 0.2071, 0.0963,\n",
       "                    0.1297, 0.1143, 0.0734, 0.1320, 0.0787, 0.1679, 0.1838, 0.1156, 0.0535,\n",
       "                    0.1115, 0.1550, 0.1573, 0.1427, 0.1055, 0.0681, 0.0565, 0.1596, 0.1596,\n",
       "                    0.1610, 0.1754, 0.0738, 0.0993, 0.1142, 0.0837, 0.1270, 0.1197, 0.0849,\n",
       "                    0.1409, 0.0697, 0.0679, 0.0811, 0.0659, 0.1310, 0.0730, 0.1284, 0.0978,\n",
       "                    0.1133, 0.1251, 0.1516, 0.1225, 0.0954, 0.1494, 0.1812, 0.0818, 0.1052,\n",
       "                    0.0910, 0.1827, 0.0724, 0.0547, 0.1151, 0.0540, 0.1352, 0.0698, 0.1168,\n",
       "                    0.1696, 0.1495, 0.1004, 0.0474, 0.1190, 0.0906, 0.1012, 0.1235, 0.1219,\n",
       "                    0.0560, 0.1382, 0.1517, 0.0730, 0.0455, 0.1015, 0.1196, 0.1212, 0.1198,\n",
       "                    0.1107, 0.1417, 0.0654, 0.1293, 0.1066, 0.0639, 0.0431, 0.1054, 0.0867,\n",
       "                    0.0847, 0.1105, 0.0951, 0.0795, 0.0613, 0.0861, 0.0782, 0.1114, 0.1641,\n",
       "                    0.0809, 0.1294, 0.0645, 0.0620, 0.0611, 0.0630, 0.0566, 0.0710, 0.0588,\n",
       "                    0.0904, 0.1171, 0.0919, 0.1496, 0.0830, 0.0653, 0.1164, 0.1582, 0.1225,\n",
       "                    0.0707, 0.1055, 0.1297, 0.0884, 0.0430, 0.0681, 0.0465, 0.0677, 0.0638,\n",
       "                    0.1034, 0.1201, 0.1237, 0.0921, 0.1020, 0.0927, 0.1185, 0.0955, 0.0604,\n",
       "                    0.0977, 0.0838, 0.0746, 0.0765, 0.1026, 0.1249, 0.0989, 0.1236, 0.1780,\n",
       "                    0.1449, 0.1454, 0.0485, 0.0479, 0.1259, 0.0492, 0.1051, 0.0711, 0.1128,\n",
       "                    0.1263, 0.1221, 0.0897, 0.1069])\n",
       "          )\n",
       "        )\n",
       "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1120]), zero_point=tensor([61], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-6.836277008056641, max_val=7.392834186553955)\n",
       "        )\n",
       "      )\n",
       "      (2): Linear(\n",
       "        in_features=256, out_features=128, bias=True\n",
       "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0012, 0.0012, 0.0006, 0.0016, 0.0015, 0.0006, 0.0011, 0.0012, 0.0005,\n",
       "                  0.0016, 0.0009, 0.0016, 0.0014, 0.0012, 0.0014, 0.0010, 0.0012, 0.0013,\n",
       "                  0.0017, 0.0015, 0.0011, 0.0009, 0.0011, 0.0012, 0.0005, 0.0009, 0.0009,\n",
       "                  0.0012, 0.0013, 0.0006, 0.0013, 0.0017, 0.0014, 0.0005, 0.0005, 0.0015,\n",
       "                  0.0017, 0.0017, 0.0006, 0.0016, 0.0007, 0.0005, 0.0005, 0.0013, 0.0008,\n",
       "                  0.0008, 0.0016, 0.0006, 0.0014, 0.0014, 0.0013, 0.0010, 0.0013, 0.0016,\n",
       "                  0.0017, 0.0013, 0.0005, 0.0007, 0.0016, 0.0005, 0.0012, 0.0014, 0.0005,\n",
       "                  0.0009, 0.0009, 0.0005, 0.0013, 0.0013, 0.0013, 0.0013, 0.0016, 0.0005,\n",
       "                  0.0014, 0.0014, 0.0015, 0.0005, 0.0012, 0.0012, 0.0005, 0.0010, 0.0010,\n",
       "                  0.0014, 0.0009, 0.0015, 0.0006, 0.0009, 0.0011, 0.0012, 0.0016, 0.0015,\n",
       "                  0.0006, 0.0005, 0.0015, 0.0015, 0.0010, 0.0014, 0.0010, 0.0015, 0.0011,\n",
       "                  0.0010, 0.0009, 0.0014, 0.0010, 0.0005, 0.0012, 0.0012, 0.0012, 0.0017,\n",
       "                  0.0014, 0.0007, 0.0014, 0.0005, 0.0005, 0.0009, 0.0013, 0.0008, 0.0014,\n",
       "                  0.0007, 0.0009, 0.0017, 0.0012, 0.0005, 0.0014, 0.0005, 0.0005, 0.0015,\n",
       "                  0.0012, 0.0014]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.1060, -0.1203, -0.0719, -0.1570, -0.1312, -0.0718, -0.1111, -0.0882,\n",
       "                    -0.0612, -0.2058, -0.0813, -0.2027, -0.1851, -0.0888, -0.1174, -0.1261,\n",
       "                    -0.1236, -0.1162, -0.1343, -0.1224, -0.0938, -0.1092, -0.1183, -0.1119,\n",
       "                    -0.0639, -0.0762, -0.1161, -0.1596, -0.1135, -0.0668, -0.1229, -0.1770,\n",
       "                    -0.1536, -0.0658, -0.0611, -0.1175, -0.1676, -0.1194, -0.0592, -0.1491,\n",
       "                    -0.0663, -0.0626, -0.0630, -0.1659, -0.0922, -0.0966, -0.1185, -0.0617,\n",
       "                    -0.1201, -0.1299, -0.1307, -0.0986, -0.1148, -0.1114, -0.1577, -0.1604,\n",
       "                    -0.0611, -0.0661, -0.1620, -0.0609, -0.1222, -0.1850, -0.0645, -0.1203,\n",
       "                    -0.0734, -0.0623, -0.1083, -0.1481, -0.1674, -0.1274, -0.1563, -0.0642,\n",
       "                    -0.1745, -0.1192, -0.1517, -0.0670, -0.1255, -0.1315, -0.0638, -0.1016,\n",
       "                    -0.1268, -0.1496, -0.0834, -0.1493, -0.0698, -0.1016, -0.0941, -0.1574,\n",
       "                    -0.1736, -0.1174, -0.0798, -0.0696, -0.1199, -0.1114, -0.0921, -0.1218,\n",
       "                    -0.0908, -0.1551, -0.1229, -0.1025, -0.1143, -0.1569, -0.1264, -0.0612,\n",
       "                    -0.1222, -0.1322, -0.1141, -0.1627, -0.1399, -0.0730, -0.1457, -0.0617,\n",
       "                    -0.0638, -0.1034, -0.1109, -0.1083, -0.1452, -0.0734, -0.0930, -0.1591,\n",
       "                    -0.0833, -0.0634, -0.1829, -0.0673, -0.0613, -0.1490, -0.1316, -0.1401]), max_val=tensor([0.1550, 0.1537, 0.0642, 0.2048, 0.1913, 0.0731, 0.1378, 0.1478, 0.0600,\n",
       "                    0.1430, 0.1083, 0.1572, 0.1538, 0.1519, 0.1788, 0.1061, 0.1545, 0.1709,\n",
       "                    0.2143, 0.1901, 0.1424, 0.1204, 0.1402, 0.1549, 0.0651, 0.1090, 0.1070,\n",
       "                    0.1143, 0.1605, 0.0788, 0.1670, 0.2100, 0.1760, 0.0665, 0.0610, 0.1885,\n",
       "                    0.2221, 0.2109, 0.0766, 0.1986, 0.0890, 0.0603, 0.0687, 0.1466, 0.1048,\n",
       "                    0.0912, 0.1979, 0.0716, 0.1813, 0.1772, 0.1656, 0.1314, 0.1618, 0.2030,\n",
       "                    0.2129, 0.1299, 0.0605, 0.0835, 0.2045, 0.0641, 0.1528, 0.1752, 0.0621,\n",
       "                    0.1007, 0.1088, 0.0644, 0.1594, 0.1684, 0.1031, 0.1650, 0.2044, 0.0618,\n",
       "                    0.1608, 0.1794, 0.1905, 0.0646, 0.1553, 0.1551, 0.0660, 0.1259, 0.1165,\n",
       "                    0.1783, 0.1183, 0.1959, 0.0737, 0.1154, 0.1460, 0.1370, 0.2019, 0.1950,\n",
       "                    0.0736, 0.0681, 0.1852, 0.1874, 0.1261, 0.1787, 0.1297, 0.1936, 0.1365,\n",
       "                    0.1258, 0.1037, 0.1764, 0.0987, 0.0614, 0.1519, 0.1558, 0.1477, 0.2180,\n",
       "                    0.1780, 0.0864, 0.1781, 0.0627, 0.0607, 0.1156, 0.1612, 0.0981, 0.1796,\n",
       "                    0.0839, 0.1193, 0.2116, 0.1504, 0.0608, 0.1564, 0.0694, 0.0608, 0.1853,\n",
       "                    0.1531, 0.1816])\n",
       "          )\n",
       "        )\n",
       "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1123]), zero_point=tensor([49], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-5.490901470184326, max_val=8.77623462677002)\n",
       "        )\n",
       "      )\n",
       "      (3): Linear(\n",
       "        in_features=128, out_features=64, bias=True\n",
       "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0020, 0.0020, 0.0014, 0.0009, 0.0018, 0.0023, 0.0016, 0.0019, 0.0016,\n",
       "                  0.0018, 0.0017, 0.0018, 0.0021, 0.0019, 0.0023, 0.0017, 0.0008, 0.0007,\n",
       "                  0.0011, 0.0025, 0.0023, 0.0018, 0.0020, 0.0018, 0.0017, 0.0013, 0.0018,\n",
       "                  0.0021, 0.0023, 0.0015, 0.0021, 0.0022, 0.0019, 0.0019, 0.0022, 0.0008,\n",
       "                  0.0018, 0.0011, 0.0022, 0.0015, 0.0017, 0.0017, 0.0021, 0.0019, 0.0015,\n",
       "                  0.0011, 0.0014, 0.0021, 0.0017, 0.0011, 0.0019, 0.0023, 0.0021, 0.0019,\n",
       "                  0.0020, 0.0018, 0.0024, 0.0023, 0.0019, 0.0022, 0.0020, 0.0020, 0.0008,\n",
       "                  0.0015]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                  0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.1904, -0.1527, -0.1392, -0.1130, -0.2026, -0.1495, -0.1239, -0.1686,\n",
       "                    -0.1349, -0.2307, -0.1352, -0.2268, -0.2303, -0.2381, -0.2069, -0.2152,\n",
       "                    -0.0896, -0.0907, -0.1036, -0.1663, -0.2021, -0.1557, -0.2073, -0.1363,\n",
       "                    -0.1774, -0.1033, -0.1637, -0.1907, -0.1851, -0.1904, -0.2012, -0.1824,\n",
       "                    -0.1912, -0.2006, -0.1607, -0.0860, -0.1883, -0.1141, -0.2075, -0.1494,\n",
       "                    -0.1492, -0.1906, -0.1562, -0.1808, -0.1608, -0.1072, -0.1828, -0.1743,\n",
       "                    -0.1486, -0.1244, -0.2250, -0.1929, -0.1750, -0.2051, -0.2133, -0.1853,\n",
       "                    -0.1996, -0.1936, -0.1649, -0.2194, -0.1903, -0.1771, -0.0873, -0.1209]), max_val=tensor([0.2500, 0.2533, 0.1832, 0.0890, 0.2330, 0.2884, 0.2036, 0.2436, 0.2094,\n",
       "                    0.1875, 0.2149, 0.1474, 0.2625, 0.1781, 0.2918, 0.1723, 0.1016, 0.0864,\n",
       "                    0.1431, 0.3136, 0.2860, 0.2340, 0.2509, 0.2227, 0.2220, 0.1621, 0.2269,\n",
       "                    0.2726, 0.2876, 0.1790, 0.2629, 0.2739, 0.2433, 0.2413, 0.2745, 0.1056,\n",
       "                    0.2346, 0.1425, 0.2853, 0.1954, 0.2161, 0.2142, 0.2618, 0.2472, 0.1870,\n",
       "                    0.1365, 0.1492, 0.2656, 0.2200, 0.1363, 0.2471, 0.2899, 0.2715, 0.2354,\n",
       "                    0.2599, 0.2274, 0.3045, 0.2861, 0.2388, 0.2826, 0.2494, 0.2558, 0.0956,\n",
       "                    0.1952])\n",
       "          )\n",
       "        )\n",
       "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.1605]), zero_point=tensor([50], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-7.9822468757629395, max_val=12.395500183105469)\n",
       "        )\n",
       "      )\n",
       "      (4): Linear(\n",
       "        in_features=64, out_features=10, bias=True\n",
       "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0034, 0.0046, 0.0057, 0.0052, 0.0045, 0.0050, 0.0048, 0.0047, 0.0045,\n",
       "                  0.0055]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "          (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "            min_val=tensor([-0.3319, -0.3676, -0.3997, -0.3805, -0.3260, -0.3662, -0.3609, -0.3921,\n",
       "                    -0.4425, -0.3908]), max_val=tensor([0.4274, 0.5900, 0.7296, 0.6633, 0.5737, 0.6330, 0.6082, 0.5959, 0.5770,\n",
       "                    0.7013])\n",
       "          )\n",
       "        )\n",
       "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.4888]), zero_point=tensor([57], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "          (activation_post_process): MovingAverageMinMaxObserver(min_val=-27.746728897094727, max_val=34.33488082885742)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relu): ReLU()\n",
       "    (soft): Softmax(\n",
       "      dim=None\n",
       "      (activation_post_process): FixedQParamsFakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0039]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine\n",
       "        (activation_post_process): FixedQParamsObserver()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attach a global qconfig, which contains information about what kind\n",
    "# of observers to attach. Use 'x86' for server inference and 'qnnpack'\n",
    "# for mobile inference. Other quantization configurations such as selecting\n",
    "# symmetric or asymmetric quantization and MinMax or L2Norm calibration techniques\n",
    "# can be specified here.\n",
    "# Note: the old 'fbgemm' is still available but 'x86' is the recommended default\n",
    "# for server inference.\n",
    "# model_fp32.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')\n",
    "model_fp32.qconfig = torch.ao.quantization.get_default_qat_qconfig('x86')\n",
    "\n",
    "# fuse the activations to preceding layers, where applicable\n",
    "# this needs to be done manually depending on the model architecture\n",
    "#model_fp32_fused = torch.ao.quantization.fuse_modules(model_fp32,\n",
    "  #  [['conv', 'bn', 'relu']])\n",
    "\n",
    "# Prepare the model for QAT. This inserts observers and fake_quants in\n",
    "# the model needs to be set to train for QAT logic to work\n",
    "# the model that will observe weight and activation tensors during calibration.\n",
    "model_fp32_prepared = torch.ao.quantization.prepare_qat(model_fp32.train())\n",
    "\n",
    "# run the training loop (not shown)\n",
    "train_model(model_fp32_prepared, train_loader, test_loader, cpu_device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the observed model to a quantized model. This does several things:\n",
    "# quantizes the weights, computes and stores the scale and bias value to be\n",
    "# used with each activation tensor, fuses modules where appropriate,\n",
    "# and replaces key operators with quantized implementations.\n",
    "model_fp32_prepared.eval()\n",
    "model_int8 = torch.ao.quantization.convert(model_fp32_prepared)\n",
    "\n",
    "# run the model, relevant calculations will happen in int8\n",
    "#res = model_int8(input_fp32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INT8 evaluation accuracy: 0.982\n"
     ]
    }
   ],
   "source": [
    "#, fp32_eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=cpu_device, criterion=None)\n",
    "_, int8_eval_accuracy = evaluate_model(model=model_int8, test_loader=test_loader, device=cpu_device, criterion=None)\n",
    "\n",
    "# Skip this assertion since the values might deviate a lot.\n",
    "# assert model_equivalence(model_1=model, model_2=quantized_jit_model, device=cpu_device, rtol=1e-01, atol=1e-02, num_tests=100, input_size=(1,3,32,32)), \"Quantized model deviates from the original model too much!\"\n",
    "\n",
    "#print(\"FP32 evaluation accuracy: {:.3f}\".format(fp32_eval_accuracy))\n",
    "print(\"INT8 evaluation accuracy: {:.3f}\".format(int8_eval_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the quantized model on the test images: 97.75 %\n",
      "Accuracy of the model on the test images: 97.75 %\n",
      "2.30 MB\n",
      "0.58 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "from torch.quantization import quantize_dynamic\n",
    "import torchvision.transforms as transforms\n",
    "from src.models import MLP\n",
    "from src import utils as u\n",
    "\n",
    "param = [ 28 * 28, # input\n",
    "                512, 256, 128, 64,\n",
    "                10 ] #output\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # convert the image to a PyTorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,)) # normalize the image with mean=0.5 and std=0.5\n",
    "])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "test_dataset = datasets.MNIST(root='data/', train=False, transform=transform, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "# Loading pretrained model\n",
    "modeldict = torch.load('models/mlp.ckpt')\n",
    "model = MLP(param)\n",
    "model.load_state_dict(modeldict)\n",
    "quantized_model = quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "\n",
    "model.to(device)\n",
    "quantized_model.eval()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correctq = 0\n",
    "    totalq = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for images, labels in test_loader:\n",
    "        images_cuda = images.to(device)\n",
    "        labels_cuda = labels.to(device)\n",
    "\n",
    "        outputsq = quantized_model(images)\n",
    "        _, predictedq = torch.max(outputsq.data, 1)\n",
    "        totalq += labels.size(0)\n",
    "        correctq += (predictedq == labels).sum().item()\n",
    "        \n",
    "        outputs = model(images_cuda)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels_cuda.size(0)\n",
    "        correct += (predicted == labels_cuda).sum().item()\n",
    "        \n",
    "\n",
    "    print('Accuracy of the quantized model on the test images: {} %'.format(100 * correctq / totalq))\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "u.print_model_size(model)\n",
    "u.print_model_size(quantized_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
